{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "characteristic-crash",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Homework 2 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0e53d",
   "metadata": {},
   "source": [
    "**I- Reading Assignment**\n",
    "\n",
    "[James, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). An Introduction to Statistical Learningâ€¯: With Applications in Python (1st ed. 2023 edition). Springer.](https://www.statlearning.com/)\n",
    "\n",
    "- Read Chapter 8, Section 1 (8.1: pages 331-342)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b356b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "# ignore warning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-ending",
   "metadata": {},
   "source": [
    "**II- Binary Classification**\n",
    "\n",
    "1- Select your binary outcome\n",
    "\n",
    "Amongst your categorical variables, select a binary variable to be used as an outcome.\n",
    "It you have no binary variable, then create a binary variable from one of your categorical variables.\n",
    "\n",
    "2- Divide your dataset into a test sample and a training sample with a ratio of 20% vs 80% respectively. <br>\n",
    "Be careful to do so that your outcomes remain representative of the initial data set and well shuffled.\n",
    "\n",
    "3- Select one classifier and fit it on the training set then estimate its score both on the training set and the test set. <br>Use the accuracy, the ROC AUC, the F1 score and the Kappa to estimate the performance of your classifier.\n",
    "\n",
    "4- Repeat the same code on two other different classifiers: \n",
    "\n",
    "- Recommendation: create a function that does all the job and run it using three different classifiers\n",
    ". \n",
    "- Which is the best classifier ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff79ca0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/fulin/Documents/GitHub/EMLyon_DSAIS/ML/homework01/data'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80b2e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/fulin/Documents/GitHub/EMLyon_DSAIS/ML/homework01/data\"\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d8e728b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"customer_data_cbn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a2410d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbn_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "16b7bf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_length</th>\n",
       "      <th>number_vmail_messages</th>\n",
       "      <th>total_day_minutes</th>\n",
       "      <th>total_day_calls</th>\n",
       "      <th>total_day_charge</th>\n",
       "      <th>total_eve_minutes</th>\n",
       "      <th>total_eve_calls</th>\n",
       "      <th>total_eve_charge</th>\n",
       "      <th>total_night_minutes</th>\n",
       "      <th>total_night_calls</th>\n",
       "      <th>total_night_charge</th>\n",
       "      <th>total_intl_minutes</th>\n",
       "      <th>total_intl_calls</th>\n",
       "      <th>total_intl_charge</th>\n",
       "      <th>number_customer_service_calls</th>\n",
       "      <th>state</th>\n",
       "      <th>area_code</th>\n",
       "      <th>international_plan</th>\n",
       "      <th>voice_mail_plan</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.9</td>\n",
       "      <td>123.0</td>\n",
       "      <td>12.050</td>\n",
       "      <td>270.26</td>\n",
       "      <td>73.0</td>\n",
       "      <td>22.976</td>\n",
       "      <td>236.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>10.62</td>\n",
       "      <td>10.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.860</td>\n",
       "      <td>3.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>510.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>223.6</td>\n",
       "      <td>86.0</td>\n",
       "      <td>37.912</td>\n",
       "      <td>244.80</td>\n",
       "      <td>139.0</td>\n",
       "      <td>20.810</td>\n",
       "      <td>94.2</td>\n",
       "      <td>81.0</td>\n",
       "      <td>4.24</td>\n",
       "      <td>9.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MT</td>\n",
       "      <td>510.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>294.7</td>\n",
       "      <td>95.0</td>\n",
       "      <td>40.878</td>\n",
       "      <td>237.30</td>\n",
       "      <td>105.0</td>\n",
       "      <td>20.170</td>\n",
       "      <td>300.3</td>\n",
       "      <td>127.0</td>\n",
       "      <td>13.51</td>\n",
       "      <td>13.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OH</td>\n",
       "      <td>408.0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.0</td>\n",
       "      <td>33.2</td>\n",
       "      <td>216.8</td>\n",
       "      <td>123.0</td>\n",
       "      <td>36.860</td>\n",
       "      <td>126.40</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.740</td>\n",
       "      <td>220.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>9.93</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.432</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NM</td>\n",
       "      <td>415.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>108.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>197.4</td>\n",
       "      <td>78.0</td>\n",
       "      <td>33.560</td>\n",
       "      <td>124.00</td>\n",
       "      <td>101.0</td>\n",
       "      <td>10.540</td>\n",
       "      <td>204.5</td>\n",
       "      <td>107.0</td>\n",
       "      <td>9.20</td>\n",
       "      <td>7.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.080</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SC</td>\n",
       "      <td>415.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_length  number_vmail_messages  total_day_minutes  total_day_calls  \\\n",
       "0           101.0                    0.0               70.9            123.0   \n",
       "1           137.0                    0.0              223.6             86.0   \n",
       "2           103.0                   29.0              294.7             95.0   \n",
       "3            99.0                   33.2              216.8            123.0   \n",
       "4           108.0                    0.0              197.4             78.0   \n",
       "\n",
       "   total_day_charge  total_eve_minutes  total_eve_calls  total_eve_charge  \\\n",
       "0            12.050             270.26             73.0            22.976   \n",
       "1            37.912             244.80            139.0            20.810   \n",
       "2            40.878             237.30            105.0            20.170   \n",
       "3            36.860             126.40             88.0            10.740   \n",
       "4            33.560             124.00            101.0            10.540   \n",
       "\n",
       "   total_night_minutes  total_night_calls  total_night_charge  \\\n",
       "0                236.0               73.0               10.62   \n",
       "1                 94.2               81.0                4.24   \n",
       "2                300.3              127.0               13.51   \n",
       "3                220.6               82.0                9.93   \n",
       "4                204.5              107.0                9.20   \n",
       "\n",
       "   total_intl_minutes  total_intl_calls  total_intl_charge  \\\n",
       "0                10.6               3.0              2.860   \n",
       "1                 9.5               7.0              2.570   \n",
       "2                13.7               4.8              3.700   \n",
       "3                 9.0               2.0              2.432   \n",
       "4                 7.7               4.0              2.080   \n",
       "\n",
       "   number_customer_service_calls state  area_code international_plan  \\\n",
       "0                            3.0    HI      510.0                 no   \n",
       "1                            0.0    MT      510.0                 no   \n",
       "2                            1.0    OH      408.0                 no   \n",
       "3                            1.0    NM      415.0                 no   \n",
       "4                            2.0    SC      415.0                 no   \n",
       "\n",
       "  voice_mail_plan churn  \n",
       "0              no    no  \n",
       "1              no    no  \n",
       "2             yes    no  \n",
       "3              no    no  \n",
       "4              no    no  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbn_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0efefb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12873 entries, 0 to 12872\n",
      "Data columns (total 20 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   account_length                 12873 non-null  float64\n",
      " 1   number_vmail_messages          12873 non-null  float64\n",
      " 2   total_day_minutes              12873 non-null  float64\n",
      " 3   total_day_calls                12873 non-null  float64\n",
      " 4   total_day_charge               12873 non-null  float64\n",
      " 5   total_eve_minutes              12873 non-null  float64\n",
      " 6   total_eve_calls                12873 non-null  float64\n",
      " 7   total_eve_charge               12873 non-null  float64\n",
      " 8   total_night_minutes            12873 non-null  float64\n",
      " 9   total_night_calls              12873 non-null  float64\n",
      " 10  total_night_charge             12873 non-null  float64\n",
      " 11  total_intl_minutes             12873 non-null  float64\n",
      " 12  total_intl_calls               12873 non-null  float64\n",
      " 13  total_intl_charge              12873 non-null  float64\n",
      " 14  number_customer_service_calls  12873 non-null  float64\n",
      " 15  state                          12873 non-null  object \n",
      " 16  area_code                      12873 non-null  float64\n",
      " 17  international_plan             12873 non-null  object \n",
      " 18  voice_mail_plan                12873 non-null  object \n",
      " 19  churn                          12873 non-null  object \n",
      "dtypes: float64(16), object(4)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "cbn_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "662b64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = cbn_df.drop(columns=[\"churn\"], axis=1)\n",
    "target_ready = cbn_df[\"churn\"].map({\"yes\": 1, \"no\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a1f8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12873 entries, 0 to 12872\n",
      "Data columns (total 19 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   account_length                 12873 non-null  float64\n",
      " 1   number_vmail_messages          12873 non-null  float64\n",
      " 2   total_day_minutes              12873 non-null  float64\n",
      " 3   total_day_calls                12873 non-null  float64\n",
      " 4   total_day_charge               12873 non-null  float64\n",
      " 5   total_eve_minutes              12873 non-null  float64\n",
      " 6   total_eve_calls                12873 non-null  float64\n",
      " 7   total_eve_charge               12873 non-null  float64\n",
      " 8   total_night_minutes            12873 non-null  float64\n",
      " 9   total_night_calls              12873 non-null  float64\n",
      " 10  total_night_charge             12873 non-null  float64\n",
      " 11  total_intl_minutes             12873 non-null  float64\n",
      " 12  total_intl_calls               12873 non-null  float64\n",
      " 13  total_intl_charge              12873 non-null  float64\n",
      " 14  number_customer_service_calls  12873 non-null  float64\n",
      " 15  state                          12873 non-null  object \n",
      " 16  area_code                      12873 non-null  float64\n",
      " 17  international_plan             12873 non-null  object \n",
      " 18  voice_mail_plan                12873 non-null  object \n",
      "dtypes: float64(16), object(3)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "predictors.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "870848b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quanti_features = predictors.iloc[:, :14].columns.tolist()\n",
    "cat_features = predictors.iloc[:, 15:].columns.tolist()\n",
    "cat_ohe_features = predictors.iloc[:, 16:].columns.tolist()\n",
    "cat_lbe_features = [\"state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f1ae0d",
   "metadata": {},
   "source": [
    "This is a place where I carry on struggling, the column state contains the abbreviation of the state, in string format. But before I can train the model I need to do a numerical operation on him. Since he has more than 50 different categories in it, it will cause a serious dimension explosion if I use solo hot coding. So I was thinking of using pca or dimensionality reduction, but this would result in a less interpretable model and no guarantee of having the same representation of the new data in the test set and the training set.\n",
    "So I am again considering me supervised learning of the state columns, for example performing k-means clustering, setting the number of clusters, and then coding using the classification obtained from the unsupervised learning model as the basis for the classification. This scenario I think would result in a very serious data leakage.\n",
    "So I am thinking again that I can only use labeled coding, but I am worried about linearity issues with non-tree models. But I had no other choice, so I added the label encoder to the pipeline of my column converter. But a new problem arose, in order to avoid data leakage, I did the column converter after dividing the training and test sets, so I couldn't guarantee that the same states were encoded consistently in my training and test sets, and there was also the possibility that some states existed in only one dataset. So in the end, I chose to manually encode the labels before dividing the training and test sets, with the help of the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "29bfac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors[\"state\"] = predictors[\"state\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1af1bc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        11\n",
       "1        26\n",
       "2        35\n",
       "3        32\n",
       "4        40\n",
       "         ..\n",
       "12868    26\n",
       "12869    26\n",
       "12870    13\n",
       "12871     0\n",
       "12872    44\n",
       "Name: state, Length: 12873, dtype: int8"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors[\"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ef03063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    predictors,\n",
    "    target_ready,\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    stratify=target_ready,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7e126974",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_state = X_train[\"state\"]\n",
    "X_test_state = X_test[\"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "405d90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), quanti_features),\n",
    "        (\"cat_ohe\", OneHotEncoder(), cat_ohe_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([(\"preprocessor\", preprocessor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e9701e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10298, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2575, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.column_stack([pipeline.fit_transform(X_train), X_train_state])\n",
    "display(X_train.shape)\n",
    "X_test = np.column_stack([pipeline.fit_transform(X_test), X_test_state])\n",
    "display(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4e26be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, cohen_kappa_score\n",
    "import time\n",
    "\n",
    "\n",
    "def evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n",
    "    start_time = time.time()\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy (train)\": accuracy_score(y_train, y_train_pred),\n",
    "        \"Accuracy (test)\": accuracy_score(y_test, y_test_pred),\n",
    "        \"ROC AUC (train)\": roc_auc_score(y_train, y_train_pred),\n",
    "        \"ROC AUC (test)\": roc_auc_score(y_test, y_test_pred),\n",
    "        \"F1 Score (train)\": f1_score(y_train, y_train_pred),\n",
    "        \"F1 Score (test)\": f1_score(y_test, y_test_pred),\n",
    "        \"Kappa (train)\": cohen_kappa_score(y_train, y_train_pred),\n",
    "        \"Kappa (test)\": cohen_kappa_score(y_test, y_test_pred),\n",
    "        \"Training Time (s)\": training_time,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bfd521e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "classifiers = [\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=100)),\n",
    "    (\"KNN\", KNeighborsClassifier()),\n",
    "    (\"Decision Trees\", DecisionTreeClassifier()),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"SVM\", SVC(probability=True)),\n",
    "    (\"XGBooster\", XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0f44440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for name, clf in classifiers:\n",
    "    results[name] = evaluate_classifier(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b6a77c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': {'Accuracy (train)': 0.8651194406680909,\n",
       "  'Accuracy (test)': 0.8710679611650486,\n",
       "  'ROC AUC (train)': 0.5626020420085276,\n",
       "  'ROC AUC (test)': 0.5760856059363522,\n",
       "  'F1 Score (train)': 0.22790439132851587,\n",
       "  'F1 Score (test)': 0.2654867256637168,\n",
       "  'Kappa (train)': 0.18400920869797077,\n",
       "  'Kappa (test)': 0.22270229379652962,\n",
       "  'Training Time (s)': 0.15522098541259766},\n",
       " 'KNN': {'Accuracy (train)': 0.9584385317537386,\n",
       "  'Accuracy (test)': 0.9013592233009708,\n",
       "  'ROC AUC (train)': 0.8757655358290836,\n",
       "  'ROC AUC (test)': 0.7096206032773197,\n",
       "  'F1 Score (train)': 0.8381240544629349,\n",
       "  'F1 Score (test)': 0.5590277777777778,\n",
       "  'Kappa (train)': 0.8145674991545708,\n",
       "  'Kappa (test)': 0.5078126881306138,\n",
       "  'Training Time (s)': 0.00038886070251464844},\n",
       " 'Decision Trees': {'Accuracy (train)': 1.0,\n",
       "  'Accuracy (test)': 0.9398058252427185,\n",
       "  'ROC AUC (train)': 1.0,\n",
       "  'ROC AUC (test)': 0.8754442075337597,\n",
       "  'F1 Score (train)': 1.0,\n",
       "  'F1 Score (test)': 0.7867950481430535,\n",
       "  'Kappa (train)': 1.0,\n",
       "  'Kappa (test)': 0.7517510376231293,\n",
       "  'Training Time (s)': 0.10955524444580078},\n",
       " 'Random Forest': {'Accuracy (train)': 1.0,\n",
       "  'Accuracy (test)': 0.9693203883495146,\n",
       "  'ROC AUC (train)': 1.0,\n",
       "  'ROC AUC (test)': 0.8937784851963956,\n",
       "  'F1 Score (train)': 1.0,\n",
       "  'F1 Score (test)': 0.8790199081163859,\n",
       "  'Kappa (train)': 1.0,\n",
       "  'Kappa (test)': 0.8617175237563075,\n",
       "  'Training Time (s)': 1.3955488204956055},\n",
       " 'SVM': {'Accuracy (train)': 0.8585162167411148,\n",
       "  'Accuracy (test)': 0.8586407766990292,\n",
       "  'ROC AUC (train)': 0.5,\n",
       "  'ROC AUC (test)': 0.5,\n",
       "  'F1 Score (train)': 0.0,\n",
       "  'F1 Score (test)': 0.0,\n",
       "  'Kappa (train)': 0.0,\n",
       "  'Kappa (test)': 0.0,\n",
       "  'Training Time (s)': 5.464594841003418},\n",
       " 'XGBooster': {'Accuracy (train)': 0.9989318314235774,\n",
       "  'Accuracy (test)': 0.9654368932038835,\n",
       "  'ROC AUC (train)': 0.9962251201098147,\n",
       "  'ROC AUC (test)': 0.8788947371036924,\n",
       "  'F1 Score (train)': 0.9962108163968308,\n",
       "  'F1 Score (test)': 0.8611544461778471,\n",
       "  'Kappa (train)': 0.9955891119854993,\n",
       "  'Kappa (test)': 0.8418303576666466,\n",
       "  'Training Time (s)': 0.1129908561706543}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4db97f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+\n",
      "|                             Training Set Performance                             |\n",
      "+---------------------+----------+---------+----------+--------+-------------------+\n",
      "|      Classifier     | Accuracy | ROC AUC | F1 Score | Kappa  | Training Time (s) |\n",
      "+---------------------+----------+---------+----------+--------+-------------------+\n",
      "| Logistic Regression |  0.8651  |  0.5626 |  0.2279  | 0.1840 |       0.1552      |\n",
      "|         KNN         |  0.9584  |  0.8758 |  0.8381  | 0.8146 |       0.0004      |\n",
      "|    Decision Trees   |  1.0000  |  1.0000 |  1.0000  | 1.0000 |       0.1096      |\n",
      "|    Random Forest    |  1.0000  |  1.0000 |  1.0000  | 1.0000 |       1.3955      |\n",
      "|         SVM         |  0.8585  |  0.5000 |  0.0000  | 0.0000 |       5.4646      |\n",
      "|      XGBooster      |  0.9989  |  0.9962 |  0.9962  | 0.9956 |       0.1130      |\n",
      "+---------------------+----------+---------+----------+--------+-------------------+\n",
      "+--------------------------------------------------------------+\n",
      "|                     Test Set Performance                     |\n",
      "+---------------------+----------+---------+----------+--------+\n",
      "|      Classifier     | Accuracy | ROC AUC | F1 Score | Kappa  |\n",
      "+---------------------+----------+---------+----------+--------+\n",
      "| Logistic Regression |  0.8711  |  0.5761 |  0.2655  | 0.2227 |\n",
      "|         KNN         |  0.9014  |  0.7096 |  0.5590  | 0.5078 |\n",
      "|    Decision Trees   |  0.9398  |  0.8754 |  0.7868  | 0.7518 |\n",
      "|    Random Forest    |  0.9693  |  0.8938 |  0.8790  | 0.8617 |\n",
      "|         SVM         |  0.8586  |  0.5000 |  0.0000  | 0.0000 |\n",
      "|      XGBooster      |  0.9654  |  0.8789 |  0.8612  | 0.8418 |\n",
      "+---------------------+----------+---------+----------+--------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "train_table = PrettyTable()\n",
    "train_table.title = \"Training Set Performance\"\n",
    "train_table.field_names = [\n",
    "    \"Classifier\",\n",
    "    \"Accuracy\",\n",
    "    \"ROC AUC\",\n",
    "    \"F1 Score\",\n",
    "    \"Kappa\",\n",
    "    \"Training Time (s)\",\n",
    "]\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    train_table.add_row(\n",
    "        [\n",
    "            name,\n",
    "            f\"{metrics['Accuracy (train)']:.4f}\",\n",
    "            f\"{metrics['ROC AUC (train)']:.4f}\",\n",
    "            f\"{metrics['F1 Score (train)']:.4f}\",\n",
    "            f\"{metrics['Kappa (train)']:.4f}\",\n",
    "            f\"{metrics['Training Time (s)']:.4f}\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "test_table = PrettyTable()\n",
    "test_table.title = \"Test Set Performance\"\n",
    "test_table.field_names = [\n",
    "    \"Classifier\",\n",
    "    \"Accuracy\",\n",
    "    \"ROC AUC\",\n",
    "    \"F1 Score\",\n",
    "    \"Kappa\",\n",
    "]\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    test_table.add_row(\n",
    "        [\n",
    "            name,\n",
    "            f\"{metrics['Accuracy (test)']:.4f}\",\n",
    "            f\"{metrics['ROC AUC (test)']:.4f}\",\n",
    "            f\"{metrics['F1 Score (test)']:.4f}\",\n",
    "            f\"{metrics['Kappa (test)']:.4f}\",\n",
    "        ]\n",
    "    )\n",
    "print(train_table)\n",
    "print(test_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f305df",
   "metadata": {},
   "source": [
    "### Model Analysis based on Performance and Computational Cost\n",
    "\n",
    "#### 1. Computational Cost vs. Performance:\n",
    "- The **SVM** has a considerably longer training time compared to the other models. Despite this high computational cost, its performance on the test set isn't top-tier. This highlights the importance of balancing model performance with computational expenses.\n",
    "  \n",
    "- On the other hand, the **Random Forest** exhibits exceptional performance on the test set but also comes with a lengthier training time. Given its superior performance, this computational cost might be deemed acceptable.\n",
    "\n",
    "#### 2. Overfitting:\n",
    "- The **Decision Trees** model showcases perfect performance on the training set. However, a drop in performance on the test set signals a classic case of overfitting.\n",
    "  \n",
    "- Both **Random Forest** and **XGBooster** shine on the training set, but they manifest a smaller performance drop on the test set. This suggests that they might be generalizing better than some of the other models.\n",
    "\n",
    "#### 3. Model Selection:\n",
    "- When factoring in both performance and computational cost, the **Random Forest** seems like an optimal choice. It stands out in terms of test set performance, although it requires more training time compared to some models.\n",
    "  \n",
    "- The near-zero training time for **KNN** is noteworthy. This is due to the nature of KNN being an instance-based learning method, with its primary computational cost being during the prediction phase rather than the training phase.\n",
    "\n",
    "In conclusion, selecting the ideal model requires a holistic view that encompasses multiple factors, including model performance, computational expenses, business requirements, and other specific contextual considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-award",
   "metadata": {},
   "source": [
    "**III- Multiclass Classification** (Optional with good bonus point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-crack",
   "metadata": {},
   "source": [
    "This time, select a categorical variable having more than 2 categories. I you have none, then categorize one of your quantitative data (e.g in using `pd.cut()` and the corresponding quantiles) into at least three classes.\n",
    "\n",
    "Copy the codes of PART I and adjust to the new outcome. Use only Accuracy and F1 Score for performance (or sklearn `classification_report`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b76f3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca05bf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HI', 'MT', 'OH', 'NM', 'SC', 'IA', 'ND', 'LA', 'MO', 'TX', 'AR',\n",
       "       'ME', 'DE', 'MN', 'KS', 'NC', 'NV', 'CO', 'TN', 'OR', 'NE', 'ID',\n",
       "       'WY', 'MD', 'AZ', 'VT', 'MS', 'CA', 'WI', 'SD', 'WV', 'MI', 'UT',\n",
       "       'NJ', 'MA', 'AL', 'FL', 'VA', 'IL', 'PA', 'RI', 'DC', 'IN', 'CT',\n",
       "       'GA', 'NY', 'NH', 'KY', 'WA', 'AK', 'OK'], dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_multi[\"state\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f5bf703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "East    7019\n",
       "Mid     3058\n",
       "West    2796\n",
       "Name: region, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the states for each region\n",
    "west_states = [\"WA\", \"OR\", \"CA\", \"NV\", \"ID\", \"UT\", \"AZ\", \"MT\", \"WY\", \"CO\", \"NM\"]\n",
    "mid_states = [\"ND\", \"SD\", \"NE\", \"KS\", \"MO\", \"IA\", \"MN\", \"WI\", \"IL\", \"IN\", \"OH\", \"MI\"]\n",
    "\n",
    "\n",
    "# Create a new column 'region' based on the 'state' column\n",
    "def state_to_region(state):\n",
    "    if state in west_states:\n",
    "        return \"West\"\n",
    "    elif state in mid_states:\n",
    "        return \"Mid\"\n",
    "    else:\n",
    "        return \"East\"\n",
    "\n",
    "\n",
    "df_multi[\"region\"] = df_multi[\"state\"].apply(state_to_region)\n",
    "\n",
    "# Display the distribution of the new 'region' column\n",
    "df_multi[\"region\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "18914433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Logistic Regression\n",
      "Training Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        East       0.55      1.00      0.71      5615\n",
      "         Mid       0.00      0.00      0.00      2446\n",
      "        West       0.00      0.00      0.00      2237\n",
      "\n",
      "    accuracy                           0.55     10298\n",
      "   macro avg       0.18      0.33      0.24     10298\n",
      "weighted avg       0.30      0.55      0.38     10298\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        East       0.55      1.00      0.71      1404\n",
      "         Mid       0.00      0.00      0.00       612\n",
      "        West       0.00      0.00      0.00       559\n",
      "\n",
      "    accuracy                           0.55      2575\n",
      "   macro avg       0.18      0.33      0.24      2575\n",
      "weighted avg       0.30      0.55      0.38      2575\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Classifier: KNN\n",
      "Training Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        East       0.86      0.93      0.89      5615\n",
      "         Mid       0.86      0.80      0.83      2446\n",
      "        West       0.86      0.76      0.81      2237\n",
      "\n",
      "    accuracy                           0.86     10298\n",
      "   macro avg       0.86      0.83      0.84     10298\n",
      "weighted avg       0.86      0.86      0.86     10298\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        East       0.70      0.82      0.75      1404\n",
      "         Mid       0.57      0.51      0.54       612\n",
      "        West       0.62      0.42      0.50       559\n",
      "\n",
      "    accuracy                           0.66      2575\n",
      "   macro avg       0.63      0.58      0.60      2575\n",
      "weighted avg       0.65      0.66      0.65      2575\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Classifier: Decision Trees\n",
      "Training Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        East       1.00      1.00      1.00      5615\n",
      "         Mid       1.00      1.00      1.00      2446\n",
      "        West       1.00      1.00      1.00      2237\n",
      "\n",
      "    accuracy                           1.00     10298\n",
      "   macro avg       1.00      1.00      1.00     10298\n",
      "weighted avg       1.00      1.00      1.00     10298\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        East       0.85      0.82      0.84      1404\n",
      "         Mid       0.75      0.76      0.75       612\n",
      "        West       0.70      0.74      0.72       559\n",
      "\n",
      "    accuracy                           0.79      2575\n",
      "   macro avg       0.77      0.77      0.77      2575\n",
      "weighted avg       0.79      0.79      0.79      2575\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Classifier: Random Forest\n",
      "Training Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        East       1.00      1.00      1.00      5615\n",
      "         Mid       1.00      1.00      1.00      2446\n",
      "        West       1.00      1.00      1.00      2237\n",
      "\n",
      "    accuracy                           1.00     10298\n",
      "   macro avg       1.00      1.00      1.00     10298\n",
      "weighted avg       1.00      1.00      1.00     10298\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        East       0.85      0.99      0.92      1404\n",
      "         Mid       0.99      0.79      0.88       612\n",
      "        West       0.99      0.79      0.88       559\n",
      "\n",
      "    accuracy                           0.90      2575\n",
      "   macro avg       0.94      0.86      0.89      2575\n",
      "weighted avg       0.91      0.90      0.90      2575\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Classifier: SVM\n",
      "Training Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        East       0.57      1.00      0.72      5615\n",
      "         Mid       0.96      0.10      0.18      2446\n",
      "        West       0.95      0.07      0.13      2237\n",
      "\n",
      "    accuracy                           0.58     10298\n",
      "   macro avg       0.83      0.39      0.34     10298\n",
      "weighted avg       0.75      0.58      0.47     10298\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        East       0.56      1.00      0.72      1404\n",
      "         Mid       0.89      0.05      0.10       612\n",
      "        West       0.86      0.04      0.09       559\n",
      "\n",
      "    accuracy                           0.57      2575\n",
      "   macro avg       0.77      0.36      0.30      2575\n",
      "weighted avg       0.70      0.57      0.43      2575\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Classifier: XGBooster\n",
      "Training Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        East       0.95      1.00      0.97      5615\n",
      "         Mid       0.99      0.93      0.96      2446\n",
      "        West       0.99      0.92      0.96      2237\n",
      "\n",
      "    accuracy                           0.97     10298\n",
      "   macro avg       0.98      0.95      0.96     10298\n",
      "weighted avg       0.97      0.97      0.97     10298\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        East       0.78      0.97      0.87      1404\n",
      "         Mid       0.92      0.67      0.78       612\n",
      "        West       0.90      0.61      0.73       559\n",
      "\n",
      "    accuracy                           0.82      2575\n",
      "   macro avg       0.87      0.75      0.79      2575\n",
      "weighted avg       0.84      0.82      0.81      2575\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare the predictors and target\n",
    "predictors_new = df_multi.drop(columns=[\"region\", \"state\", \"churn\"])\n",
    "target_new = df_multi[\"region\"]\n",
    "\n",
    "# Encode the target variable since many algorithms require numerical input\n",
    "label_encoder = LabelEncoder()\n",
    "target_encoded = label_encoder.fit_transform(target_new)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "    predictors_new,\n",
    "    target_encoded,\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    stratify=target_encoded,\n",
    ")\n",
    "\n",
    "# Update the preprocessor to exclude 'state'\n",
    "quanti_features_new = predictors_new.iloc[:, :13].columns.tolist()\n",
    "cat_ohe_features_new = predictors_new.iloc[:, 15:].columns.tolist()\n",
    "\n",
    "preprocessor_new = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), quanti_features_new),\n",
    "        (\"cat_ohe\", OneHotEncoder(), cat_ohe_features_new),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipeline_new = Pipeline([(\"preprocessor\", preprocessor_new)])\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_processed = pipeline_new.fit_transform(X_train_new)\n",
    "X_test_processed = pipeline_new.transform(X_test_new)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "results_new = {}\n",
    "for name, clf in classifiers:\n",
    "    clf.fit(X_train_processed, y_train_new)\n",
    "    y_train_pred_new = clf.predict(X_train_processed)\n",
    "    y_test_pred_new = clf.predict(X_test_processed)\n",
    "\n",
    "    print(f\"Classifier: {name}\")\n",
    "    print(\"Training Set Performance:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_train_new, y_train_pred_new, target_names=label_encoder.classes_\n",
    "        )\n",
    "    )\n",
    "    print(\"Test Set Performance:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_test_new, y_test_pred_new, target_names=label_encoder.classes_\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6de2de",
   "metadata": {},
   "source": [
    "Classifier: Logistic Regression\n",
    "Training Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        East       0.55      1.00      0.71      5615\n",
    "         Mid       0.00      0.00      0.00      2446\n",
    "        West       0.00      0.00      0.00      2237\n",
    "\n",
    "    accuracy                           0.55     10298\n",
    "   macro avg       0.18      0.33      0.24     10298\n",
    "weighted avg       0.30      0.55      0.38     10298\n",
    "\n",
    "Test Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        East       0.55      1.00      0.71      1404\n",
    "         Mid       0.00      0.00      0.00       612\n",
    "        West       0.00      0.00      0.00       559\n",
    "\n",
    "    accuracy                           0.55      2575\n",
    "   macro avg       0.18      0.33      0.24      2575\n",
    "weighted avg       0.30      0.55      0.38      2575\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Classifier: KNN\n",
    "Training Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        East       0.86      0.93      0.89      5615\n",
    "         Mid       0.86      0.80      0.83      2446\n",
    "        West       0.86      0.76      0.81      2237\n",
    "\n",
    "    accuracy                           0.86     10298\n",
    "   macro avg       0.86      0.83      0.84     10298\n",
    "weighted avg       0.86      0.86      0.86     10298\n",
    "\n",
    "Test Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        East       0.70      0.82      0.75      1404\n",
    "         Mid       0.57      0.51      0.54       612\n",
    "        West       0.62      0.42      0.50       559\n",
    "\n",
    "    accuracy                           0.66      2575\n",
    "   macro avg       0.63      0.58      0.60      2575\n",
    "weighted avg       0.65      0.66      0.65      2575\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Classifier: Decision Trees\n",
    "Training Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        East       1.00      1.00      1.00      5615\n",
    "         Mid       1.00      1.00      1.00      2446\n",
    "        West       1.00      1.00      1.00      2237\n",
    "\n",
    "    accuracy                           1.00     10298\n",
    "   macro avg       1.00      1.00      1.00     10298\n",
    "weighted avg       1.00      1.00      1.00     10298\n",
    "\n",
    "Test Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        East       0.85      0.82      0.84      1404\n",
    "         Mid       0.75      0.76      0.75       612\n",
    "        West       0.70      0.74      0.72       559\n",
    "\n",
    "    accuracy                           0.79      2575\n",
    "   macro avg       0.77      0.77      0.77      2575\n",
    "weighted avg       0.79      0.79      0.79      2575\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Classifier: Random Forest\n",
    "Training Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        East       1.00      1.00      1.00      5615\n",
    "         Mid       1.00      1.00      1.00      2446\n",
    "        West       1.00      1.00      1.00      2237\n",
    "\n",
    "    accuracy                           1.00     10298\n",
    "   macro avg       1.00      1.00      1.00     10298\n",
    "weighted avg       1.00      1.00      1.00     10298\n",
    "\n",
    "Test Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        East       0.85      0.99      0.92      1404\n",
    "         Mid       0.99      0.79      0.88       612\n",
    "        West       0.99      0.79      0.88       559\n",
    "\n",
    "    accuracy                           0.90      2575\n",
    "   macro avg       0.94      0.86      0.89      2575\n",
    "weighted avg       0.91      0.90      0.90      2575\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Classifier: SVM\n",
    "Training Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        East       0.57      1.00      0.72      5615\n",
    "         Mid       0.96      0.10      0.18      2446\n",
    "        West       0.95      0.07      0.13      2237\n",
    "\n",
    "    accuracy                           0.58     10298\n",
    "   macro avg       0.83      0.39      0.34     10298\n",
    "weighted avg       0.75      0.58      0.47     10298\n",
    "\n",
    "Test Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        East       0.56      1.00      0.72      1404\n",
    "         Mid       0.89      0.05      0.10       612\n",
    "        West       0.86      0.04      0.09       559\n",
    "\n",
    "    accuracy                           0.57      2575\n",
    "   macro avg       0.77      0.36      0.30      2575\n",
    "weighted avg       0.70      0.57      0.43      2575\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Classifier: XGBooster\n",
    "Training Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        East       0.95      1.00      0.97      5615\n",
    "         Mid       0.99      0.93      0.96      2446\n",
    "        West       0.99      0.92      0.96      2237\n",
    "\n",
    "    accuracy                           0.97     10298\n",
    "   macro avg       0.98      0.95      0.96     10298\n",
    "weighted avg       0.97      0.97      0.97     10298\n",
    "\n",
    "Test Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        East       0.78      0.97      0.87      1404\n",
    "         Mid       0.92      0.67      0.78       612\n",
    "        West       0.90      0.61      0.73       559\n",
    "\n",
    "    accuracy                           0.82      2575\n",
    "   macro avg       0.87      0.75      0.79      2575\n",
    "weighted avg       0.84      0.82      0.81      2575\n",
    "\n",
    "--------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f319b63",
   "metadata": {},
   "source": [
    "Logistic regression and SVM perform relatively poorly on this multi-class classification problem.\n",
    "Decision trees and random forests both perform 100% on the training set, but the performance on the test set drops slightly, which could be a sign of overfitting.\n",
    "XGBooster performs well on both the training and test sets and seems to be a good choice for this multi-class classification problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
