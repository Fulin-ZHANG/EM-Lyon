{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arnY8EH0RHWH"
   },
   "source": [
    "# Text Classification for the IMDB Dataset using BERT.\n",
    "\n",
    "**Objective:** classify the IMDB Reviews into positive or negative. <br>\n",
    "In this notebook we explore [BERT](https://arxiv.org/abs/1810.04805), a pre-Trained NLP model by google released in 2018. <br> \n",
    "We use the [Transfer Learning](https://towardsdatascience.com/transfer-learning-in-nlp-fecc59f546e4) procedure by loading a pre-trained BERT and refining its weights on our own textual dataset. <br>\n",
    "The dataset can be downloaded from [Kaggle](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).<br>\n",
    "This notebook requires a GPU; you can use google colab if your local machine has none. <br>\n",
    "**Useful documentation:** <br>\n",
    "[Pytorch Pretrained Model](https://huggingface.co/transformers/v1.0.0/quickstart.html) <br>\n",
    "[Pytorch DataLoader](https://pytorch.org/docs/stable/data.html) <br>\n",
    "[BERT Adam Optimiser](https://huggingface.co/transformers/v1.0.0/migration.html)<br>\n",
    "[Pytorch BERT For Sequence Classification](https://huggingface.co/transformers/v3.0.2/model_doc/bert.html) <br>\n",
    "\n",
    "# Installation of needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "MK-POIlJE0Eu",
    "outputId": "ecdc6f83-2d57-4930-f520-e094e2499e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_pretrained_bert==0.6.2\n",
      "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl.metadata (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytorch-nlp==0.5.0\n",
      "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: torch>=0.4.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from pytorch_pretrained_bert==0.6.2) (2.1.2)\n",
      "Requirement already satisfied: numpy in /Users/fulin/anaconda3/lib/python3.11/site-packages (from pytorch_pretrained_bert==0.6.2) (1.26.4)\n",
      "Collecting boto3 (from pytorch_pretrained_bert==0.6.2)\n",
      "  Downloading boto3-1.34.69-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: requests in /Users/fulin/anaconda3/lib/python3.11/site-packages (from pytorch_pretrained_bert==0.6.2) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/fulin/anaconda3/lib/python3.11/site-packages (from pytorch_pretrained_bert==0.6.2) (4.65.0)\n",
      "Requirement already satisfied: regex in /Users/fulin/anaconda3/lib/python3.11/site-packages (from pytorch_pretrained_bert==0.6.2) (2023.10.3)\n",
      "Requirement already satisfied: filelock in /Users/fulin/anaconda3/lib/python3.11/site-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/fulin/anaconda3/lib/python3.11/site-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.2) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/fulin/anaconda3/lib/python3.11/site-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.2) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/fulin/anaconda3/lib/python3.11/site-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.2) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/fulin/anaconda3/lib/python3.11/site-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.6.2) (2023.10.0)\n",
      "Collecting botocore<1.35.0,>=1.34.69 (from boto3->pytorch_pretrained_bert==0.6.2)\n",
      "  Downloading botocore-1.34.69-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from boto3->pytorch_pretrained_bert==0.6.2) (1.0.1)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch_pretrained_bert==0.6.2)\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests->pytorch_pretrained_bert==0.6.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests->pytorch_pretrained_bert==0.6.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests->pytorch_pretrained_bert==0.6.2) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests->pytorch_pretrained_bert==0.6.2) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.69->boto3->pytorch_pretrained_bert==0.6.2) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert==0.6.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert==0.6.2) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.69->boto3->pytorch_pretrained_bert==0.6.2) (1.16.0)\n",
      "Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.34.69-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.34.69-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytorch-nlp, botocore, s3transfer, boto3, pytorch_pretrained_bert\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.64\n",
      "    Uninstalling botocore-1.31.64:\n",
      "      Successfully uninstalled botocore-1.31.64\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.7.0 requires botocore<1.31.65,>=1.31.16, but you have botocore 1.34.69 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.34.69 botocore-1.34.69 pytorch-nlp-0.5.0 pytorch_pretrained_bert-0.6.2 s3transfer-0.10.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy==1.19.5\n",
    "# !pip install tensorflow==1.14.0\n",
    "# !pip install torch==1.9.0\n",
    "# !pip install scikit-learn==0.21.3\n",
    "# !pip install pytorch_pretrained_bert==0.6.2 pytorch-nlp==0.5.0     \n",
    "# !pip install tqdm==4.41.1\n",
    "# !pip install pandas==1.1.5\n",
    "# !pip install matplotlib==3.2.2\n",
    "# !pip install beautifulsoup4==4.6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TtokjlkCQbiw"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# specify GPU device\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "if torch.cuda.is_available():\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yynlzmp6d33G",
    "outputId": "676683be-9fa7-46b9-a618-86274435c788"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\",engine='python') #, error_bad_lines=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnozotyVMBkn",
    "outputId": "17150186-522e-4c66-b8d9-05afb8bd6ce2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(review       0\n",
       " sentiment    0\n",
       " dtype: int64,\n",
       " {'negative', 'positive'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum(), set(df.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "MD5sedPwN7tX",
    "outputId": "0db376ab-cb03-49ab-d28f-f3b2e8fbc513"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AzOdtJlCRAfb"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#sentiment is positive and negative we need to convert it to 0,1\n",
    "le = LabelEncoder()\n",
    "df[\"sentiment\"] = le.fit_transform(df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dop1ppy6XQYn",
    "outputId": "90752ab4-431b-4e58-88da-1f73893ee6c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    25000\n",
       "0    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        0\n",
       "4        1\n",
       "        ..\n",
       "49995    1\n",
       "49996    0\n",
       "49997    0\n",
       "49998    0\n",
       "49999    0\n",
       "Name: sentiment, Length: 50000, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "azXUgzdKXqX3"
   },
   "outputs": [],
   "source": [
    "#cleaning the text\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def strip(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = re.sub('\\[[^]]*\\]', '', soup.get_text())\n",
    "    pattern=r\"[^a-zA-Z\\s]\"#r\"[^a-zA-Z0-9\\s,']\"\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "df['review']=df['review'].apply(strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "BiRlwEJ2WxW-",
    "outputId": "b9706058-50e8-45e2-dc26-ddf2f4932dae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bhdezhhfezff hbffk'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip('bhdezhhfez1235,,,,ff hbffk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "va7N2bOMjjKs",
    "outputId": "cc974fa8-ad35-4f09-aa1f-0f8f64dc3775"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically theres a family where a little boy J...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Matteis Love in the Time of Money is a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production The filming tech...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically theres a family where a little boy J...          0\n",
       "4  Petter Matteis Love in the Time of Money is a ...          1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYHojJ9Kr_5w"
   },
   "source": [
    "# Adapting our input corpus to BERT's requirements\n",
    "BERT expects input data in a specific format\n",
    "1. We are performing a classification task so we use a special token [CLS] to indicate this to BERT.\n",
    "2. It needs to know the end of a sentence so we use the [SEP] token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-yCoUW-FZtD7"
   },
   "outputs": [],
   "source": [
    "sentences = df['review']\n",
    "sentence = [\"[CLS] \"+i+\" [SEP]\" for i in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "NUpOJpndZw5J",
    "outputId": "cef9dc78-8390-4076-8f4f-822be034eece"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] One of the other reviewers has mentioned that after watching just  Oz episode youll be hooked They are right as this is exactly what happened with meThe first thing that struck me about Oz was its brutality and unflinching scenes of violence which set in right from the word GO Trust me this is not a show for the faint hearted or timid This show pulls no punches with regards to drugs sex or violence Its is hardcore in the classic use of the wordIt is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda Em City is home to manyAryans Muslims gangstas Latinos Christians Italians Irish and moreso scuffles death stares dodgy dealings and shady agreements are never far awayI would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare Forget pretty pictures painted for mainstream audiences forget charm forget romanceOZ doesnt mess around The first episode I ever saw struck me as so nasty it was surreal I couldnt say I was ready for it but as I watched more I developed a taste for Oz and got accustomed to the high levels of graphic violence Not just violence but injustice crooked guards wholl be sold out for a nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5A_V7BT2bKM"
   },
   "source": [
    "# Tokenizing text\n",
    "We need to tokenize our text into tokens that correspond to BERT’s vocabulary --> BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vjfr85BSbY41",
    "outputId": "dc4ace42-150d-46bb-fd4e-d9a223bc1bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', 'oz', 'episode', 'you', '##ll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'met', '##he', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'un', '##fl', '##in', '##ching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'tim', '##id', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', '##it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'pen', '##ite', '##nta', '##ry', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inward', '##s', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', '##ary', '##ans', 'muslims', 'gangs', '##tas', 'latino', '##s', 'christians', 'italians', 'irish', 'and', 'more', '##so', 'sc', '##uf', '##fles', 'death', 'stares', 'dod', '##gy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', '##i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn', '##t', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', '##oz', 'doesn', '##t', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'couldn', '##t', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', '##ll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', '##ll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'manner', '##ed', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitch', '##es', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', '##tha', '##ts', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize with BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Restricting the max size of Tokens to 512(BERT doest accept any more than this)\n",
    "tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:512] , sentence))\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1u24cpd2mBl"
   },
   "source": [
    "# First step of vectorization: converting words into their ids using BERT vcabulary\n",
    "The id of a token (word) is a sequence of integers which uniquely identify this token to this index number. <br>\n",
    "Thus we call the method \"convert_tokens_to_ids\" of the BertTokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vgssRTWCc_Xl",
    "outputId": "25018bcb-215c-4c96-f47e-0de45d3bcb5a"
   },
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. \n",
    "MAX_LEN = 128\n",
    "\n",
    "# Pad our input tokens so that everything has a uniform length\n",
    "input_ids = list(map(tokenizer.convert_tokens_to_ids, tokenized_texts))\n",
    "# or input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "841lYnXcxQW1",
    "outputId": "3ad109cb-a9f3-43df-a604-130b5259548d"
   },
   "source": [
    "Since we are dealing with a classical ML/DL model, input dimension should always be fixed. <br>\n",
    "As in a traditional ML model, the number of attributes/features/columns should be fixed, in a DL model, the input dimension should be fixed as well. <br>\n",
    "In our case, the input features are sentences i.e. list of words. In order to make sure that the input has a fixed size, i.e. the sentences having the same size, we will need to fix a max length (MAX_LEN) parameter, which is the maximum number of words composing a sentence. <br>\n",
    "You might ask yourselves, But every sentence has a different set of words, shouldn't we create an input size that is equal to the number of unique words in our corpus? <br>\n",
    "The answer is No, because, we will never deal with words, we will deal with embeddings such that all words are embedded with vectors having the same dimension $d$ &rarr; every sentence of our corpus will be transformed into an input of size MAX_LEN $\\times d$ &rarr; our input will have the same size. <br>\n",
    "Thus: <br>\n",
    "- sentences with number of words > than MAX_LEN will be truncated; we chose a post truncating i.e., the first MAX_LEN are retained and the remaining words are removed. \n",
    "- sentences with number of words < than MAX_LEN will be padded; we chose a post padding i.e., the 0 id will be added after the ids of the words present in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DQTrbT5Vydwe"
   },
   "outputs": [],
   "source": [
    "input_ids = pad_sequences(input_ids,\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8utMcMS1CMp"
   },
   "source": [
    "Let us look at a sentence with words < MAX_LEN (the sentence max length) and see what the padding does exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3EEAhwA0TZ7",
    "outputId": "4369affa-ec62-4e32-8e81-e0147131215a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413\n",
      "1417\n",
      "1783\n",
      "2895\n",
      "3709\n",
      "4304\n",
      "6195\n",
      "9435\n",
      "9744\n",
      "10973\n",
      "11926\n",
      "13109\n",
      "13315\n",
      "14420\n",
      "14916\n",
      "15009\n",
      "18400\n",
      "18424\n",
      "19089\n",
      "19669\n",
      "19874\n",
      "20053\n",
      "20274\n",
      "21042\n",
      "24940\n",
      "27521\n",
      "27680\n",
      "28634\n",
      "28920\n",
      "30598\n",
      "30763\n",
      "30926\n",
      "30995\n",
      "31072\n",
      "31087\n",
      "31328\n",
      "31761\n",
      "32935\n",
      "33907\n",
      "33967\n",
      "34501\n",
      "34692\n",
      "34817\n",
      "35821\n",
      "36008\n",
      "36088\n",
      "36368\n",
      "36484\n",
      "39182\n",
      "40038\n",
      "40817\n",
      "40888\n",
      "41377\n",
      "42319\n",
      "42356\n",
      "42626\n",
      "43477\n",
      "43607\n",
      "44126\n",
      "44962\n",
      "45359\n",
      "45373\n",
      "46512\n",
      "48448\n",
      "48927\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tokenized_texts)):\n",
    "    if len(tokenized_texts[i])<30:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCWzv72p0oqC",
    "outputId": "39ced123-8767-49bc-d170-9ba4c7dc4957"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1037,  5790,  1997,  2515,  2025,  4088,  2000,  4671,\n",
       "        2129, 10634,  2139, 24128,  1998, 21660,  2135,  2919,  2023,\n",
       "        3185,  2003,   102,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[413]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sentence 0: 21\n",
      "\n",
      "the first 20 words of the sentence are:\n",
      "['[CLS]', 'a', 'rating', 'of', 'does', 'not', 'begin', 'to', 'express', 'how', 'dull', 'de', '##pressing', 'and', 'relentless', '##ly', 'bad', 'this', 'movie', 'is']\n",
      "\n",
      "the first 20 words of the sentence represented by their indices are:\n",
      "[  101  1037  5790  1997  2515  2025  4088  2000  4671  2129 10634  2139\n",
      " 24128  1998 21660  2135  2919  2023  3185  2003]\n",
      "\n",
      "length of sentence 0: 21 vs length of sentence 1: 128\n",
      "they are different!!! Hence, padding is needed!\n"
     ]
    }
   ],
   "source": [
    "print(\"length of sentence 0:\", len(tokenized_texts[413]))\n",
    "print(\"\\nthe first 20 words of the sentence are:\")\n",
    "print(tokenized_texts[413][:20])\n",
    "print(\"\\nthe first 20 words of the sentence represented by their indices are:\")\n",
    "print(input_ids[413][:20]) \n",
    "print(\"\\nlength of sentence 0:\", len(tokenized_texts[413]), \"vs length of sentence 1:\", len(input_ids[413]))\n",
    "print(\"they are different!!! Hence, padding is needed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StJqWj6qw8Kw",
    "outputId": "73bfa036-52c9-4cc3-8022-3e917956760e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding: \n",
      " length of sentence 0: 128 vs length of sentence 1: 128\n",
      "Now all sentences have equal lengths, such that this length is the MAX_LEN variable defined by us: 128\n"
     ]
    }
   ],
   "source": [
    "print(\"After padding: \\n length of sentence 0:\", len(input_ids[0]), \"vs length of sentence 1:\", len(input_ids[1]))\n",
    "print(\"Now all sentences have equal lengths, such that this length is the MAX_LEN variable defined by us:\", MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbHLewX_24ef"
   },
   "source": [
    "# Masked Language Model (MLM)\n",
    "The first task in pre-training BERT is the MLM. <br>\n",
    "Words that can be masked are all words that are not padding i.e., words with id!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uDp1d_FIFhKz"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evbWDiQ73QZj"
   },
   "source": [
    "# Training\n",
    "## Split the dataset into train, validation, and test sets\n",
    "1. Split the data into train (70%), validation (20%), and test (10%). \n",
    "2. Convert it to tensors. \n",
    "3. Create a data iterator using pytorch DataLoader for each of the sets.\n",
    "**NB:** The code below should be changed for the split should be done twice, at two stages: <br>\n",
    "1. split the dataset into train and test\n",
    "2. split the train into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wT7bDRYEFhPJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Select a batch size for training. \n",
    "# batch_size = 16\n",
    "# labels = list(df['sentiment'])\n",
    "# # Use train_test_split to split our data into train and validation sets for training\n",
    "# train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "#                                                             random_state=2018, test_size=0.2)\n",
    "# train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "#                                              random_state=2018, test_size=0.2)\n",
    "                                             \n",
    "# # Convert all of our data into torch tensors, the required datatype for our model\n",
    "# train_inputs = torch.tensor(train_inputs)\n",
    "# validation_inputs = torch.tensor(validation_inputs)\n",
    "# train_labels = torch.tensor(train_labels)\n",
    "# validation_labels = torch.tensor(validation_labels)\n",
    "# train_masks = torch.tensor(train_masks)\n",
    "# validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "# # Create an iterator of our data with torch DataLoader \n",
    "# train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "# validation_sampler = SequentialSampler(validation_data)\n",
    "# validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. \n",
    "batch_size = 16\n",
    "labels = list(df['sentiment'])\n",
    "\n",
    "# First, split the dataset into train+validation and test sets\n",
    "train_val_inputs, test_inputs, train_val_labels, test_labels = train_test_split(\n",
    "    input_ids, labels, random_state=2018, test_size=0.1)\n",
    "\n",
    "# Now split the train_val dataset into train and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    train_val_inputs, train_val_labels, random_state=2018, test_size=(20/90))\n",
    "\n",
    "# Similarly, split the attention masks\n",
    "train_val_masks, test_masks, _, _ = train_test_split(\n",
    "    attention_masks, input_ids, random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(\n",
    "    train_val_masks, train_val_inputs, random_state=2018, test_size=(20/90))\n",
    "                                             \n",
    "# Convert all data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "\n",
    "# Create an iterator for each dataset with torch DataLoader \n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre trained BERT\n",
    "Since our problem is a classification one, we will load the BERT model that was fine-tuned for a similar classification task. <br>\n",
    "Pytorch has fine-tuned BERT for several tasks: for BertForMultipleChoice,  BertForSequenceClassification, etc. <br>\n",
    "In our case, we do a classification, thus we will use BertForSequenceClassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0xRtmpSlFhSP",
    "outputId": "fb17e720-7c48-4e00-c0b4-8695f68d2c31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)#binary classification\n",
    "if torch.cuda.is_available():\n",
    "    print(model.cuda())\n",
    "else:\n",
    "    print(model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzxxuH_6310Y"
   },
   "source": [
    "# Fine-Tuning BERT with our specific task\n",
    "The training could take around an hour and a half to run on Colab with a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "id": "eyXoiBmJFhIt",
    "outputId": "570e06c6-f04b-4717-8461-10f5f01f1952"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|          | 0/4 [23:53<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m   loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Update parameters and take a step using the computed gradient\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Update tracking variables\u001b[39;00m\n\u001b[1;32m     55\u001b[0m   tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_pretrained_bert/optimization.py:271\u001b[0m, in \u001b[0;36mBertAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Add grad clipping\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_grad_norm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 271\u001b[0m     clip_grad_norm_(p, group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_grad_norm\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[1;32m    275\u001b[0m next_m\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1, grad)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:69\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe total norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for gradients from \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset `error_if_nonfinite=False`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m clip_coef \u001b[38;5;241m=\u001b[39m max_norm \u001b[38;5;241m/\u001b[39m (total_norm \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Note: multiplying by the clamped coef is redundant when the coef is clamped to 1, but doing so\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# avoids a `if clip_coef < 1:` conditional which can require a CPU <=> device synchronization\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# when the gradients do not reside in CPU memory.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m clip_coef_clamped \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(clip_coef, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:913\u001b[0m, in \u001b[0;36mTensor.__rdiv__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rdiv__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreciprocal() \u001b[38;5;241m*\u001b[39m other\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(#model.parameters(),#\n",
    "                     optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten() # Returns the indices of the maximum values along an axis.\n",
    "    labels_flat = labels.flatten()# Converts an N dimensional array to a 1D array\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat) # accuracy\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "# Number of training epochs \n",
    "epochs = 4\n",
    "\n",
    "# BERT training loop\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "  \n",
    "  ## TRAINING\n",
    "  \n",
    "  # Set our model to training mode\n",
    "    model.train()  \n",
    "  # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "      # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "      # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "      # Forward pass\n",
    "      #https://huggingface.co/transformers/v3.0.2/model_doc/bert.html\n",
    "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        train_loss_set.append(loss.item())    \n",
    "      # Backward pass\n",
    "        loss.backward()\n",
    "      # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "      # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "       \n",
    "  ## VALIDATION\n",
    "\n",
    "  # Put model in evaluation mode\n",
    "    model.eval()\n",
    "  # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "  # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "      # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "      # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "      # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "          logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n",
    "      # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "\n",
    "# plot training performance\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4cQWvpq97Z0"
   },
   "source": [
    "## Test the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in evaluation mode to evaluate loss on the test set\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables\n",
    "test_accuracy = 0\n",
    "nb_test_steps = 0\n",
    "nb_test_examples = 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up test\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n",
    "    tmp_test_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    test_accuracy += tmp_test_accuracy\n",
    "    nb_test_examples += b_input_ids.size(0)\n",
    "    nb_test_steps += 1\n",
    "\n",
    "# Print the final accuracy for this test run.\n",
    "print(\"Test Accuracy: {}\".format(test_accuracy/nb_test_steps))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
