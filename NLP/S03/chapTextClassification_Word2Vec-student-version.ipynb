{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVtvH58nb_Hp"
   },
   "source": [
    "# Word2Vec for Text Classification\n",
    "\n",
    "In this short notebook, we will see an example of how to use a pre-trained Word2vec model for doing feature extraction and performing text classification.\n",
    "\n",
    "We will use the sentiment labelled sentences dataset from UCI repository\n",
    "http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    "The dataset consists of 1500 positive, and 1500 negative sentiment sentences from Amazon, Yelp, IMDB. Let us first combine all the three separate data files into one using the following unix command:\n",
    "\n",
    "```cat amazon_cells_labelled.txt imdb_labelled.txt yelp_labelled.txt > sentiment_sentences.txt```\n",
    "\n",
    "For a pre-trained embedding model, we will use the Google News vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JQX8DAmBb_Hr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/fulin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/fulin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#basic imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import wget\n",
    "import gzip\n",
    "import shutil\n",
    "from time import time\n",
    "\n",
    "#pre-processing imports\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "#imports related to modeling\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vta6y9ZN6IhN"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G7ptp19a6y6_"
   },
   "outputs": [],
   "source": [
    "#Read text data, categories.\n",
    "#the file path consists of tab separated sentences and categories.\n",
    "texts = []\n",
    "categories = []\n",
    "fh = open('./Data/sentiment_sentences.txt')\n",
    "for line in fh:\n",
    "    text, sentiment = line.split(\"\\t\")\n",
    "    texts.append(text)\n",
    "    categories.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yQdoBaBL67Ic"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3000\n",
      "the category of text 1: ' Good case, Excellent value. ' is  1\n",
      "\n",
      "The set of categories are:  {'0\\n', '1\\n'}\n"
     ]
    }
   ],
   "source": [
    "#Inspect the dataset\n",
    "print(len(categories), len(texts))\n",
    "print(\"the category of text 1: '\",texts[1], \"' is \", categories[1])\n",
    "print(\"The set of categories are: \", set(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rV-BNwX27Hmd"
   },
   "source": [
    "## Loading the word2vec model\n",
    "For that task use KeyedVectors imported from the gensim library. <br>\n",
    "* Load the pre-trained google news 300 word2vec model to the variable w2v_model\n",
    "* Inspect the model by checking the words inside w2v_model: the number of words\n",
    "\n",
    "**NB:**<br>\n",
    "The magic command *%time* used returns the computational cost of the operation following it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31018,
     "status": "ok",
     "timestamp": 1644768745258,
     "user": {
      "displayName": "Waad Masri",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12332609461921223513"
     },
     "user_tz": -60
    },
    "id": "_2rMsyQ02N9g",
    "outputId": "dae62081-e080-4c89-b2ae-639ecc95e6fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 667 ms, total: 18.5 s\n",
      "Wall time: 19 s\n",
      "done loading Word2Vec\n"
     ]
    }
   ],
   "source": [
    "#Load W2V model. This will take some time.\n",
    "%time w2v_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "print('done loading Word2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3078,
     "status": "ok",
     "timestamp": 1644770176993,
     "user": {
      "displayName": "Waad Masri",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12332609461921223513"
     },
     "user_tz": -60
    },
    "id": "m-WjFyC6b_IE",
    "outputId": "30448859-eae0-4cda-df8d-0b797fc93826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000 2702150\n"
     ]
    }
   ],
   "source": [
    "#Inspect the model\n",
    "word2vec_vocab = list(w2v_model.key_to_index.keys())\n",
    "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
    "print(len(set(word2vec_vocab)), len(set(word2vec_vocab_lower)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMKkFZL_9oJx"
   },
   "source": [
    "## Preprocessing text\n",
    "* Remove the stopwords\n",
    "* Remove punctuations\n",
    "* Remove digits\n",
    "* Convert all text to lower case\n",
    "* Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MFOGaDTwb_Ig"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3000\n",
      "good case excellent value\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#preprocess the text.\n",
    "\n",
    "import string\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_texts = []\n",
    "\n",
    "    for text in texts:\n",
    "        text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        words_filtered = [word for word in words if word not in stop_words and not word.isdigit()]\n",
    "        processed_texts.append(' '.join(words_filtered))\n",
    "\n",
    "    return processed_texts\n",
    "\n",
    "texts_processed = preprocess_corpus(texts)\n",
    "print(len(categories), len(texts_processed))\n",
    "print(texts_processed[1])\n",
    "print(categories[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv3CyuIv9nkY"
   },
   "source": [
    "## Converting text to numeric\n",
    "In this section, we will convet the text to numerical data to be fed into a Machine Learning model for classification. <br>\n",
    "We will extract the embeddings of words using w2v_model. <br>\n",
    "Finally, every sentence is the average of the embeddings of its constituting words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fXRiGtY1b_Iq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "# Creating a feature vector by averaging all embeddings for all sentences\n",
    "def embedding_feats(list_of_lists):\n",
    "    DIMENSION = 300\n",
    "    zero_vector = np.zeros(DIMENSION)\n",
    "    features = []\n",
    "    for tokens in list_of_lists:\n",
    "        zero_vector = np.zeros(DIMENSION)\n",
    "        vectors = 0\n",
    "        for token in tokens.split():\n",
    "            if token in w2v_model:\n",
    "                zero_vector += w2v_model[token]\n",
    "                vectors += 1\n",
    "        if vectors > 0:\n",
    "            zero_vector /= vectors\n",
    "        features.append(zero_vector)\n",
    "    return features\n",
    "\n",
    "\n",
    "embedded_texts = embedding_feats(texts_processed)\n",
    "print(len(embedded_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3vOSvA8-m5M"
   },
   "source": [
    "## Text Classification\n",
    "For this example, we will use a simple Logistic Regression to classify the text.<br>\n",
    "* Initialize a Logistic Regression model\n",
    "* Split the embedded_texts and categories to train and test data and target\n",
    "* Fit the model with the training data samples\n",
    "* Predict on the test data samples\n",
    "* Print the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [int(category.strip()) for category in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(embedded_texts, categories, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mr9IaQppb_Ix"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       299\n",
      "           1       0.81      0.81      0.81       301\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.81      0.81      0.81       600\n",
      "weighted avg       0.81      0.81      0.81       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "chapTextClassification_Word2Vec-student-version.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
