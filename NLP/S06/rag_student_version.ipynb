{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
      "metadata": {
        "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe"
      },
      "source": [
        "# RAG: retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
      "metadata": {
        "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.15-py3-none-any.whl.metadata (621 bytes)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.12-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.57.tar.gz (36.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.9/36.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from langchain_community) (2.0.25)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from langchain_community) (3.9.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.31 (from langchain_community)\n",
            "  Downloading langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
            "  Downloading langsmith-0.1.29-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from langchain_community) (8.2.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from tiktoken) (2023.10.3)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.31.0.20240311-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from chromadb) (2.6.3)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.28.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from chromadb) (4.9.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp311-cp311-macosx_10_15_universal2.whl.metadata (1.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.1-cp311-cp311-macosx_11_0_universal2.whl.metadata (4.2 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.23.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.23.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from chromadb) (0.15.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from chromadb) (4.65.0)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from chromadb) (7.4.0)\n",
            "Collecting importlib-resources (from chromadb)\n",
            "  Downloading importlib_resources-6.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from chromadb) (1.62.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-macosx_10_12_universal2.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain_community)\n",
            "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
            "Collecting orjson>=3.9.12 (from chromadb)\n",
            "  Downloading orjson-3.9.15-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m880.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.37.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: scikit-learn in /Users/fulin/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.4.1.post1)\n",
            "Requirement already satisfied: scipy in /Users/fulin/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /Users/fulin/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (10.2.0)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\n",
            "Requirement already satisfied: packaging>=19.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (23.1)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.0.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: filelock in /Users/fulin/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.28.1)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (0.58.0)\n",
            "Requirement already satisfied: requests-oauthlib in /Users/fulin/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: anyio<5,>=3 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.31->langchain_community) (4.2.0)\n",
            "Collecting packaging>=19.0 (from build>=1.0.3->chromadb)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /Users/fulin/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /Users/fulin/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
            "Requirement already satisfied: sympy in /Users/fulin/anaconda3/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.23.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.23.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting opentelemetry-proto==1.23.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (68.2.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.4)\n",
            "Requirement already satisfied: networkx in /Users/fulin/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain_community) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
            "Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
            "\u001b[?25hDownloading tiktoken-0.6.0-cp311-cp311-macosx_11_0_arm64.whl (949 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading langchainhub-0.1.15-py3-none-any.whl (4.6 kB)\n",
            "Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp311-cp311-macosx_11_0_arm64.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.7/198.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.1.12-py3-none-any.whl (809 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.1.2-cp39-abi3-macosx_10_12_universal2.whl (528 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.5/528.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hDownloading build-1.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
            "\u001b[?25hDownloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Downloading langsmith-0.1.29-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-4.1.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
            "Downloading onnxruntime-1.17.1-cp311-cp311-macosx_11_0_universal2.whl (14.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.23.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.23.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.23.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.23.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl (28 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl (14 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl (36 kB)\n",
            "Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.23.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m698.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.9.15-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (248 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.6/248.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pulsar_client-3.4.0-cp311-cp311-macosx_10_15_universal2.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading types_requests-2.31.0.20240311-py3-none-any.whl (14 kB)\n",
            "Downloading uvicorn-0.28.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_resources-6.3.1-py3-none-any.whl (35 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.1/229.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp311-cp311-macosx_10_9_universal2.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.9/145.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m555.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.19.0-cp311-cp311-macosx_10_9_universal2.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.21.0-cp311-cp311-macosx_11_0_arm64.whl (418 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.2/418.2 kB\u001b[0m \u001b[31m819.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp311-cp311-macosx_11_0_arm64.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m664.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python, pypika\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for llama-cpp-python \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[65 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.8.2\u001b[0m using \u001b[94mCMake 3.28.3\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n",
            "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
            "  \u001b[31m   \u001b[0m 2024-03-19 16:09:57,251 - scikit_build_core - WARNING - libdir/ldlibrary: /Users/fulin/anaconda3/lib/libpython3.11.a is not a real file!\n",
            "  \u001b[31m   \u001b[0m 2024-03-19 16:09:57,251 - scikit_build_core - WARNING - Can't find a Python library, got libdir=/Users/fulin/anaconda3/lib, ldlibrary=libpython3.11.a, multiarch=darwin, masd=None\n",
            "  \u001b[31m   \u001b[0m loading initial cache file /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/CMakeInit.txt\n",
            "  \u001b[31m   \u001b[0m -- The C compiler identification is AppleClang 15.0.0.15000309\n",
            "  \u001b[31m   \u001b[0m -- The CXX compiler identification is AppleClang 15.0.0.15000309\n",
            "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info\n",
            "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info - done\n",
            "  \u001b[31m   \u001b[0m -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
            "  \u001b[31m   \u001b[0m -- Detecting C compile features\n",
            "  \u001b[31m   \u001b[0m -- Detecting C compile features - done\n",
            "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info\n",
            "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info - done\n",
            "  \u001b[31m   \u001b[0m -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
            "  \u001b[31m   \u001b[0m -- Detecting CXX compile features\n",
            "  \u001b[31m   \u001b[0m -- Detecting CXX compile features - done\n",
            "  \u001b[31m   \u001b[0m -- Found Git: /usr/bin/git (found version \"2.39.3 (Apple Git-146)\")\n",
            "  \u001b[31m   \u001b[0m -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  \u001b[31m   \u001b[0m -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  \u001b[31m   \u001b[0m -- Found Threads: TRUE\n",
            "  \u001b[31m   \u001b[0m -- Accelerate framework found\n",
            "  \u001b[31m   \u001b[0m -- Metal framework found\n",
            "  \u001b[31m   \u001b[0m -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n",
            "  \u001b[31m   \u001b[0m -- CMAKE_SYSTEM_PROCESSOR: arm64\n",
            "  \u001b[31m   \u001b[0m -- ARM detected\n",
            "  \u001b[31m   \u001b[0m -- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E\n",
            "  \u001b[31m   \u001b[0m -- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed\n",
            "  \u001b[31m   \u001b[0m \u001b[33mCMake Warning (dev) at vendor/llama.cpp/CMakeLists.txt:1218 (install):\n",
            "  \u001b[31m   \u001b[0m   Target llama has RESOURCE files but no RESOURCE DESTINATION.\n",
            "  \u001b[31m   \u001b[0m This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "  \u001b[31m   \u001b[0m \u001b[0m\n",
            "  \u001b[31m   \u001b[0m \u001b[33mCMake Warning (dev) at CMakeLists.txt:21 (install):\n",
            "  \u001b[31m   \u001b[0m   Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  \u001b[31m   \u001b[0m This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "  \u001b[31m   \u001b[0m \u001b[0m\n",
            "  \u001b[31m   \u001b[0m \u001b[33mCMake Warning (dev) at CMakeLists.txt:30 (install):\n",
            "  \u001b[31m   \u001b[0m   Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  \u001b[31m   \u001b[0m This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "  \u001b[31m   \u001b[0m \u001b[0m\n",
            "  \u001b[31m   \u001b[0m -- Configuring done (3.4s)\n",
            "  \u001b[31m   \u001b[0m -- Generating done (0.3s)\n",
            "  \u001b[31m   \u001b[0m -- Build files have been written to: /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build\n",
            "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1mBuilding project with \u001b[94mNinja\u001b[0m...\u001b[0m\n",
            "  \u001b[31m   \u001b[0m Change Dir: '/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build'\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Run Build Command(s): /Users/fulin/anaconda3/bin/ninja -v\n",
            "  \u001b[31m   \u001b[0m [1/25] cd /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/vendor/llama.cpp && xcrun -sdk macosx metal -O3 -c /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/ggml-metal.metal -o /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/ggml-metal.air && xcrun -sdk macosx metallib /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/ggml-metal.air -o /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/default.metallib && rm -f /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/ggml-metal.air && rm -f /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/ggml-common.h && rm -f /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/ggml-metal.metal\n",
            "  \u001b[31m   \u001b[0m \u001b[31mFAILED: \u001b[0mbin/default.metallib /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/default.metallib\n",
            "  \u001b[31m   \u001b[0m cd /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/vendor/llama.cpp && xcrun -sdk macosx metal -O3 -c /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/ggml-metal.metal -o /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/ggml-metal.air && xcrun -sdk macosx metallib /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/ggml-metal.air -o /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/default.metallib && rm -f /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/ggml-metal.air && rm -f /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/ggml-common.h && rm -f /var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/tmpj7n9mlzs/build/bin/ggml-metal.metal\n",
            "  \u001b[31m   \u001b[0m xcrun: error: unable to find utility \"metal\", not a developer tool or in PATH\n",
            "  \u001b[31m   \u001b[0m [2/25] cd /private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp && /private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-build-env-_hqe1wju/normal/lib/python3.11/site-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=15.0.0.15000309 -DCMAKE_C_COMPILER_ID=AppleClang -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/Library/Developer/CommandLineTools/usr/bin/cc -P /private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n",
            "  \u001b[31m   \u001b[0m -- Found Git: /usr/bin/git (found version \"2.39.3 (Apple Git-146)\")\n",
            "  \u001b[31m   \u001b[0m [3/25] /Library/Developer/CommandLineTools/usr/bin/cc -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/ggml-alloc.c\n",
            "  \u001b[31m   \u001b[0m [4/25] /Library/Developer/CommandLineTools/usr/bin/cc -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -c /private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/ggml-backend.c\n",
            "  \u001b[31m   \u001b[0m [5/25] /Library/Developer/CommandLineTools/usr/bin/c++ -DGGML_USE_METAL -DLLAMA_BUILD -DLLAMA_SHARED -I/private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/examples/llava/. -I/private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/examples/llava/../.. -I/private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/examples/llava/../../common -I/private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/examples/llava/llava.cpp\n",
            "  \u001b[31m   \u001b[0m [6/25] /Library/Developer/CommandLineTools/usr/bin/cc -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-metal.m.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-metal.m.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-metal.m.o -c /private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/ggml-metal.m\n",
            "  \u001b[31m   \u001b[0m [7/25] /Library/Developer/CommandLineTools/usr/bin/cc -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/ggml-quants.c\n",
            "  \u001b[31m   \u001b[0m [8/25] /Library/Developer/CommandLineTools/usr/bin/c++ -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DLLAMA_BUILD -DLLAMA_SHARED -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -c /private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/unicode.cpp\n",
            "  \u001b[31m   \u001b[0m [9/25] /Library/Developer/CommandLineTools/usr/bin/cc -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_ACCELERATE -DGGML_USE_METAL -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -I/private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/ggml.c\n",
            "  \u001b[31m   \u001b[0m [10/25] /Library/Developer/CommandLineTools/usr/bin/c++ -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DLLAMA_BUILD -DLLAMA_SHARED -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/. -F/Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/System/Library/Frameworks -O3 -DNDEBUG -std=gnu++11 -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /private/var/folders/cr/dlw4zhlj7ld9t1grnb2q9hvw0000gn/T/pip-install-vte4atxs/llama-cpp-python_5faaa3cf010549a683a132a41e3d270d/vendor/llama.cpp/llama.cpp\n",
            "  \u001b[31m   \u001b[0m ninja: build stopped: subcommand failed.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \u001b[91m\u001b[1m*** CMake build failed\u001b[0m\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for llama-cpp-python\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=0300d9da1ec25a18f2b998ab294d3c5a35ea3dee25cf61a2021f91544c986b45\n",
            "  Stored in directory: /Users/fulin/Library/Caches/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Failed to build llama-cpp-python\n",
            "\u001b[31mERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install langchain_community tiktoken langchainhub chromadb langchain sentence-transformers llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75a8ab66-8477-429f-bbbe-ba439322d085",
      "metadata": {
        "id": "75a8ab66-8477-429f-bbbe-ba439322d085"
      },
      "source": [
        "`(2) LangSmith`\n",
        "\n",
        "https://docs.smith.langchain.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76f68a8-4745-4377-8057-6090b87377d1",
      "metadata": {
        "id": "b76f68a8-4745-4377-8057-6090b87377d1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = <langsmith-key>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c",
      "metadata": {
        "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c"
      },
      "source": [
        "## Part 1: Overview\n",
        "\n",
        "### References:\n",
        "- [RAG quickstart](https://python.langchain.com/docs/use_cases/question_answering/quickstart)\n",
        "- [WebBaseLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\n",
        "- [SoupStrainer](https://beautiful-soup-4.readthedocs.io/en/latest/index.html?highlight=SoupStrainer#soupstrainer)\n",
        "- [Recursive Character Text Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n",
        "- [Chroma vector database](https://python.langchain.com/docs/integrations/vectorstores/chroma)\n",
        "- [LamaCpp Git repo](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file)\n",
        "- [LamaCpp with Langchain](https://python.langchain.com/docs/integrations/llms/llamacpp)\n",
        "- [LamaCpp docs](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama)\n",
        "- [Callbacks](https://python.langchain.com/docs/modules/callbacks/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98070313-0c2f-4ba6-ae3e-79e2418ce4df",
      "metadata": {
        "id": "98070313-0c2f-4ba6-ae3e-79e2418ce4df"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.language_models import LLM\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "#### INDEXING ####\n",
        "\n",
        "# Load Documents\n",
        "# we are using 'parse_only' to extract from the web page only the parts that are interesting for us\n",
        "# If you open the source code of the page https://lilianweng.github.io/posts/2023-06-23-agent/, you will find three classes are important for us:\n",
        "# the post-header, the post-title, and the post-content.\n",
        "# Thus, we will only extract these.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0z9q0mCH8o2q",
      "metadata": {
        "id": "0z9q0mCH8o2q"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "# Initialize a recursive character text splitter with a chunk size of 1000 and a chunk overlap of 200.\n",
        "text_splitter = ...\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Embed\n",
        "# Load the 'all-mpnet-base-v2' sentence transformer's embedder using the HuggingFaceEmbeddings, put the model_kwargs parameter to 'model_kwargs_value'\n",
        "# 'all-mpnet-base-v2' takes as input a sentence and outputs an embedding of dimension 768\n",
        "model_kwargs_value = {'device': 'cpu'}\n",
        "embedding = ...\n",
        "# Create a Chroma vector database from the splitted documents, 'splits', and the embedding model: 'embedding'\n",
        "vectorstore = ...\n",
        "\n",
        "# Initializing the retriever\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X6waA_SrHJTO",
      "metadata": {
        "id": "X6waA_SrHJTO"
      },
      "source": [
        "Downloading the LLM model Mistral 7B from the"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pGRez8DWG9Y3",
      "metadata": {
        "id": "pGRez8DWG9Y3"
      },
      "outputs": [],
      "source": [
        "mkdir model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LHiBkPbIG0rm",
      "metadata": {
        "id": "LHiBkPbIG0rm"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_S.gguf?download=true -O model/mistral-7b-instruct-v0.2.Q4_K_S.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CQZMKnV3AoM9",
      "metadata": {
        "id": "CQZMKnV3AoM9"
      },
      "outputs": [],
      "source": [
        "#### RETRIEVAL and GENERATION ####\n",
        "\n",
        "# Prompt\n",
        "# here we are taking a previously created prompt and we are using it\n",
        "# The prompt can be found here: https://smith.langchain.com/hub/rlm/rag-prompt-mistral\n",
        "prompt = hub.pull(\"rlm/rag-prompt-mistral\")\n",
        "print('The prompt used here is:\\n', prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kNhRDpxYr-vr",
      "metadata": {
        "id": "kNhRDpxYr-vr"
      },
      "outputs": [],
      "source": [
        "# LLM\n",
        "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "model_path = 'model/mistral-7b-instruct-v0.2.Q4_K_S.gguf'\n",
        "# temp = the temperature of the model; the lower the value, the more deterministic the model is, i.e., for Temp=0, the same question asked several time will get you the same answer.\n",
        "temp = .1\n",
        "# maxtokens = the maximum number of tokens to be generated in the model’s response\n",
        "maxtokens = 1000\n",
        "# top_p = It is used to control the diversity of the predictions, meaning that it selects the most probable tokens whose cumulative probability exceeds a given threshold.\n",
        "top_p = 1\n",
        "# n_ctx = # Text context size; it is the max nbr of tokens that the model can account for when processing a response. This includes the prompt, and the response itself --> the context needs to be set large enough for both the question and answer.\n",
        "n_ctx = 2048\n",
        "\n",
        "# Initialize a LlamaCpp LLM model by setting the params:\n",
        "# model_path to the previously defined model_path\n",
        "# the temperature to 0.1\n",
        "# the max_tokens to 1000\n",
        "# top_p = 1\n",
        "# the context size to 2048\n",
        "# callback_manager to the previosuly defined callback_manager\n",
        "# verbose to True; verbose is required to pass to the callback manager\n",
        "llm = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BobfXvrmSdf_",
      "metadata": {
        "id": "BobfXvrmSdf_"
      },
      "outputs": [],
      "source": [
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vge-LQUTSgHj",
      "metadata": {
        "id": "vge-LQUTSgHj"
      },
      "outputs": [],
      "source": [
        "# Question\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dt1prtmYz0ep",
      "metadata": {
        "id": "Dt1prtmYz0ep"
      },
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"Who is Beyonce?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18e8e856-bafd-469e-b99a-11596b18aad4",
      "metadata": {
        "id": "18e8e856-bafd-469e-b99a-11596b18aad4"
      },
      "source": [
        "## Part 2: Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd7beeb-21fa-4f4b-b8fa-5a4f70489a16",
      "metadata": {
        "id": "edd7beeb-21fa-4f4b-b8fa-5a4f70489a16"
      },
      "outputs": [],
      "source": [
        "# Documents\n",
        "question = \"What kinds of classes do I like?\"\n",
        "document = \"My favorite class is NLP.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0552ea4-935d-4dfa-bd2b-56d148e96304",
      "metadata": {
        "id": "e0552ea4-935d-4dfa-bd2b-56d148e96304"
      },
      "source": [
        "- [Count tokens for OpenAI-based LLM, i.e., GPT3.5, GPT4, etc.](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) considering [~4 char / token](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)\n",
        "- [Count tokens for Open source LLMs, i.e., like Mistral 7B, llama, etc.](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df119cca-1676-4caa-bad4-11805d69e616",
      "metadata": {
        "id": "df119cca-1676-4caa-bad4-11805d69e616"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string_openAI(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"\n",
        "    Returns the number of tokens in a text string.\n",
        "\n",
        "    Args:\n",
        "    -----\n",
        "      string: str\n",
        "        Input string text\n",
        "      encoding_name: str\n",
        "        Encoding Type\n",
        "    Returns:\n",
        "    --------\n",
        "      num_tokens: int\n",
        "        the number of tokens in a text string.\n",
        "      \"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens_from_string_openAI(question, \"cl100k_base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7nFTVYF4aM6d",
      "metadata": {
        "id": "7nFTVYF4aM6d"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaTokenizerFast\n",
        "\n",
        "def num_tokens_from_string(string: str) -> int:\n",
        "  \"\"\"\n",
        "    Returns the number of tokens in a text string.\n",
        "\n",
        "    Args:\n",
        "    -----\n",
        "      string: str\n",
        "        Input string text\n",
        "    Returns:\n",
        "    --------\n",
        "      num_tokens: int\n",
        "        the number of tokens in a text string.\n",
        "\n",
        "    NB: To get the tokenized text, use the command: tokenized_text = tokenizer.tokenize(string)\n",
        "      \"\"\"\n",
        "  tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
        "  tokenized_text_as_ids = tokenizer.encode(string)\n",
        "  num_tokens = len(tokenized_text_as_ids)\n",
        "  return num_tokens\n",
        "num_tokens_from_string(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd98786-755d-4d49-ba97-30c5a623b74e",
      "metadata": {
        "id": "6bd98786-755d-4d49-ba97-30c5a623b74e"
      },
      "outputs": [],
      "source": [
        "# Load the 'all-mpnet-base-v2' sentence transformer's embedder using the HuggingFaceEmbeddings, put the model_kwargs parameter to 'model_kwargs_value'\n",
        "embd =\n",
        "# embed the question using the method 'embed_query'\n",
        "query_result = ...\n",
        "# embed the document using the method 'embed_query'\n",
        "document_result = ...\n",
        "len(query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e0e35f-6861-4c5e-9301-04fd5408f8f8",
      "metadata": {
        "id": "f5e0e35f-6861-4c5e-9301-04fd5408f8f8"
      },
      "source": [
        "[Cosine similarity](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions) is reccomended (1 indicates identical) for OpenAI embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8001998-b08c-4560-b124-bfa1fced8958",
      "metadata": {
        "id": "b8001998-b08c-4560-b124-bfa1fced8958"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "# Compute the cosine similarity between the embeddings of the question and document\n",
        "similarity = ...\n",
        "print(\"Cosine Similarity:\", similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aea73bc-98e3-4fdc-ba72-d190736bed20",
      "metadata": {
        "id": "8aea73bc-98e3-4fdc-ba72-d190736bed20"
      },
      "source": [
        "[Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5778c31a-6138-4130-8865-31a08e82b9fb",
      "metadata": {
        "id": "5778c31a-6138-4130-8865-31a08e82b9fb"
      },
      "outputs": [],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798e731e-c6ff-46e3-a8bc-386832362af2",
      "metadata": {
        "id": "798e731e-c6ff-46e3-a8bc-386832362af2"
      },
      "source": [
        "[Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n",
        "\n",
        "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
        "\n",
        "It takes in the large text then tries to split it by the first character \\n\\n. If the first split by \\n\\n is still large then it moves to the next character which is \\n and tries to split by it. If it is still larger than our specified chunk size it moves to the next character in the set until we get a split that is less than our specified chunk size.\n",
        "\n",
        "This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e668d339-3951-4662-8387-c3d296646906",
      "metadata": {
        "id": "e668d339-3951-4662-8387-c3d296646906"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# Initialize a recursive character text splitter with a chunk size of 300 and a chunk overlap of 50.\n",
        "text_splitter = ...\n",
        "\n",
        "# Split the blog_docs\n",
        "splits = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "427303a1-3ed4-430c-bfc7-cb3e48022f1d",
      "metadata": {
        "id": "427303a1-3ed4-430c-bfc7-cb3e48022f1d"
      },
      "source": [
        "[Vectorstores](https://python.langchain.com/docs/integrations/vectorstores/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa90aaf-cc1b-46a1-9fba-cf20804dcb41",
      "metadata": {
        "id": "baa90aaf-cc1b-46a1-9fba-cf20804dcb41"
      },
      "outputs": [],
      "source": [
        "# Index\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=embd)\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba890329-1411-4922-bd27-fe0490dd1208",
      "metadata": {
        "id": "ba890329-1411-4922-bd27-fe0490dd1208"
      },
      "source": [
        "## Part 3: Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fafdada1-4c4e-41f8-ad1a-33861aae3930",
      "metadata": {
        "id": "fafdada1-4c4e-41f8-ad1a-33861aae3930"
      },
      "outputs": [],
      "source": [
        "# Index\n",
        "from langchain_community.vectorstores import Chroma\n",
        "# Create a Chroma vector database from the splitted documents, 'splits', and the embedding model: 'embd'\n",
        "vectorstore = ...\n",
        "# Initialize the retriever by setting the parameter serach_kwargs to {'k':3}\n",
        "# k = the number of top relevant documents to be retrieved from the vector database\n",
        "retriever = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c2de7a-93e6-4072-bc5b-db6516f96dda",
      "metadata": {
        "id": "57c2de7a-93e6-4072-bc5b-db6516f96dda"
      },
      "outputs": [],
      "source": [
        "# use the method get_relevant_documents to find the releveant documents of the query\n",
        "query = \"What is Task Decomposition?\"\n",
        "docs = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db96f877-60d3-4741-9846-e2903831583d",
      "metadata": {
        "id": "db96f877-60d3-4741-9846-e2903831583d"
      },
      "outputs": [],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T6df5_nwmR6I",
      "metadata": {
        "id": "T6df5_nwmR6I"
      },
      "outputs": [],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beda1b07-7bd2-4f5b-8d44-1fc52f5d2ce2",
      "metadata": {
        "id": "beda1b07-7bd2-4f5b-8d44-1fc52f5d2ce2"
      },
      "source": [
        "## Part 4: Generation\n",
        "\n",
        "![Screenshot 2024-02-12 at 1.37.38 PM.png](attachment:f9b0e284-58e4-4d33-9594-2dad351c569a.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8beb6c14-5e18-43e7-9d04-59e3b8a81cc9",
      "metadata": {
        "id": "8beb6c14-5e18-43e7-9d04-59e3b8a81cc9"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "# Let us modify the https://smith.langchain.com/hub/rlm/rag-prompt-mistral prompt to something more customized\n",
        "template = \"\"\"\n",
        "<s> [INST] You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, precise that the answer is based on your own knowledge and not the context.\n",
        "Be as much detailed as possible.\n",
        "[/INST] </s>\n",
        "\n",
        "[INST] Question: {question}\n",
        "Context: {context}\n",
        "Answer: [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4461264-5cac-479a-917c-9bf589826da4",
      "metadata": {
        "id": "e4461264-5cac-479a-917c-9bf589826da4"
      },
      "outputs": [],
      "source": [
        "# LLM\n",
        "\n",
        "# Initialize a LlamaCpp LLM model similar to the one previously initialized\n",
        "llm = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d",
      "metadata": {
        "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d"
      },
      "outputs": [],
      "source": [
        "# Chain\n",
        "# create a chain of only 2 steps : prompt and then llm\n",
        "chain = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
      "metadata": {
        "id": "94470770-8df4-4359-9504-ef6c8b3137ff"
      },
      "outputs": [],
      "source": [
        "# Run\n",
        "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ffe29a1-5527-419e-9f12-8a3061d12885",
      "metadata": {
        "id": "8ffe29a1-5527-419e-9f12-8a3061d12885"
      },
      "source": [
        "[RAG chains](https://python.langchain.com/docs/expression_language/get_started#rag-search-example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8208a8bc-c75f-4e8e-8601-680746cd6276",
      "metadata": {
        "id": "8208a8bc-c75f-4e8e-8601-680746cd6276"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "# Initialize a rag chain with:\n",
        "# 1 - the context and question (without any format_docs)\n",
        "# 2 - the prompt\n",
        "# 3 - the llm\n",
        "# 4 - the output parser\n",
        "# PS: check the chain above in the overview section\n",
        "rag_chain = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7Covyoadzspi",
      "metadata": {
        "id": "7Covyoadzspi"
      },
      "outputs": [],
      "source": [
        "query = \"What is Task Decomposition?\"\n",
        "# Test your rag chain with the query\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb474b9-1e38-4a20-91cc-24cdce6d8631",
      "metadata": {
        "id": "3fb474b9-1e38-4a20-91cc-24cdce6d8631"
      },
      "outputs": [],
      "source": [
        "query2 = \"Who is Beyonce?\"\n",
        "# Test your rag chain with the query2\n",
        "..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
