{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Travel Poster Generation System\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project aims to develop an automated system for generating travel posters using Generative AI technologies. The intended users are travel agencies and content creators who need to produce engaging and visually appealing marketing materials efficiently. This system leverages text and image generation models to automate the creation of customized travel posters, streamlining content production and enhancing creative outputs.\n",
    "\n",
    "## Step 1: Business Use Case Identification\n",
    "\n",
    "### Objective\n",
    "\n",
    "The primary objective is to create a tool that enables rapid production of travel posters, which can be used to enhance online engagement and marketing efforts. By automating this process, we aim to reduce the time and cost associated with graphic design while allowing for high scalability and customization.\n",
    "\n",
    "### Justification\n",
    "\n",
    "In the digital age, visual content is key to capturing audience attention. Travel agencies and social media content creators often require quick turnaround and high volumes of diverse and eye-catching promotional materials. By automating poster creation, we provide a value-added solution that meets these market demands efficiently.\n",
    "\n",
    "## Step 2: Model Selection and Setup\n",
    "\n",
    "### Text Generation Model: GPT-2\n",
    "\n",
    "**Choice Justification:**\n",
    "- **Performance vs. Resource Efficiency:** GPT-2 offers a good balance, making it ideal for local deployment where resources like memory and processing power may be limited.\n",
    "- **Ease of Use:** The `transformers` library provides an accessible interface to leverage this model without deep technical knowledge of its inner workings.\n",
    "\n",
    "### Image Generation Model: Stable Diffusion\n",
    "\n",
    "**Choice Justification:**\n",
    "- **High-Quality Outputs:** Known for generating detailed and visually appealing images that can be customized through textual prompts.\n",
    "- **Local Performance Optimization:** Compatible with MPS (Metal Performance Shaders), enabling efficient GPU acceleration on macOS devices.\n",
    "\n",
    "### Implementation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 23.3.1 from /Users/fulin/anaconda3/lib/python3.11/site-packages/pip (python 3.11)\n",
      "Requirement already satisfied: diffusers in /Users/fulin/anaconda3/lib/python3.11/site-packages (0.27.2)\n",
      "Requirement already satisfied: importlib-metadata in /Users/fulin/anaconda3/lib/python3.11/site-packages (from diffusers) (7.0.1)\n",
      "Requirement already satisfied: filelock in /Users/fulin/anaconda3/lib/python3.11/site-packages (from diffusers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.2 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from diffusers) (0.20.3)\n",
      "Requirement already satisfied: numpy in /Users/fulin/anaconda3/lib/python3.11/site-packages (from diffusers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from diffusers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/fulin/anaconda3/lib/python3.11/site-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from diffusers) (0.4.2)\n",
      "Requirement already satisfied: Pillow in /Users/fulin/anaconda3/lib/python3.11/site-packages (from diffusers) (10.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers) (2023.10.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers) (23.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from importlib-metadata->diffusers) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests->diffusers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests->diffusers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests->diffusers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests->diffusers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# I got a macbook so use mps firstly\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Integration and Workflow\n",
    "\n",
    "### Generating Textual Content\n",
    "\n",
    "First, the system generates a text description using GPT-2, tailored to the theme of \"travel\". This description guides the visual content creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generator\n",
    "text_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# generate a text\n",
    "topic = \"travel\"\n",
    "prompt = f\"Creat a {topic} poster:\"\n",
    "text_output = text_generator(prompt, max_length=100)\n",
    "generated_text = text_output[0]['generated_text']\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Poster Image\n",
    "\n",
    "Using the text description as a prompt, the Stable Diffusion model generates a corresponding image. This process is tuned to operate efficiently on local machines while maintaining high output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image generator\n",
    "image_generator = StableDiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", \n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# generate the photo\n",
    "image_output = image_generator(generated_text, num_inference_steps=50).images[0]\n",
    "\n",
    "# show\n",
    "image_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Optimization and Adaptation\n",
    "\n",
    "### Objective\n",
    "\n",
    "To improve the efficiency and performance of the generative models used (GPT-2 for text generation and Stable Diffusion for image generation) when deployed locally. The adaptation aims to reduce resource consumption (like memory and computation time) without significantly sacrificing the output quality.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "1. **Fine-Tuning the GPT-2 Model:**\n",
    "   - **Data Acquisition:** Identify and acquire a travel-related dataset. Possible sources include travel blogs, descriptions from travel websites, or a dataset from repositories like Kaggle.\n",
    "   - **Training Objective:** Fine-tune GPT-2 on the travel dataset to enhance its ability to generate relevant and context-specific text for travel posters.\n",
    "   - **Tools and Libraries:** Use `transformers` and `datasets` from Hugging Face for training and data handling.\n",
    "\n",
    "2. **Optimizing Stable Diffusion:**\n",
    "   - **Model Pruning:** Implement pruning techniques to reduce the size of the model while maintaining the quality of the generated images. This involves systematically removing parameters that have little impact on the output.\n",
    "   - **Quantization:** Apply quantization to decrease the modelâ€™s precision from float32 to float16 or even int8, which can reduce the model size and speed up inference time, with minimal impact on image quality.\n",
    "\n",
    "### Experimentation\n",
    "\n",
    "- **Setup Experimentation Environment:** Ensure that all experiments are reproducible with version control for both data and model configurations.\n",
    "- **Performance Metrics:** Track performance improvements in terms of inference time and resource utilization. Use qualitative assessments (like user studies) to measure any changes in output quality.\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of fine-tuning GPT-2 with the Hugging Face 'Trainer' API\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load and prepare the dataset\n",
    "dataset = load_dataset('path_to_travel_dataset')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned-travel\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
