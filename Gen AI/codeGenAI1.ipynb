{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad2263c-36ff-49a4-b579-af9d9fc37af8",
   "metadata": {},
   "source": [
    "# Let's try HuggingFace to play with LLMs !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb1cc0-eb5a-43ea-9bd4-0ce7cce7c58c",
   "metadata": {},
   "source": [
    "First, create an account on HuggingFace !\n",
    "\n",
    "### But, what's HuggingFace ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec06a0-2f19-4075-9e55-3a5b4c60c793",
   "metadata": {},
   "source": [
    "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d689e-9ecc-438e-bee4-3f1fb7fbc8a1",
   "metadata": {},
   "source": [
    "## Level 1 : Pipeline\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/pipeline_tutorial\n",
    "\n",
    "Basic tasks :\n",
    "- \"text-classification\"\n",
    "-  \"audio-classification\"\n",
    "-  \"text-to-audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c19cd7-6b4d-45c1-9b02-8bb61ae88d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/fulin/anaconda3/lib/python3.11/site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in /Users/fulin/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/fulin/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f3258-5b66-44a3-b1be-27567f95d1ce",
   "metadata": {},
   "source": [
    "### 1) Audio recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8b901-3188-47be-abc5-c23f1c8f0584",
   "metadata": {},
   "source": [
    "#### Basic task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a68896-b2a4-458e-8b3e-13743f3fa305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 55bb623 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b69f8f8b0b34dbfbac6b83184df7281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d343284e1e4ac19042ab7ef7f76597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb621e55407462b8c980b9a7f5c53c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00472ebc75674ab09c51365cf77a5f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ed1207d8d1480e903558c44de013c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef93529efd93405a9ebe06acd7ab38e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "transcriber = pipeline(task=\"automatic-speech-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69d2c3c-d75c-45ce-9d30-49e7babac6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edce0364-ac9b-40fe-86f9-752f54af6c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'AARE YOU COMENGARIVO'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a recording of your own / our record something now !\n",
    "transcriber(\"test2.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c96f71-0752-4e03-8141-db834202413a",
   "metadata": {},
   "source": [
    "#### The results is not that good, can we find a better model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7de2afa-d3fa-4cad-a610-3155abbe6162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fe2348359f445299dc649736aecf96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c099c2c1b3ed4a24814b5e1de7f7fc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/4.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e1aa6d0a6f4d5fb54552fc360b61cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b063f46e41ed49399a6157502a1fa417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2004fa719e4ee1b1de5b108ac7e84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a604ec0c48a4c3d913559ef437dac30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ab33197ccb4262b1343144602048e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db790336cd94ca299eb786cc128b602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1dc3a9c4e045f49d639a3f77d79bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f65c757118430b989cf6821d17a45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber = pipeline(model=\"openai/whisper-large-v2\")\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "528790e8-2b79-4249-80f3-25931fbfcad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' How are you?'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber(\"test2.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7592b46-290a-438b-be6c-a5b099011272",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Find the parameter that could help you to create subtitle in the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f47913ed-8575-4735-9197-c812e0e6bc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Nous sommes actuellement indisponibles, nous vous rappellerons dès que possible. Merci.',\n",
       " 'chunks': [{'timestamp': (0.0, 2.88),\n",
       "   'text': ' Nous sommes actuellement indisponibles, nous vous rappellerons dès que possible.'},\n",
       "  {'timestamp': (2.88, 3.38), 'text': ' Merci.'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber(\"/Users/simonabad/Music/Music/Media.localized/Music/Unknown Artist/Unknown Album/répondeur global.mp3\", return_timestamps=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9ae4b-bc80-47f1-aade-7d6f4e8c2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try a longer document, audiobook for example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705226a-800d-4e99-b7e1-499fa2c681e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber(\"/Users/simonabad/Downloads/ladysusan_1_austen.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c053b67-906a-468b-a11f-74aec3e8b630",
   "metadata": {},
   "source": [
    "#### What can you say about that ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2fd926-3559-4ab8-9892-fa67ae3dc5ec",
   "metadata": {},
   "source": [
    "### 2) Vision pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f56db-ba8a-4891-a165-f5ef9424a512",
   "metadata": {},
   "source": [
    "#### Basic vision recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a3d4b7f-5bc2-4452-b184-86a1b648c77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303fae2b089340e0ae6dc05c47581ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b5af0e5b1947cb980fa6ccc3685bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965b7a78ff3844c8925921f5853b3e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'lynx, catamount', 'score': 0.44030290842056274}, {'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 'score': 0.03433411940932274}, {'label': 'snow leopard, ounce, Panthera uncia', 'score': 0.03214804828166962}, {'label': 'Egyptian cat', 'score': 0.023539088666439056}, {'label': 'tiger cat', 'score': 0.023034201934933662}]\n"
     ]
    }
   ],
   "source": [
    "vision_classifier = pipeline(model=\"google/vit-base-patch16-224\")\n",
    "preds = vision_classifier(\n",
    "    images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")\n",
    "print (preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524cbfb4-2450-4eaf-aba5-bd1ca0213afe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Try with an image on the internet ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf989b39-de79-489d-a094-237408a87d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'ant, emmet, pismire', 'score': 0.18471653759479523}, {'label': 'sulphur butterfly, sulfur butterfly', 'score': 0.16416318714618683}, {'label': 'lycaenid, lycaenid butterfly', 'score': 0.08593808114528656}, {'label': 'lacewing, lacewing fly', 'score': 0.05107032507658005}, {'label': 'cabbage butterfly', 'score': 0.030738649889826775}]\n"
     ]
    }
   ],
   "source": [
    "preds = vision_classifier(\n",
    "    images=\"https://linsecterie.com/cdn/shop/products/ManteOrchidee-Hymenopuscoronatus.jpg?v=1664384644&width=990\")\n",
    "print (preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12985fef-5468-4e92-b012-55b80a0ef2c2",
   "metadata": {},
   "source": [
    "### So, knowing you can do this easily : what do you think as changed thanks to these models ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdc306-6b41-468d-b37d-1d2a64836509",
   "metadata": {},
   "source": [
    "check here all the possibilities : https://huggingface.co/docs/transformers/v4.38.1/en/main_classes/pipelines#transformers.TextToAudioPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e05456-eb1b-4508-a0bb-2a2a945eff90",
   "metadata": {},
   "source": [
    "### Bias (gender example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf0d8921-f889-410a-a6bf-396764208242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac26aae7d0594f0faf8754381e3d950b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b052437929f46c291607ed30a74299f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba574821209426ea4ce05cdd60bfbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c111de28a511404cbf413e68341fdd46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bef38e4013438b80740423002e6b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62855e-3b27-4630-9c00-0a8426a246e5",
   "metadata": {},
   "source": [
    "## Install Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88b52af0-652c-4d25-9551-351ac807f120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc546f-ac31-45e7-ab71-6b8e6c579bd0",
   "metadata": {},
   "source": [
    "## Now let's go back to class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54d125e-c41f-47fa-ad71-0e31228e3995",
   "metadata": {},
   "source": [
    "## Level 2 : What is inside the pipe line ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235511c-ff96-4f2c-a05f-597944279eec",
   "metadata": {},
   "source": [
    "#### 1) Pre-processing with a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a31fbec1-dae6-4877-8f84-3619c09dceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724e24005a414fc5afcc3cd0dd67e795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5325ab24b7bb4e199da9445d4e2920ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df7abd3277b4041960becbdbafce780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82541c7260949a9889793e767868aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d18b88-4482-4b2f-89b7-281bb0130ed3",
   "metadata": {},
   "source": [
    "Objective of the tokenizer :\n",
    "\n",
    "Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "\n",
    "Mapping each token to an integer\n",
    "\n",
    "Adding additional inputs that may be useful to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a384d539-2eb9-42c1-85bb-5bdf9b559d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c4e580daca415a91c8d9a013ef8976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223b02ceb0a0426182d202ede73c7c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250132334b954caab427417c0bdb2ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50718388-2891-4e45-9667-32224eaef646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff529ca-3d9a-4ba5-b883-9f43d3b49d54",
   "metadata": {},
   "source": [
    "#### 2) Model and Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debac5b-43f7-472b-a3dd-4ef81884f1a4",
   "metadata": {},
   "source": [
    "This architecture contains only the base Transformer module: given some inputs, it outputs what we’ll call hidden states, also known as features. For each model input, we’ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model.\n",
    "\n",
    "If this doesn’t make sense, don’t worry about it. We’ll explain it all later.\n",
    "\n",
    "While these hidden states can be useful on their own, they’re usually inputs to another part of the model, known as the head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac2253-2414-4a2b-b4d6-b8e1a79c683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6619c-3a4c-4be2-9afd-3cc7c2cc971b",
   "metadata": {},
   "source": [
    "##### The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "##### Batch size: The number of sequences processed at a time (2 in our example).\n",
    "##### Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "##### Hidden size: The vector dimension of each model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e8192a-6d77-4da6-9b47-ff5b22cb069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac7a3f-ae5d-44ab-8327-bf97fe4eed79",
   "metadata": {},
   "source": [
    "#### There are many different architectures available in Transformers, with each one designed around tackling a specific task. \n",
    "\n",
    "Here is a non-exhaustive list:\n",
    "\n",
    "*Model (retrieve the hidden states)\n",
    "\n",
    "*ForCausalLM\n",
    "\n",
    "*ForMaskedLM\n",
    "\n",
    "*ForMultipleChoice\n",
    "\n",
    "*ForQuestionAnswering\n",
    "\n",
    "*ForSequenceClassification\n",
    "\n",
    "*ForTokenClassification\n",
    "\n",
    "and others 🤗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6339ad11-c65c-47e1-973e-efce4b592c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14eec1ea-310d-4831-a1a3-9412fac724d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2dfa9-d5a1-4da1-847c-042997890d94",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b1e5a96-eb68-40c5-b809-d05eb96dfef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd61c92e-282a-48b1-9a9b-910d7b8d33c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5981e-01],\n",
      "        [9.9946e-01, 5.4419e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9037f60-49e9-457c-a0a6-d0d630bb9aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27e5bef0-8155-444b-b531-d8170fd447e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "851630d7-23a9-442c-b399-bc9e042ddbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 16770, 1024, 1013, 1013, 17662, 12172, 1012, 2522, 1013, 2951, 13462, 2015, 1013, 6583, 2869, 4014, 1013, 2004, 2099, 1035, 24369, 1013, 10663, 1013, 2364, 1013, 19875, 2243, 1012, 13109, 6305, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "audio_sequence = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n",
    "print(tokenizer(audio_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93119447-28e1-47f8-8727-03d236ad462f",
   "metadata": {},
   "source": [
    "#### OLD - to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5710bc44-aab2-4c57-963a-f27ef65065b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e7584e-cc7b-4894-b235-1c328e72b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HUGGING_FACE_API_KEY = os.environ.get(\"HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c72216f-f97d-4d5c-a441-33cbbaa537af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "filenames = [\n",
    "        \"pytorch_model.bin\", \"added_tokens.json\", \"config.json\", \"generation_config.json\", \n",
    "        \"special_tokens_map.json\", \"spiece.model\", \"tokenizer_config.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2546c614-c1b3-4bae-98e6-6a8a584a04a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/simonabad/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/pytorch_model.bin\n",
      "/Users/simonabad/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/added_tokens.json\n",
      "/Users/simonabad/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/config.json\n",
      "/Users/simonabad/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/generation_config.json\n",
      "/Users/simonabad/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/special_tokens_map.json\n",
      "/Users/simonabad/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/spiece.model\n",
      "/Users/simonabad/.cache/huggingface/hub/models--lmsys--fastchat-t5-3b-v1.0/snapshots/0b1da230a891854102d749b93f7ddf1f18a81024/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "for filename in filenames:\n",
    "        downloaded_model_path = hf_hub_download(\n",
    "                    repo_id=model_id,\n",
    "                    filename=filename,\n",
    "                    token=HUGGING_FACE_API_KEY\n",
    "        )\n",
    "        print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78815b5c-3893-48f8-92a2-63d32a3ca214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
