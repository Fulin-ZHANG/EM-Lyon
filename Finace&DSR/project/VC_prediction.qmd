---
title: "Venture Capital Funding"
editor: visual
author:
  - name: "ZHANG Fulin"
    affiliation: "EM-Lyon Business School"
created: "2024-01-30"
date: "`r lubridate::today()`"
abstract: |
  This project focuses on a classification problem with the objective of predicting whether a company received venture capital (VC) funding in the year 2000. It presents a unique challenge that combines the application of data science and machine learning methodologies with an in-depth exploration of investment patterns and business growth dynamics. We will utilize a dataset featuring several variables, including the company identifier (firmcode), company age (Age), country of operation (Country), industry sector (Industry), and a range of financial metrics related to company size and performance (such as the natural logarithm of sales revenue, number of employees, and total assets). Additionally, the dataset contains a special variable (VCinv), which represents the frequency of VC investments in the company's country and industry from 1996 to 2005 and will be used as an instrumental variable for VC in Heckman models.
  <br><br><br>
format: html
toc: true
toc-float: true
toc-depth: 2
number-sections: true
code-folding: hide
theme: sketchy
---

# Import Packages

Import the necessary libraries

```{r, warning = FALSE, message=FALSE}
library(tidyverse)
library(data.table)         
library(dplyr)          
library(DataExplorer)
library(fastDummies)      
library(ggplot2)           
library(vcd)
library(gridExtra)        
library(corrplot)
library(DT)
library(lightgbm)      
library(caret)
library(Metrics)
library(ROSE)
library(fastDummies)
```

Let's load the dataset

```{r}
load("VC_data.RData")
```

# Description of the dataset

Let's take a brief look on the data

```{r}
dim(VCdata)
ls(VCdata)
glimpse(VCdata)
```

The VCdata dataset contains **23 features**, and **1 target value**. There are a total of **2610 observations**. There are 3 string variables in the features and the rest are all type of **double**. The target value is a double but will be treated as a 2-categorical variable considering that it is a binary problem.

Here are the top and bottom 5 rows:

```{r}
head(VCdata, 5)
```

```{r}
tail(VCdata, 5)
```

Let's check the missing value in total

```{r}
sprintf("Nbr of missing value in total: %f",sum(is.na(VCdata)))
plot_missing(VCdata)
```

Contains 6239 null values distributed in different rows and columns. Compared to 2610 observations, that's an average of 3 per row, so I can't perform a simple delete operation, huge amounts of information would be lost.

Then briefly check the target values' distribution.

```{r}
sprintf("The nbr of observations of VC = 1: %f", count(VCdata[VCdata$VC == '1', ]))
sprintf("The nbr of observations of VC = 0: %f", count(VCdata[VCdata$VC == '0', ]))
```

The results show that the ratio of the two categories of data for the target value is 71:2539, making the data extremely unbalanced.

# EDA

In this part, I am going to further explorer the features.

## Univariate Analysis

Firstly, let's take a look of the summary of all features.

```{r}
summary(VCdata)
```

Firmcode are the IDs which are meaningless, so drop it.

```{r}
VCdata <- VCdata %>% 
  select(-firmcode)
```

### Categirical variables

To find out the distribution of the categorical variables, I define a function to plot the population's distribution and percent distribution as well.

```{r}
plot_categorical <- function(data, column_name) {
  
  data_summary <- data %>%
    group_by(!!sym(column_name)) %>%
    summarise(Count = n()) %>%
    mutate(Percent = Count / sum(Count) * 100)

  p1 <- ggplot(data_summary, aes_string(x = column_name, y = "Count")) +
    geom_bar(stat = "identity", fill = "steelblue") +
    theme_minimal() +
    labs(title = paste("Count Distribution of", column_name), x = column_name, y = "Count")

  p2 <- ggplot(data_summary, aes_string(x = column_name, y = "Percent")) +
    geom_bar(stat = "identity", fill = "lightblue") +
    theme_minimal() +
    labs(title = paste("Percent Distribution of", column_name), x = column_name, y = "Percent")

  grid.arrange(p1, p2, ncol = 2)
}
```

-   Distribution of firms' age

```{r}
unique(VCdata$age)
length(unique(VCdata$age))
```

```{r, fig.width=10, fig.height=2}
plot_categorical(VCdata, "age")
```

Here, even though age is a continuous variable, it is a discrete variable, so here I chose to view the distribution in a bar chart. According to the image, there are more than 600 companies with age 0, which is more than a quarter of the total dataset.

-   Distribution of country

```{r, fig.width=10, fig.height=2}
plot_categorical(VCdata, "country")
```

The UK's accounted for more than a quarter of the total, France's for nearly 20%, and the others for about 10% each, and mainly in Western Europe.

-   Distribution of industry

```{r, fig.width=10, fig.height=2}
plot_categorical(VCdata, "industry")
```

Depends on the distribution of the type of industry, the grapes show that software accounted for for than 40% of the total, manufacturing for nearly 20%. All the others are around 5%, with Aerospace being the least.

### Quantitative variables

Define a function to show box-plot and violin-plot in order to show the distribution of quantitative variables

```{r}
plot_quantitative <- function(data, column_name) {
  box_plot <- ggplot(data, aes(x = "", y = !!sym(column_name))) +
    geom_boxplot() +
    theme_minimal() +
    labs(title = paste("Box plot of", column_name), x = "", y = column_name)

  violin_plot <- ggplot(data, aes(x = "", y = !!sym(column_name))) +
    geom_violin(trim = FALSE) +
    theme_minimal() +
    labs(title = paste("Violin Plot of", column_name), x = "", y = column_name)

  grid.arrange(box_plot, violin_plot, ncol = 2)
}
```

-   Distribution of VCinv

```{r}
plot_quantitative(VCdata, "VCinv")
```

The overall investment frequency is mainly distributed around 0.005 Â± 0.0025 and there are a lot of outliers mainly in the interval 0.015-0.05.

So I wanted to take a more visual look at these discrete values.

```{r}
VCinv0015 <- VCdata %>%
  filter(VCinv > 0.015)

print(VCinv0015)
```

These discrete observations are mainly Biotech or Internet industries. What specific insights are available may require further research.

## Bivariate Analysis

-   Age vs VC

```{r}
ggplot(VCdata, aes(x = factor(VC), y = age, fill = factor(VC))) +
  geom_boxplot() +
  labs(x = "VC Received", y = "Age", title = "Distribution of Age by VC Status") +
  scale_fill_discrete(name = "VC Status", labels = c("No", "Yes")) +
  theme_minimal()

anova_result <- aov(VC ~ factor(age), data = VCdata)
summary(anova_result)
```

According to the box plot demonstration, for the category of `VC=1`, there is a smaller trend relative to `VC=0`, which means that it may be possible for the public young firms to receive capital more instead. **p-value of 2.01e-07** was then obtained by using ANOVA analysis, which indicates that there is a significant difference between the age groups on the variable of receiving VC.

-   Logxxxx vs VC

```{r, fig.width=10, fig.height=20, warning=FALSE}
log_vars <- names(VCdata)[grepl("log", names(VCdata))]

plot_list <- lapply(log_vars, function(var) {
  ggplot(VCdata, aes(x = factor(VC), y = get(var), fill = factor(VC))) +
    geom_boxplot() +
    labs(x = "VC Received", y = var, title = paste(var, "by VC Status")) +
    scale_fill_discrete(name = "VC Status", labels = c("No", "Yes")) +
    theme_minimal()
})

n_cols <- 3
n_rows <- ceiling(length(plot_list) / n_cols)
grid.arrange(grobs = plot_list, ncol = n_cols, nrow = n_rows)
```

```{r}
anova_results <- sapply(log_vars, function(var) {
  anova_result <- aov(reformulate(var, response = "VC"), data = VCdata)
  summary(anova_result)[[1]]["Pr(>F)"][1, 1]
})

anova_df <- data.frame(Variable = names(anova_results), P_Value = unlist(anova_results))

datatable(anova_df) %>%
  formatStyle(
    'P_Value',
    backgroundColor = styleInterval(0.05, c('lightgreen', 'lightcoral'))
  )
```

Based on the distribution and the p-value after ANOVA analysis, I found that the vast majority of log-related FEATURES showed a significant difference between the two groups.

```{r, fig.width=15, fig.height=15}
log_data <- VCdata[log_vars]
cor_matrix <- cor(log_data, use = "pairwise.complete.obs")

corrplot(cor_matrix, method = "color", type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45, addCoef.col = "white")
```

Here they are all positively correlated with each other, even strongly positively correlated, I actually did not find a reasonable explanation for this, the log was chosen to store this information in order to correct the original distribution that was skewed to the left into one that is easier to recognize, but if earnings, market capitalization, and number of employees are all increasing as the year changes, then there should be a 50/50 split between positive and negative correlation. Further research is needed here.

-   Industry / Country vs VC

```{r}
fisher_industry_vc <- fisher.test(table(VCdata$industry, VCdata$VC), simulate.p.value = TRUE)
fisher_country_vc <- fisher.test(table(VCdata$country, VCdata$VC), simulate.p.value = TRUE)

print(fisher_industry_vc)
print(fisher_country_vc)
```

```{r, fig.width=10, fig.height=15}
par(mfrow = c(2, 1))

mosaicplot(table(VCdata$industry, VCdata$VC), main = "Mosaic Plot of Industry and VC", color = TRUE, las = 2)
mosaicplot(table(VCdata$country, VCdata$VC), main = "Mosaic Plot of Country and VC", color = TRUE, las = 2)
```

Here the chi-square test was abandoned in favor of the Fischer test due to the extremely limited amount of data of certain types. Based on the p-value it was shown that there was indeed a significant difference in the distribution between the groups. Again, based on the mosaic plot it is easy to see that the **Internet** and **TLC** sectors have a higher percentage of investments, with **Biotech** coming in second. In terms of country orientation, **Germany** is the highest, followed by **Italy**. **France** has a relatively small share, albeit from a large base.

# Modeling

In this part, I am going to train the model, and here I chose lightgbm. The power for binary classification problems, the support for nulls, and the fast computational power were the main reasons for me to choose it.

So apply one-hot encoding on the categorical variables.

```{r}
categorical_columns <- c("country", "industry")

VCdata_encoded <- dummy_cols(VCdata, select_columns = categorical_columns, remove_first_dummy = TRUE, remove_selected_columns = TRUE)

colnames(VCdata_encoded) <- make.names(colnames(VCdata_encoded), unique = TRUE)
```

Take a look on the encoded data to make sure the process has been done correctly.

```{r}
head(VCdata_encoded)
```

Next divide the train, validation, and test sets. The ratio is 8:1:1. Due to the extreme imbalance in the dataset, here I chose to use **oversampling** to address the imbalance. I also tried mixed sampling before this, but the accuracy of the predicted results was only 50%.

```{r}
set.seed(48)

trainIndex <- createDataPartition(VCdata_encoded$VC, p = .8, list = FALSE)
data_train <- VCdata_encoded[trainIndex, ]
data_temp <- VCdata_encoded[-trainIndex, ]

valIndex <- createDataPartition(data_temp$VC, p = 0.5, list = FALSE)
data_val <- data_temp[valIndex, ]
data_test <- data_temp[-valIndex, ]

data_train <- ovun.sample(VC ~ ., data = data_train, method = "over", N = nrow(data_train) * 2)$data
```

Initialize the model and set the parameters, after several attempts, the learning rate is better at 0.001, while setting the early stop parameter to prevent overfitting.

```{r, warning=FALSE}
train_set <- lgb.Dataset(data = as.matrix(data_train[,-which(names(data_train) == "VC")]), label = data_train$VC)
val_set <- lgb.Dataset(data = as.matrix(data_val[,-which(names(data_val) == "VC")]), label = data_val$VC)

params <- list(
  objective = "binary",
  metric = "binary_logloss",
  learning_rate = 0.001
)

model_lgb <- lgb.train(
  params = params,
  data = train_set,
  valids = list(validation = val_set),
  nrounds = 100000,
  early_stopping_rounds = 10,
  verbose = 0
)
```

`In order to prevent the html page from taking up too much space, I will temporarily set the verbose to 0. If you need to check it further, you can adjust its value to 1.`

After 4341 iterations, the **loss** was reduced from 1.36246 to a minimum of 0.0794232.

Then look at the accuracy on the training and validation sets.

```{r}
train_preds <- predict(model_lgb, data.matrix(data_train[,-which(names(data_train) == "VC")]), 
                       num_iteration = model_lgb$best_iter)
val_preds <- predict(model_lgb, data.matrix(data_val[,-which(names(data_val) == "VC")]), 
                     num_iteration = model_lgb$best_iter)

train_preds_class <- ifelse(train_preds > 0.5, 1, 0)
val_preds_class <- ifelse(val_preds > 0.5, 1, 0)

train_accuracy <- accuracy(train_preds_class, data_train$VC)
val_accuracy <- accuracy(val_preds_class, data_val$VC)

print(paste("Training accuracy:", train_accuracy))
print(paste("Validation accuracy:", val_accuracy))
```

The accuracy reached **100%** on the training set and **98%** on the validation set, with little difference and no overfitting.

Since the model performance is already good, grid searching and cross-validation are not performed here, and other operations such as further hyper-parameter tuning and improving generalization are not performed.

I then make predictions on a test set and print the accuracy.

```{r}
predictions <- predict(model_lgb, data.matrix(data_test[,-which(names(data_test) == "VC")]))

predictions_binary <- ifelse(predictions > 0.5, 1, 0)

test_accuracy <- accuracy(data_test$VC, predictions_binary)

print(paste("Test accuracy:", test_accuracy))
```

The accuracy on the test set is **94.2%**, indicating a high generalization capability and good performance.

```{r}
conf_mat <- confusionMatrix(as.factor(predictions_binary), as.factor(data_test$VC))
recall <- conf_mat$byClass['Sensitivity']

print(paste("Recall:", recall))

preds_vs_actual <- data.frame(Predicted = predictions_binary, Actual = data_test$VC)

#preds_vs_actual

actual_ones <- preds_vs_actual %>%
  filter(Actual == 1)

#actual_ones
```

I also checked the recall rate here because the first time I used a combination of oversampling and not dealing with missing values (these two attempts are not reflected in the code part). To prevent the model from achieving high accuracy by blindly predicting the majority class, I further output the recall rate. Here, the recall rate reached **98%**, indicating that the prediction of the positive cases (the minority class, which, after oversampling, is no longer an absolute minority) is also precise. Additionally, I output a comparison table of predicted values and actual values. `I have commented it out here to prevent taking up too much space on the HTML page.`

Then I looked at the feature importance.

```{r}
importance <- lgb.importance(model_lgb)

importance_sorted <- importance[order(-importance$Gain), ]

ggplot(importance_sorted, aes(x = reorder(Feature, -Gain), y = Gain)) +
  geom_col(fill = 'steelblue') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Feature Importance", x = "Features", y = "Importance (Gain)")
```

Based on the feature importance, I found that **logemp_3** (the natural logarithm of the number of employees in 2002) and **age** made an unusually significant contribution to the prediction of VC. Industry and country did not seem to provide much help. Here, all the feature values did not differ by thousands, so no normalization or similar operations were performed.

# Thinking of datalackage(Beta version)

I've always harbored a concern, not just because the model's accuracy is exceptionally high without any signs of overfitting, but also regarding the possibility of a misrepresentation in the feature descriptions provided in the problem statement. My concern arises from the definition of VC as a dummy variable equal to one if the company received VC in the year 2000. However, the variables logemp_1 to logemp_5, logsales_1 to logsales_5, and logassets_1 to logassets_5 represent the natural logarithms of the company's number of employees, sales revenues, and total assets at the end of the year 2000 through to 2004 respectively. I am questioning whether using data from the year 2000 onwards could lead to data leakage, thus accounting for the model's unusually high predictive accuracy. Alternatively, could there be an error in the problem statement's explanation of VC? To address these concerns, I have decided to eliminate all data from the year 2000 onwards and to repeat the above processes.

```{r}
log_vars_to_remove <- grep("log.*_[1-5]", names(VCdata), value = TRUE)

VCdata_1999 <- VCdata[ , !(names(VCdata) %in% log_vars_to_remove)]

head(VCdata_1999)
```

```{r}
categorical_columns <- c("country", "industry")

VCdata_encoded <- dummy_cols(VCdata_1999, select_columns = categorical_columns, remove_first_dummy = TRUE, remove_selected_columns = TRUE)

colnames(VCdata_encoded) <- make.names(colnames(VCdata_encoded), unique = TRUE)
```

```{r}
set.seed(48)

trainIndex <- createDataPartition(VCdata_encoded$VC, p = .8, list = FALSE)
data_train <- VCdata_encoded[trainIndex, ]
data_temp <- VCdata_encoded[-trainIndex, ]

valIndex <- createDataPartition(data_temp$VC, p = 0.5, list = FALSE)
data_val <- data_temp[valIndex, ]
data_test <- data_temp[-valIndex, ]

data_train_balanced <- ovun.sample(VC ~ ., data = data_train, method = "over", N = nrow(data_train) * 2)$data
```

```{r}
train_set <- lgb.Dataset(data = as.matrix(data_train_balanced[,-which(names(data_train_balanced) == "VC")]), label = data_train_balanced$VC)
val_set <- lgb.Dataset(data = as.matrix(data_val[,-which(names(data_val) == "VC")]), label = data_val$VC)

params <- list(
  objective = "binary",
  metric = "binary_logloss",
  learning_rate = 0.001
)

model_lgb <- lgb.train(
  params = params,
  data = train_set,
  valids = list(validation = val_set),
  nrounds = 100000,
  early_stopping_rounds = 10,
  verbose = 0
)
```

```{r}
train_preds <- predict(model_lgb, data.matrix(data_train_balanced[,-which(names(data_train_balanced) == "VC")]), 
                       num_iteration = model_lgb$best_iter)
val_preds <- predict(model_lgb, data.matrix(data_val[,-which(names(data_val) == "VC")]), 
                     num_iteration = model_lgb$best_iter)

train_preds_class <- ifelse(train_preds > 0.5, 1, 0)
val_preds_class <- ifelse(val_preds > 0.5, 1, 0)

train_accuracy <- accuracy(train_preds_class, data_train_balanced$VC)
val_accuracy <- accuracy(val_preds_class, data_val$VC)

print(paste("Training accuracy:", train_accuracy))
print(paste("Validation accuracy:", val_accuracy))
```

```{r}
predictions <- predict(model_lgb, data.matrix(data_test[,-which(names(data_test) == "VC")]))

predictions_binary <- ifelse(predictions > 0.5, 1, 0)

test_accuracy <- accuracy(data_test$VC, predictions_binary)

print(paste("Test accuracy:", test_accuracy))
```

```{r}
conf_mat <- confusionMatrix(as.factor(predictions_binary), as.factor(data_test$VC))
recall <- conf_mat$byClass['Sensitivity']

print(paste("Recall:", recall))
```

```{r}
importance <- lgb.importance(model_lgb)

importance_sorted <- importance[order(-importance$Gain), ]

ggplot(importance_sorted, aes(x = reorder(Feature, -Gain), y = Gain)) +
  geom_col(fill = 'steelblue') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Feature Importance", x = "Features", y = "Importance (Gain)")
```

The following points can be found here based on the results:

-   "Training accuracy: 0.925287356321839 "

-   "Validation accuracy: 0.842911877394636 "

-   "Test accuracy: 0.823754789272031 "

-   "Test Recall: 0.844621513944223 "

-   Overall accuracy has decreased

-   The accuracy of the validation set has decreased compared to the training set, there may be over-fitting problems and the model has poor generalization ability.

-   According to the new feature importance, we can find that age is still the highest after missing logassets_1, which indicates that age does dominate some predictions, and logassets_1 and VC are highly correlated.

-   Next, we can try to find the best hyper-parameters by grid search and use cross-validation to enhance the model generalization ability and reduce over-fitting.
