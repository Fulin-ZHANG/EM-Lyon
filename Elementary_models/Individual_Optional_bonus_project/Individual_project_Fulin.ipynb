{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Optional Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 1. Data Inspection\n",
    "\n",
    "$\\qquad \\bullet$ [About the dataset and Kaggle](#about)<br>\n",
    "$\\qquad \\bullet$ [Load the data](#load_data)<br>\n",
    "$\\qquad \\bullet$ [Look at the data](#look_data)<br>\n",
    "$\\qquad \\bullet$ [Look at the columns](#look_columns)<br>\n",
    "\n",
    "\n",
    "### 2. Data Preprocessing\n",
    "\n",
    "$\\qquad \\bullet$ [Variable to predict](#output_variable)<br>\n",
    "$\\qquad \\bullet$ [Naive first model training](#naive_training)<br>\n",
    "$\\qquad \\bullet$ [Convert date times](#convert_date)<br>\n",
    "$\\qquad \\bullet$ [Convert strings into numerotated categories](#convert_strings)<br>\n",
    "$\\qquad \\bullet$ [Inspect missing values](#nans)<br>\n",
    "$\\qquad \\bullet$ [Save preprocessed data](#save_data)<br>\n",
    "$\\qquad \\bullet$ [Fully numericalize data](#numericalize_data)<br>\n",
    "\n",
    "### 3. Regression with Random Forests\n",
    "\n",
    "$\\qquad \\bullet$ [Regression Model scoring, underfitting, overfitting](#model_scoring)<br>\n",
    "$\\qquad \\bullet$ [Base model](#model_base)<br>\n",
    "$\\qquad \\bullet$ [Decision Tree Regressor](#decision_tree)<br>\n",
    "$\\qquad \\bullet$ [Tree bagging](#tree_bagging)<br>\n",
    "$\\qquad \\bullet$ [OOB score](#oob_score)<br>\n",
    "$\\qquad \\bullet$ [Stoping criteria](#tree_pruning)<br>\n",
    "$\\qquad \\bullet$ [Data subsampling](#data_sampling)<br>\n",
    "$\\qquad \\bullet$ [Feature subsampling](#feature_sampling)<br>\n",
    "$\\qquad \\bullet$ [Bagging as general Ensemble method](#general_bagging)<br>\n",
    "$\\qquad \\bullet$ [Cross validation](#cross_validation)<br>\n",
    "$\\qquad \\bullet$ [Hyperparameter tuning for final model selection](#hyperparameter_tuning)<br>\n",
    "\n",
    "\n",
    "### 4. Classification, and differences with Regression\n",
    "\n",
    "$\\qquad \\bullet$ [Categorical target](#categorical_target)<br>\n",
    "$\\qquad \\bullet$ [Decision Tree Classifier](#decision_tree_classifier)<br>\n",
    "$\\qquad \\bullet$ [Random Forest Classifier](#random_forest_classifier)<br>\n",
    "$\\qquad \\bullet$ [Evaluation criteria of binary classifiers](#binary_classifier_scoring)<br>\n",
    "$\\qquad \\bullet$ [Dealing with imbalanced data 1 : Precision - Recall curve and threshold selection](#rec_prc_curve)<br>\n",
    "$\\qquad \\bullet$ [Dealing with imbalanced data 2 : Class weights](#class_weight)<br>\n",
    "$\\qquad \\bullet$ [Dealing with imbalanced data 3 : Model calibration](#model_calibration)<br>\n",
    "$\\qquad \\bullet$ [Multiclass classification](#multiclass_classification)<br>\n",
    "\n",
    "\n",
    "### 5. Model Interpretation\n",
    "\n",
    "$\\qquad \\bullet$ [Random Forest features importance](#RF_feature_importance)<br>\n",
    "$\\qquad \\bullet$ [Permutation importance](#permutation_feature_importance)<br>\n",
    "$\\qquad \\bullet$ [Feature correlations](#redundant_features)<br>\n",
    "$\\qquad \\bullet$ [Individual feature explicability](#feature_explicability)<br>\n",
    "\n",
    "\n",
    "[Bottom](#bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base modules\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import logging\n",
    "import IPython\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# for manipulating data\n",
    "try:\n",
    "  from pandas_summary import DataFrameSummary\n",
    "except:\n",
    "  !pip install pandas_summary\n",
    "  from pandas_summary import DataFrameSummary\n",
    "\n",
    "try:\n",
    "  import dill\n",
    "except:\n",
    "  !pip install dill\n",
    "  import dill\n",
    "\n",
    "# for Machine Learning\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# for visualization\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#!pip install -U plotnine\n",
    "# install plotnine with conda command :  conda install -c conda-forge plotnine\n",
    "\n",
    "from plotnine import ggplot, aes\n",
    "from plotnine.stats import stat_smooth\n",
    "try:\n",
    "  from pdpbox import pdp\n",
    "except:\n",
    "  !pip install pdpbox==0.1.0\n",
    "  from pdpbox import pdp\n",
    "try:\n",
    "  import graphviz\n",
    "except:\n",
    "  !pip install graphviz\n",
    "  import graphviz\n",
    "  \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datepart(df, fldnames, drop=True, time=False, errors=\"raise\"):\n",
    "    \"\"\"add_datepart converts a column of df from a datetime64 to many columns containing\n",
    "    the information from the date. This applies changes inplace.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas data frame. df gain several new columns.\n",
    "    fldname: A string or list of strings that is the name of the date column you wish to expand.\n",
    "        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.\n",
    "    drop: If true then the original date column will be removed.\n",
    "    time: If true time features: Hour, Minute, Second will be added.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({ 'A' : pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000'], infer_datetime_format=False) })\n",
    "    >>> df\n",
    "        A\n",
    "    0   2000-03-11\n",
    "    1   2000-03-12\n",
    "    2   2000-03-13\n",
    "    >>> add_datepart(df, 'A')\n",
    "    >>> df\n",
    "        AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed\n",
    "    0   2000  3      10    11   5          71         False         False           False           False             False        False          952732800\n",
    "    1   2000  3      10    12   6          72         False         False           False           False             False        False          952819200\n",
    "    2   2000  3      11    13   0          73         False         False           False           False             False        False          952905600\n",
    "    >>>df2 = pd.DataFrame({'start_date' : pd.to_datetime(['3/11/2000','3/13/2000','3/15/2000']),\n",
    "                            'end_date':pd.to_datetime(['3/17/2000','3/18/2000','4/1/2000'],infer_datetime_format=True)})\n",
    "    >>>df2\n",
    "        start_date\tend_date\n",
    "    0\t2000-03-11\t2000-03-17\n",
    "    1\t2000-03-13\t2000-03-18\n",
    "    2\t2000-03-15\t2000-04-01\n",
    "    >>>add_datepart(df2,['start_date','end_date'])\n",
    "    >>>df2\n",
    "    \tstart_Year\tstart_Month\tstart_Week\tstart_Day\tstart_Dayofweek\tstart_Dayofyear\tstart_Is_month_end\tstart_Is_month_start\tstart_Is_quarter_end\tstart_Is_quarter_start\tstart_Is_year_end\tstart_Is_year_start\tstart_Elapsed\tend_Year\tend_Month\tend_Week\tend_Day\tend_Dayofweek\tend_Dayofyear\tend_Is_month_end\tend_Is_month_start\tend_Is_quarter_end\tend_Is_quarter_start\tend_Is_year_end\tend_Is_year_start\tend_Elapsed\n",
    "    0\t2000\t    3\t        10\t        11\t        5\t            71\t            False\t            False\t                False\t                False\t                False\t            False\t            952732800\t    2000\t    3\t        11\t        17\t    4\t            77\t            False\t            False\t            False\t            False\t                False\t        False\t            953251200\n",
    "    1\t2000\t    3\t        11\t        13\t        0\t            73\t            False\t            False\t                False\t                False               \tFalse           \tFalse           \t952905600     \t2000       \t3\t        11      \t18  \t5           \t78          \tFalse\t            False           \tFalse           \tFalse               \tFalse          \tFalse           \t953337600\n",
    "    2\t2000\t    3\t        11\t        15\t        2           \t75          \tFalse           \tFalse               \tFalse               \tFalse               \tFalse               False           \t953078400      \t2000    \t4          \t13      \t1   \t5           \t92          \tFalse           \tTrue            \tFalse           \tTrue                \tFalse          \tFalse           \t954547200\n",
    "    \"\"\"\n",
    "    if isinstance(fldnames,str):\n",
    "        fldnames = [fldnames]\n",
    "    for fldname in fldnames:\n",
    "        fld = df[fldname] # fld = df['saledate']\n",
    "        fld_dtype = fld.dtype\n",
    "        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "            fld_dtype = np.datetime64\n",
    "\n",
    "        if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n",
    "        targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "        if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "        for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "        df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "        if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "\n",
    "def train_cats(df):\n",
    "    \"\"\"Change any columns of strings in a panda's dataframe to a column of\n",
    "    categorical values. This applies the changes inplace.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas dataframe. Any columns of strings will be changed to\n",
    "        categorical values.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    note the type of col2 is string\n",
    "    >>> train_cats(df)\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    now the type of col2 is category\n",
    "    \"\"\"\n",
    "    for n, c in df.items():\n",
    "        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n",
    "\n",
    "def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n",
    "            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n",
    "    \"\"\" proc_df takes a data frame df and splits off the response variable, and\n",
    "    changes the df into an entirely numeric dataframe. For each column of df\n",
    "    which is not in skip_flds nor in ignore_flds, na values are replaced by the\n",
    "    median value of the column.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: The data frame you wish to process.\n",
    "    y_fld: The name of the response variable\n",
    "    skip_flds: A list of fields that dropped from df.\n",
    "    ignore_flds: A list of fields that are ignored during processing.\n",
    "    do_scale: Standardizes each column in df. Takes Boolean Values(True,False)\n",
    "    na_dict: a dictionary of na columns to add. Na columns are also added if there\n",
    "        are any missing values.\n",
    "    preproc_fn: A function that gets applied to df.\n",
    "    max_n_cat: The maximum number of categories to break into dummy values, instead\n",
    "        of integer codes.\n",
    "    subset: Takes a random subset of size subset from df.\n",
    "    mapper: If do_scale is set as True, the mapper variable\n",
    "        calculates the values used for scaling of variables during training time (mean and standard deviation).\n",
    "    Returns:\n",
    "    --------\n",
    "    [x, y, nas, mapper(optional)]:\n",
    "        x: x is the transformed version of df. x will not have the response variable\n",
    "            and is entirely numeric.\n",
    "        y: y is the response variable\n",
    "        nas: returns a dictionary of which nas it created, and the associated median.\n",
    "        mapper: A DataFrameMapper which stores the mean and standard deviation of the corresponding continuous\n",
    "        variables which is then used for scaling of during test-time.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    note the type of col2 is string\n",
    "    >>> train_cats(df)\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    now the type of col2 is category { a : 1, b : 2}\n",
    "    >>> x, y, nas = proc_df(df, 'col1')\n",
    "    >>> x\n",
    "       col2\n",
    "    0     1\n",
    "    1     2\n",
    "    2     1\n",
    "    >>> data = DataFrame(pet=[\"cat\", \"dog\", \"dog\", \"fish\", \"cat\", \"dog\", \"cat\", \"fish\"],\n",
    "                 children=[4., 6, 3, 3, 2, 3, 5, 4],\n",
    "                 salary=[90, 24, 44, 27, 32, 59, 36, 27])\n",
    "    >>> mapper = DataFrameMapper([(:pet, LabelBinarizer()),\n",
    "                          ([:children], StandardScaler())])\n",
    "    >>>round(fit_transform!(mapper, copy(data)), 2)\n",
    "    8x4 Array{Float64,2}:\n",
    "    1.0  0.0  0.0   0.21\n",
    "    0.0  1.0  0.0   1.88\n",
    "    0.0  1.0  0.0  -0.63\n",
    "    0.0  0.0  1.0  -0.63\n",
    "    1.0  0.0  0.0  -1.46\n",
    "    0.0  1.0  0.0  -0.63\n",
    "    1.0  0.0  0.0   1.04\n",
    "    0.0  0.0  1.0   0.21\n",
    "    \"\"\"\n",
    "    if not ignore_flds: ignore_flds=[]\n",
    "    if not skip_flds: skip_flds=[]\n",
    "    if subset: df = get_sample(df, subset)\n",
    "    else: df = df.copy()\n",
    "    ignored_flds = df.loc[:, ignore_flds]\n",
    "    df = df.drop(ignore_flds, axis = 1, inplace = False)\n",
    "    if preproc_fn: preproc_fn(df)\n",
    "    if y_fld is None: y = None\n",
    "    else:\n",
    "        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes\n",
    "        y = df[y_fld].values\n",
    "        skip_flds += [y_fld]\n",
    "    df.drop(skip_flds, axis=1, inplace=True)\n",
    "\n",
    "    if na_dict is None: na_dict = {}\n",
    "    else: na_dict = na_dict.copy()\n",
    "    na_dict_initial = na_dict.copy()\n",
    "    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n",
    "    if len(na_dict_initial.keys()) > 0:\n",
    "        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n",
    "    if do_scale: mapper = scale_vars(df, mapper)\n",
    "    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n",
    "    df = pd.get_dummies(df, dummy_na=True)\n",
    "    df = pd.concat([ignored_flds, df], axis=1)\n",
    "    res = [df, y, na_dict]\n",
    "    if do_scale: res = res + [mapper]\n",
    "    return res\n",
    "\n",
    "def fix_missing(df, col, name, na_dict):\n",
    "    \"\"\" Fill missing data in a column of df with the median, and add a {name}_na column\n",
    "    which specifies if the data was missing.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: The data frame that will be changed.\n",
    "    col: The column of data to fix by filling in missing data.\n",
    "    name: The name of the new filled column in df.\n",
    "    na_dict: A dictionary of values to create na's of and the value to insert. If\n",
    "        name is not a key of na_dict the median will fill any missing data. Also\n",
    "        if name is not a key of na_dict and there is no missing data in col, then\n",
    "        no {name}_na column is not created.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    5\n",
    "    1   nan    2\n",
    "    2     3    2\n",
    "    >>> fix_missing(df, df['col1'], 'col1', {})\n",
    "    >>> df\n",
    "       col1 col2 col1_na\n",
    "    0     1    5   False\n",
    "    1     2    2    True\n",
    "    2     3    2   False\n",
    "    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    5\n",
    "    1   nan    2\n",
    "    2     3    2\n",
    "    >>> fix_missing(df, df['col2'], 'col2', {})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    5\n",
    "    1   nan    2\n",
    "    2     3    2\n",
    "    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    5\n",
    "    1   nan    2\n",
    "    2     3    2\n",
    "    >>> fix_missing(df, df['col1'], 'col1', {'col1' : 500})\n",
    "    >>> df\n",
    "       col1 col2 col1_na\n",
    "    0     1    5   False\n",
    "    1   500    2    True\n",
    "    2     3    2   False\n",
    "    \"\"\"\n",
    "    if is_numeric_dtype(col):\n",
    "        if pd.isnull(col).sum() or (name in na_dict):\n",
    "            df[name+'_na'] = pd.isnull(col)\n",
    "            filler = na_dict[name] if name in na_dict else col.median()\n",
    "            df[name] = col.fillna(filler)\n",
    "            na_dict[name] = filler\n",
    "    return na_dict\n",
    "\n",
    "def numericalize(df, col, name, max_n_cat):\n",
    "    \"\"\" Changes the column col from a categorical type to it's integer codes.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas dataframe. df[name] will be filled with the integer codes from\n",
    "        col.\n",
    "    col: The column you wish to change into the categories.\n",
    "    name: The column name you wish to insert into df. This column will hold the\n",
    "        integer codes.\n",
    "    max_n_cat: If col has more categories than max_n_cat it will not change the\n",
    "        it to its integer codes. If max_n_cat is None, then col will always be\n",
    "        converted.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    note the type of col2 is string\n",
    "    >>> train_cats(df)\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    now the type of col2 is category { a : 1, b : 2}\n",
    "    >>> numericalize(df, df['col2'], 'col3', None)\n",
    "       col1 col2 col3\n",
    "    0     1    a    1\n",
    "    1     2    b    2\n",
    "    2     3    a    1\n",
    "    \"\"\"\n",
    "    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n",
    "        df[name] = pd.Categorical(col).codes+1\n",
    "\n",
    "def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n",
    "    \"\"\" Draws a representation of a random forest in IPython.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    t: The tree you wish to draw\n",
    "    df: The data used to train the tree. This is used to get the names of the features.\n",
    "    \"\"\"\n",
    "    from sklearn.tree import export_graphviz\n",
    "    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n",
    "                      special_characters=True, rotate=True, precision=precision)\n",
    "    IPython.display.display(graphviz.Source(re.sub('Tree {',\n",
    "       f'Tree {{ size={size}; ratio={ratio}', s)))\n",
    "\n",
    "def get_sample(df,n):\n",
    "    \"\"\" Gets a random sample of n rows from df, without replacement.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas data frame, that you wish to sample from.\n",
    "    n: The number of rows you wish to sample.\n",
    "    Returns:\n",
    "    --------\n",
    "    return value: A random sample of n rows of df.\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n",
    "    >>> df\n",
    "       col1 col2\n",
    "    0     1    a\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    >>> get_sample(df, 2)\n",
    "       col1 col2\n",
    "    1     2    b\n",
    "    2     3    a\n",
    "    \"\"\"\n",
    "    idxs = sorted(np.random.permutation(len(df))[:n])\n",
    "    return df.iloc[idxs].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_bulldozers = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ About the dataset and Kaggle\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Kaggle provides info about some of the fields of our dataset; on the [Kaggle Data info](https://www.kaggle.com/c/bluebook-for-bulldozers/data) page they say the following:\n",
    "\n",
    "For this competition, you are predicting the sale price of bulldozers sold at auctions. The data for this competition is split into three parts:\n",
    "\n",
    "- **Train.csv** is the training set, which contains data through the end of 2011.\n",
    "- **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012. You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n",
    "- **Test.csv** is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Import the data\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fulin/Documents/GitHub/EMLyon_DSAIS/Elementary_models/Individual_Optional_bonus_project/data/Train.csv\n"
     ]
    }
   ],
   "source": [
    "# select path do the dataset\n",
    "path_to_datasets = os.path.join(path_to_bulldozers,'data','Train.csv')\n",
    "print(path_to_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(path_to_datasets, low_memory = False, parse_dates = [\"saledate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 401125 entries, 0 to 401124\n",
      "Data columns (total 53 columns):\n",
      " #   Column                    Non-Null Count   Dtype         \n",
      "---  ------                    --------------   -----         \n",
      " 0   SalesID                   401125 non-null  int64         \n",
      " 1   SalePrice                 401125 non-null  int64         \n",
      " 2   MachineID                 401125 non-null  int64         \n",
      " 3   ModelID                   401125 non-null  int64         \n",
      " 4   datasource                401125 non-null  int64         \n",
      " 5   auctioneerID              380989 non-null  float64       \n",
      " 6   YearMade                  401125 non-null  int64         \n",
      " 7   MachineHoursCurrentMeter  142765 non-null  float64       \n",
      " 8   UsageBand                 69639 non-null   object        \n",
      " 9   saledate                  401125 non-null  datetime64[ns]\n",
      " 10  fiModelDesc               401125 non-null  object        \n",
      " 11  fiBaseModel               401125 non-null  object        \n",
      " 12  fiSecondaryDesc           263934 non-null  object        \n",
      " 13  fiModelSeries             56908 non-null   object        \n",
      " 14  fiModelDescriptor         71919 non-null   object        \n",
      " 15  ProductSize               190350 non-null  object        \n",
      " 16  fiProductClassDesc        401125 non-null  object        \n",
      " 17  state                     401125 non-null  object        \n",
      " 18  ProductGroup              401125 non-null  object        \n",
      " 19  ProductGroupDesc          401125 non-null  object        \n",
      " 20  Drive_System              104361 non-null  object        \n",
      " 21  Enclosure                 400800 non-null  object        \n",
      " 22  Forks                     192077 non-null  object        \n",
      " 23  Pad_Type                  79134 non-null   object        \n",
      " 24  Ride_Control              148606 non-null  object        \n",
      " 25  Stick                     79134 non-null   object        \n",
      " 26  Transmission              183230 non-null  object        \n",
      " 27  Turbocharged              79134 non-null   object        \n",
      " 28  Blade_Extension           25219 non-null   object        \n",
      " 29  Blade_Width               25219 non-null   object        \n",
      " 30  Enclosure_Type            25219 non-null   object        \n",
      " 31  Engine_Horsepower         25219 non-null   object        \n",
      " 32  Hydraulics                320570 non-null  object        \n",
      " 33  Pushblock                 25219 non-null   object        \n",
      " 34  Ripper                    104137 non-null  object        \n",
      " 35  Scarifier                 25230 non-null   object        \n",
      " 36  Tip_Control               25219 non-null   object        \n",
      " 37  Tire_Size                 94718 non-null   object        \n",
      " 38  Coupler                   213952 non-null  object        \n",
      " 39  Coupler_System            43458 non-null   object        \n",
      " 40  Grouser_Tracks            43362 non-null   object        \n",
      " 41  Hydraulics_Flow           43362 non-null   object        \n",
      " 42  Track_Type                99153 non-null   object        \n",
      " 43  Undercarriage_Pad_Width   99872 non-null   object        \n",
      " 44  Stick_Length              99218 non-null   object        \n",
      " 45  Thumb                     99288 non-null   object        \n",
      " 46  Pattern_Changer           99218 non-null   object        \n",
      " 47  Grouser_Type              99153 non-null   object        \n",
      " 48  Backhoe_Mounting          78672 non-null   object        \n",
      " 49  Blade_Type                79833 non-null   object        \n",
      " 50  Travel_Controls           79834 non-null   object        \n",
      " 51  Differential_Type         69411 non-null   object        \n",
      " 52  Steering_Controls         69369 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(6), object(44)\n",
      "memory usage: 162.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
