{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"images/emlyon.png\" style=\"height:60px; float:left; padding-right:10px; margin-top:5px\" />\n",
    "    <span>\n",
    "        <h1 style=\"padding-bottom:5px;\"> Introduction to Deep Learning </h1>\n",
    "        <a href=\"https://masters.em-lyon.com/fr/msc-in-data-science-artificial-intelligence-strategy\">[DSAIS]</a> MSc in Data Science & Artificial Intelligence Strategy <br/>\n",
    "         Paris | © Saeed VARASTEH\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\" style=\"border-bottom: solid 1px lightgray; background-color:#ece4f5;\">\n",
    "    <img src=\"images/assignment.png\" style=\"height:60px; float:left; padding-right:10px;\" />\n",
    "    <span style=\"font-weight:bold; color:#8966b0\">\n",
    "        <h4 style=\"padding-top:25px;\"> FINAL PROJECT </h4>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DSAIS Face Recognition Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll build a facial recognition system to recognize all of your friends.\n",
    "\n",
    "Face recognition problems generally fall into one of two categories:\n",
    "\n",
    "- __Face verification__, where you build a model to distinguish your face from others. This is a binary classification problem.\n",
    "- __Face Recognition__, where you build a model to identify multiple faces. This is a multi-class classification problem.\n",
    "\n",
    "Imagine a system for the Emlyon building where we want to provide face recognition to allow students to enter the building.\n",
    "\n",
    "The goal is to fine-tune the classification head of __FaceNet__ to classify all your friends.\n",
    "\n",
    "The project consists of two phases. \n",
    "\n",
    "- __Face Detection:__ In the first phase, you will need to prepare your datasets; you will use the __MTCNN__ model to create cropped face images from your raw images and store them in new directories.\n",
    "\n",
    "- __Face Recognition:__ The second phase is very similar to what we did with transfer learning. You modify the classifier head of __FaceNet__ and train it on the cropped face images.\n",
    "\n",
    "You will find FaceNet documentation here: [FaceNet](https://github.com/timesler/facenet-pytorch)\n",
    "\n",
    "Finally, you can use your trained model to test it on test data.\n",
    "\n",
    "__Extra step (high bounce!)__:\n",
    "\n",
    "Build a Python application that shows recognized faces of DSAIS students on video feeds (or real-time images from the camera). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting face samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, you will need eleven photos of yourself and each of your friends on which to base your facial recognition application. \n",
    "\n",
    "You will have to work together to get them. \n",
    "\n",
    "__The sooner you take your photos and share them with others, the easier it will be for everyone to start working on the project.__\n",
    "\n",
    "Let us say the deadline for uploading your pictures is <span style=\"color:crimson\">Sunday, February 25, 23:59.</span>\n",
    "\n",
    "Once you have your photos, post them [here](https://drive.google.com/drive/folders/1O8InPDGGy1eK7qjT92rL0DU9nd16lbo8?usp=sharing) under your name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Names = ['Gautier'  'Alexandre'  'Chadi'  'Ruicong'  'Shiqing'  'Namrata'  'Martino'  'Romain'\n",
    "         'Fulin'  'Eze'  'Tristan'  'Braxton'  'Gabriel'  'JeanLuis'\n",
    "         'Benjamin'  'Xinyue'  'Qinyi'  'JiaXin'  'Qiqi'  'Charlie'  'AswinSri'\n",
    "         'KingMan'  'Marouane'  'SongRim'  'Selin'  'HoangMinhThu'  'Selle'  'Peiwen'\n",
    "         'JiyuanPeter'  'Jing'  'Muyun'  'Mathieu'  'Michelle'  'Shiyao'  'Nicholas'\n",
    "         'Thomas'  'Maria'  'Jad'  'Ata'  'Manuel'  'Yuekai'  'Wujiachen'\n",
    "         'Saeed'  'Ahmed'  'Valentin'  'Mohamed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "your_name = \"Fulin\" # replace it with one of the names above (your name)\n",
    "try:  \n",
    "    os.mkdir(your_name) \n",
    "    print(\"Directory created.\")\n",
    "except OSError as error:  \n",
    "    print(\"The directory exists.\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenCV Library\n",
    "\n",
    "Install the opencv library if you do not have it. It is needed to access the camera on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/77/df/b56175c3fb5bc058774bdcf35f5a71cf9c3c5b909f98a1c688eb71cd3b1f/opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/fulin/anaconda3/lib/python3.11/site-packages (from opencv-python) (1.24.3)\n",
      "Downloading opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n"
     ]
    }
   ],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will allow you to access your system camera.\n",
    "\n",
    "Once you have the feeds. Press \"__s__\" to save a new picture of your face.\n",
    "\n",
    "You will need to do this 11 times.\n",
    "\n",
    "When you are done, press \"__q__\" to close the camera window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving image: 2024-03-05 09:11:37.jpg\n",
      "Saving image: 2024-03-05 09:11:38.jpg\n",
      "Saving image: 2024-03-05 09:11:39.jpg\n",
      "Saving image: 2024-03-05 09:11:40.jpg\n",
      "Saving image: 2024-03-05 09:11:41.jpg\n",
      "Saving image: 2024-03-05 09:11:42.jpg\n",
      "Saving image: 2024-03-05 09:11:43.jpg\n",
      "Saving image: 2024-03-05 09:11:44.jpg\n",
      "Saving image: 2024-03-05 09:11:48.jpg\n",
      "Saving image: 2024-03-05 09:11:49.jpg\n",
      "Saving image: 2024-03-05 09:11:50.jpg\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "src = your_name+\"/\"\n",
    "    \n",
    "while True:\n",
    "    _, frame = video_capture.read()\n",
    "    \n",
    "    scale_percent = 50\n",
    "    dim = (int(frame.shape[1] * scale_percent / 100), int(frame.shape[0] * scale_percent / 100))\n",
    "    frame = cv2.resize(frame, dim)\n",
    "    \n",
    "    cv2.imshow('Video Stream', frame)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xff\n",
    "\n",
    "    if key  == ord('s'):\n",
    "        timestr = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(\"Saving image: \"+timestr + \".jpg\")\n",
    "        filename = src + timestr + \".jpg\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "        \n",
    "    if key == ord('q'):\n",
    "        video_capture.release()\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 01: Face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./samples/Gautier/2024-02-24 18-08-12.jpg\n",
      "Processing: ./samples/Gautier/2024-02-24 18-08-13.jpg\n",
      "Processing: ./samples/Gautier/2024-02-24 18-08-07.jpg\n",
      "Processing: ./samples/Gautier/2024-02-24 18-08-29.jpg\n",
      "Processing: ./samples/Gautier/2024-02-24 18-08-15.jpg\n",
      "Processing: ./samples/Gautier/2024-02-24 18-08-49.jpg\n",
      "Processing: ./samples/Gautier/2024-02-24 18-08-44.jpg\n",
      "Processing: ./samples/Gautier/2024-02-24 18-08-46.jpg\n",
      "Processing: ./samples/Gautier/2024-02-24 18-08-21.jpg\n",
      "Processing: ./samples/Gautier/2024-02-24 18-08-20.jpg\n",
      "Processing: ./samples/Gautier/2024-02-24 18-08-23.jpg\n",
      "Processing: ./samples/Alexandre/WIN_20240223_16_57_53_Pro.jpg\n",
      "Processing: ./samples/Alexandre/WIN_20240223_16_57_53_Pro (2).jpg\n",
      "Processing: ./samples/Alexandre/WIN_20240223_16_57_42_Pro.jpg\n",
      "Processing: ./samples/Alexandre/WIN_20240223_16_57_52_Pro.jpg\n",
      "Processing: ./samples/Alexandre/WIN_20240223_16_57_49_Pro.jpg\n",
      "Processing: ./samples/Alexandre/WIN_20240223_16_57_50_Pro.jpg\n",
      "Processing: ./samples/Alexandre/WIN_20240223_16_57_45_Pro.jpg\n",
      "Processing: ./samples/Alexandre/WIN_20240223_16_57_55_Pro.jpg\n",
      "Processing: ./samples/Alexandre/WIN_20240223_16_57_47_Pro.jpg\n",
      "Processing: ./samples/Alexandre/WIN_20240223_16_57_46_Pro.jpg\n",
      "Processing: ./samples/Alexandre/WIN_20240223_16_57_56_Pro.jpg\n",
      "Processing: ./samples/Chadi/WIN_20240229_11_32_38_Pro.jpg\n",
      "Processing: ./samples/Chadi/WIN_20240229_11_50_29_Pro.jpg\n",
      "Processing: ./samples/Chadi/WIN_20240229_11_52_13_Pro (1).jpg\n",
      "Processing: ./samples/Chadi/WIN_20240229_11_37_57_Pro (1).jpg\n",
      "Processing: ./samples/Chadi/WIN_20240229_12_22_03_Pro.jpg\n",
      "Processing: ./samples/Chadi/WIN_20240229_11_52_13_Pro.jpg\n",
      "Processing: ./samples/Chadi/IMG_9129.JPG\n",
      "Processing: ./samples/Chadi/WIN_20240229_11_38_57_Pro.jpg\n",
      "Processing: ./samples/Chadi/WIN_20240229_11_47_21_Pro.jpg\n",
      "Processing: ./samples/Chadi/WIN_20240229_11_34_06_Pro.jpg\n",
      "Processing: ./samples/Chadi/WIN_20240229_11_37_57_Pro.jpg\n",
      "Processing: ./samples/Ruicong/WechatIMG640.jpg\n",
      "Processing: ./samples/Ruicong/Wang Ruicong.jpeg\n",
      "Processing: ./samples/Ruicong/Photo.jpg\n",
      "Processing: ./samples/Ruicong/WechatIMG665.jpg\n",
      "Processing: ./samples/Ruicong/WechatIMG664.jpg\n",
      "Processing: ./samples/Shiqing/2019-04-26 163811(1).jpg\n",
      "Processing: ./samples/Shiqing/shiqing (2).jpg\n",
      "Processing: ./samples/Shiqing/IMG20181013200450.jpg\n",
      "Processing: ./samples/Shiqing/shiqing (8).jpg\n",
      "Processing: ./samples/Shiqing/shiqing (9).JPG\n",
      "Processing: ./samples/Shiqing/shiqing (5).jpg\n",
      "Processing: ./samples/Shiqing/2019-04-26 163831.jpg\n",
      "Processing: ./samples/Shiqing/shiqing (6).jpg\n",
      "Processing: ./samples/Shiqing/shiqing (7).jpg\n",
      "Processing: ./samples/Shiqing/shiqing (10).JPG\n",
      "Processing: ./samples/Shiqing/shiqing (1).JPG\n",
      "Processing: ./samples/Namrata/1_10.jpg\n",
      "Processing: ./samples/Namrata/1_9.jpg\n",
      "Processing: ./samples/Namrata/1_8.jpg\n",
      "Processing: ./samples/Namrata/1_11.jpg\n",
      "Processing: ./samples/Namrata/1_3.jpg\n",
      "Processing: ./samples/Namrata/1_2.jpg\n",
      "Processing: ./samples/Namrata/1_1.jpg\n",
      "Processing: ./samples/Namrata/1_5.jpg\n",
      "Processing: ./samples/Namrata/1_4.jpg\n",
      "Processing: ./samples/Namrata/1_6.jpg\n",
      "Processing: ./samples/Namrata/1_7.jpg\n",
      "Processing: ./samples/Martino/martino4.jpeg\n",
      "Processing: ./samples/Martino/martino5.jpeg\n",
      "Processing: ./samples/Martino/martino2.jpeg\n",
      "Processing: ./samples/Martino/martino6.jpg\n",
      "Processing: ./samples/Martino/martino7.jpg\n",
      "Processing: ./samples/Martino/martino3.jpeg\n",
      "Processing: ./samples/Martino/martino9.jpg\n",
      "Processing: ./samples/Martino/martino1.jpeg\n",
      "Processing: ./samples/Martino/martino8.jpg\n",
      "Processing: ./samples/Martino/martino10.jpg\n",
      "Processing: ./samples/Martino/martino11.jpg\n",
      "Processing: ./samples/Romain/8.JPG\n",
      "Processing: ./samples/Romain/9.JPG\n",
      "Processing: ./samples/Romain/10.JPG\n",
      "Processing: ./samples/Romain/4.JPG\n",
      "Processing: ./samples/Romain/5.JPG\n",
      "Processing: ./samples/Romain/7.JPG\n",
      "Processing: ./samples/Romain/6.JPG\n",
      "Processing: ./samples/Romain/2.JPG\n",
      "Processing: ./samples/Romain/3.JPG\n",
      "Processing: ./samples/Romain/1.JPG\n",
      "Processing: ./samples/Fulin/IMG_0376.jpg\n",
      "Processing: ./samples/Fulin/IMG_0377.jpg\n",
      "Processing: ./samples/Fulin/IMG_0375.jpg\n",
      "Processing: ./samples/Fulin/IMG_0374.jpg\n",
      "Processing: ./samples/Fulin/IMG_3900.jpg\n",
      "Processing: ./samples/Fulin/IMG_9281.jpg\n",
      "Processing: ./samples/Fulin/IMG_3148.JPG\n",
      "Processing: ./samples/Fulin/IMG_0380.jpg\n",
      "Processing: ./samples/Fulin/IMG_0382.jpg\n",
      "Processing: ./samples/Fulin/IMG_0379.jpg\n",
      "Processing: ./samples/Fulin/IMG_0378.jpg\n",
      "Processing: ./samples/Eze/PXL_20230209_040634658.MP~4.jpg\n",
      "Processing: ./samples/Eze/IMG_20220102_184654_365.jpg\n",
      "Processing: ./samples/Eze/WIN_20240302_17_59_36_Pro.jpg\n",
      "Processing: ./samples/Eze/WIN_20240302_17_59_15_Pro.jpg\n",
      "Processing: ./samples/Eze/WIN_20240302_17_59_33_Pro.jpg\n",
      "Processing: ./samples/Eze/PXL_20221024_031828579.MP.jpg\n",
      "Processing: ./samples/Eze/WIN_20240302_17_59_32_Pro.jpg\n",
      "Processing: ./samples/Eze/WIN_20240302_17_59_20_Pro.jpg\n",
      "Processing: ./samples/Eze/PXL_20240127_101704251.PORTRAIT.ORIGINAL~2.jpg\n",
      "Processing: ./samples/Eze/WIN_20240302_17_59_17_Pro.jpg\n",
      "Processing: ./samples/Eze/WIN_20240302_17_59_38_Pro.jpg\n",
      "Processing: ./samples/Tristan/23FA7752-DF99-4913-B311-8FFCEC9C3EE1.jpeg\n",
      "Processing: ./samples/Tristan/95FB6182-DA0D-4DD5-949A-913F08E41659.jpeg\n",
      "Processing: ./samples/Tristan/27CFD56A-6C08-49DF-9719-C8E817B0DEDC.jpeg\n",
      "Processing: ./samples/Tristan/59C50CCF-E92C-4008-B809-741BD1E555C4.jpeg\n",
      "Processing: ./samples/Tristan/795E19A4-035C-4CDD-A161-C5796F1CF837.jpeg\n",
      "Processing: ./samples/Tristan/13F4BD32-DEEC-4917-BA0E-3546D44FD26E.jpeg\n",
      "Processing: ./samples/Tristan/A53EBC7E-ADD6-4C28-B103-201CD5382D8A.jpeg\n",
      "Processing: ./samples/Tristan/7E770B55-FA84-49A8-9F72-1DED14184CEA.jpeg\n",
      "Processing: ./samples/Tristan/CA4474C2-C622-46FF-9087-21DD023FD7B4.jpeg\n",
      "Processing: ./samples/Tristan/E00CF3BC-5DCF-4DE6-9B09-1F63BA6A36C7.jpeg\n",
      "Processing: ./samples/Tristan/0398F5B8-435E-4AF0-9416-8370D313893E.jpeg\n",
      "Processing: ./samples/Braxton/IMG_0414.jpeg\n",
      "Processing: ./samples/Braxton/37A6B48C-2CA7-4314-9EA8-EA13F3AEF75A.jpeg\n",
      "Processing: ./samples/Braxton/IMG_0298.jpeg\n",
      "Processing: ./samples/Braxton/FE29F8BC-F1E8-414D-A8F6-DEB8146888C7.jpeg\n",
      "Processing: ./samples/Braxton/IMG_0369.jpeg\n",
      "Processing: ./samples/Braxton/IMG_0386.jpeg\n",
      "Processing: ./samples/Braxton/C13B9F72-D2F8-424C-97FF-7B783EEECFB4.jpeg\n",
      "Processing: ./samples/Braxton/A4F14C66-867C-42C1-8D33-D706534AA0F9.jpeg\n",
      "Processing: ./samples/Braxton/IMG_0387.jpeg\n",
      "Processing: ./samples/Braxton/IMG_1148.jpeg\n",
      "Processing: ./samples/Braxton/IMG_0299.jpeg\n",
      "Processing: ./samples/Gabriel/WIN_20240227_10_28_59_Pro.jpg\n",
      "Processing: ./samples/Gabriel/WIN_20240227_10_29_35_Pro.jpg\n",
      "Processing: ./samples/Gabriel/WIN_20240227_10_29_03_Pro.jpg\n",
      "Processing: ./samples/Gabriel/WIN_20240227_10_28_58_Pro.jpg\n",
      "Processing: ./samples/Gabriel/WIN_20240227_10_29_00_Pro.jpg\n",
      "Processing: ./samples/Gabriel/WIN_20240227_10_29_36_Pro.jpg\n",
      "Processing: ./samples/Gabriel/WIN_20240227_10_29_05_Pro.jpg\n",
      "Processing: ./samples/Gabriel/WIN_20240227_10_29_04_Pro.jpg\n",
      "Processing: ./samples/Gabriel/WIN_20240227_10_29_06_Pro.jpg\n",
      "Processing: ./samples/Gabriel/WIN_20240227_10_28_54_Pro.jpg\n",
      "Processing: ./samples/Gabriel/WIN_20240227_10_29_07_Pro.jpg\n",
      "Processing: ./samples/JeanLuis/8.jpg\n",
      "Processing: ./samples/JeanLuis/9.jpg\n",
      "Processing: ./samples/JeanLuis/11.jpg\n",
      "Processing: ./samples/JeanLuis/10.jpg\n",
      "Processing: ./samples/JeanLuis/4.jpg\n",
      "Processing: ./samples/JeanLuis/5.jpg\n",
      "Processing: ./samples/JeanLuis/7.jpg\n",
      "Processing: ./samples/JeanLuis/6.jpg\n",
      "Processing: ./samples/JeanLuis/2.jpg\n",
      "Processing: ./samples/JeanLuis/3.jpg\n",
      "Processing: ./samples/JeanLuis/1.jpg\n",
      "Processing: ./samples/Benjamin/IMG_5273.HEIC\n",
      "Processing: ./samples/Benjamin/IMG_5340.HEIC\n",
      "Processing: ./samples/Benjamin/8374CAF3-76EF-4A82-BA1C-F6A4681C80FB.jpg\n",
      "Processing: ./samples/Benjamin/IMG_4443.jpeg\n",
      "Processing: ./samples/Benjamin/img_5472.jpg\n",
      "Processing: ./samples/Benjamin/IMG_1711.HEIC\n",
      "Processing: ./samples/Benjamin/IMG_5375.HEIC\n",
      "Processing: ./samples/Benjamin/cachedImage.png\n",
      "Processing: ./samples/Benjamin/B0702651-7AD2-4DE5-B2AD-01B78603EAAA.jpg\n",
      "Processing: ./samples/Benjamin/image.png\n",
      "Processing: ./samples/Benjamin/IMG_4640.HEIC\n",
      "Processing: ./samples/Xinyue/20240225165245.jpg\n",
      "Processing: ./samples/Xinyue/20240225165244.jpg\n",
      "Processing: ./samples/Xinyue/20240225165246.jpg\n",
      "Processing: ./samples/Xinyue/20240225165247.jpg\n",
      "Processing: ./samples/Xinyue/20240225165243.jpg\n",
      "Processing: ./samples/Xinyue/20240225165242.jpg\n",
      "Processing: ./samples/Xinyue/20240225165240.jpg\n",
      "Processing: ./samples/Xinyue/20240225164924.jpg\n",
      "Processing: ./samples/Xinyue/20240225164923.jpg\n",
      "Processing: ./samples/Xinyue/20240225164922.jpg\n",
      "Processing: ./samples/Xinyue/20240225164920.jpg\n",
      "Processing: ./samples/Qinyi/f65b7d75-da4a-4fd3-b44a-28fe0b2dcdf1.JPG\n",
      "Processing: ./samples/Qinyi/31e5e6bf-b19b-44b0-8a61-85655a0894b6.JPG\n",
      "Processing: ./samples/Qinyi/0b6af004-d2f4-40cb-a212-07bed13f762b.JPG\n",
      "Processing: ./samples/Qinyi/f092c659-9274-45cf-bdcd-bdde0d97efa7.JPG\n",
      "Processing: ./samples/Qinyi/bb06fc74-b3e0-42f6-a0df-ea0695bc20ea.JPG\n",
      "Processing: ./samples/Qinyi/622eea18-5f2d-41fe-a200-87fbe9527917.JPG\n",
      "Processing: ./samples/Qinyi/f85e9298-af99-4660-a865-75e9cbf31e28.JPG\n",
      "Processing: ./samples/Qinyi/1ac6adda-70d5-4216-af03-e1571b393359.JPG\n",
      "Processing: ./samples/Qinyi/a398c54c-a454-491a-8f19-93a0063b5a08.JPG\n",
      "Processing: ./samples/Qinyi/6391709200784_.pic.jpg\n",
      "Processing: ./samples/Qinyi/8df94953-ecbc-4b9c-b863-523eb6fe4768.JPG\n",
      "Processing: ./samples/JiaXin/IMG_1061.jpg\n",
      "Processing: ./samples/JiaXin/IMG_1075.jpg\n",
      "Processing: ./samples/JiaXin/IMG_1060.jpg\n",
      "Processing: ./samples/JiaXin/IMG_1062.PNG\n",
      "Processing: ./samples/JiaXin/IMG_1073.jpg\n",
      "Processing: ./samples/JiaXin/IMG_1066.PNG\n",
      "Processing: ./samples/JiaXin/IMG_1072.jpg\n",
      "Processing: ./samples/JiaXin/IMG_1064.PNG\n",
      "Processing: ./samples/JiaXin/IMG_1071.jpg\n",
      "Processing: ./samples/Qiqi/8.jpg\n",
      "Processing: ./samples/Qiqi/9.jpg\n",
      "Processing: ./samples/Qiqi/11.JPG\n",
      "Processing: ./samples/Qiqi/10.JPG\n",
      "Processing: ./samples/Qiqi/4.jpg\n",
      "Processing: ./samples/Qiqi/5.JPG\n",
      "Processing: ./samples/Qiqi/7.JPG\n",
      "Processing: ./samples/Qiqi/6.JPG\n",
      "Processing: ./samples/Qiqi/2.JPG\n",
      "Processing: ./samples/Qiqi/3.JPG\n",
      "Processing: ./samples/Qiqi/1.JPG\n",
      "Processing: ./samples/Charlie/PXL_20240205_162624732.jpg\n",
      "Processing: ./samples/Charlie/IMG20230909024922.jpg\n",
      "Processing: ./samples/Charlie/PXL_20240129_141210386.jpg\n",
      "Processing: ./samples/Charlie/PXL_20220627_205513333.jpg\n",
      "Processing: ./samples/Charlie/IMG20230629113011.jpg\n",
      "Processing: ./samples/Charlie/PXL_20240206_201202283.jpg\n",
      "Processing: ./samples/Charlie/Screenshot_20230302-225001.png\n",
      "Processing: ./samples/Charlie/PXL_20230404_174239058.jpg\n",
      "Processing: ./samples/Charlie/PXL_20210123_145637183.jpg\n",
      "Processing: ./samples/Charlie/IMG20230811081335.jpg\n",
      "Processing: ./samples/Charlie/IMG20230831114214~2.jpg\n",
      "Processing: ./samples/AswinSri/8.jpg\n",
      "Processing: ./samples/AswinSri/9.jpg\n",
      "Processing: ./samples/AswinSri/11.jpg\n",
      "Processing: ./samples/AswinSri/10.jpg\n",
      "Processing: ./samples/AswinSri/4.jpg\n",
      "Processing: ./samples/AswinSri/5.JPG\n",
      "Processing: ./samples/AswinSri/7.jpg\n",
      "Processing: ./samples/AswinSri/6.jpg\n",
      "Processing: ./samples/AswinSri/2.JPG\n",
      "Processing: ./samples/AswinSri/3.JPG\n",
      "Processing: ./samples/AswinSri/1.jpg\n",
      "Processing: ./samples/KingMan/signal-2024-02-29-114312.jpeg\n",
      "Processing: ./samples/KingMan/IMG_9345.jpg\n",
      "Processing: ./samples/KingMan/99a31a82-bb84-4cff-84b5-0115a7592fa3.jpg\n",
      "Processing: ./samples/KingMan/548639d2-d371-43ca-ae4c-dc8826b627a5.jpg\n",
      "Processing: ./samples/KingMan/IMG_2244.jpg\n",
      "Processing: ./samples/KingMan/4c4db33e-316c-4f76-9939-172c5c6f04bb.jpg\n",
      "Processing: ./samples/KingMan/IMG_7854.JPG\n",
      "Processing: ./samples/KingMan/eb88f04c-ee50-465c-9eb9-df79dd2bbbaa.jpg\n",
      "Processing: ./samples/KingMan/IMG_8590.jpg\n",
      "Processing: ./samples/KingMan/88FBC1C9-2AF3-4264-B02D-7A41E06757A9.jpg\n",
      "Processing: ./samples/KingMan/IMG_8181.jpg\n",
      "Processing: ./samples/Marouane/SAKAI_10.jpg\n",
      "Processing: ./samples/Marouane/SAKAI_11.jpg\n",
      "Processing: ./samples/Marouane/SAKAI_8.jpg\n",
      "Processing: ./samples/Marouane/SAKAI_9.jpg\n",
      "Processing: ./samples/Marouane/SAKAI_2.jpg\n",
      "Processing: ./samples/Marouane/SAKAI_3.jpg\n",
      "Processing: ./samples/Marouane/SAKAI_1.jpg\n",
      "Processing: ./samples/Marouane/SAKAI_4.jpg\n",
      "Processing: ./samples/Marouane/SAKAI_5.jpg\n",
      "Processing: ./samples/Marouane/SAKAI_7.jpg\n",
      "Processing: ./samples/Marouane/SAKAI_6.jpg\n",
      "Processing: ./samples/SongRim/8.jpg\n",
      "Processing: ./samples/SongRim/9.jpg\n",
      "Processing: ./samples/SongRim/11.jpg\n",
      "Processing: ./samples/SongRim/10.jpg\n",
      "Processing: ./samples/SongRim/4.jpg\n",
      "Processing: ./samples/SongRim/5.jpg\n",
      "Processing: ./samples/SongRim/7.jpg\n",
      "Processing: ./samples/SongRim/6.jpg\n",
      "Processing: ./samples/SongRim/2.jpg\n",
      "Processing: ./samples/SongRim/3.jpg\n",
      "Processing: ./samples/SongRim/1.jpg\n",
      "Processing: ./samples/Selin/IMG_5804.PNG\n",
      "Processing: ./samples/Selin/IMG_5806.PNG\n",
      "Processing: ./samples/Selin/IMG_3779_Original.HEIC\n",
      "Error processing ./samples/Selin/IMG_3779_Original.HEIC: Invalid input: No 'ftyp' box\n",
      "Processing: ./samples/Selin/IMG_5803.PNG\n",
      "Processing: ./samples/Selin/IMG_5800.PNG\n",
      "Processing: ./samples/Selin/IMG_9301_Original.HEIC\n",
      "Processing: ./samples/Selin/C0FA2BC8-8663-4D07-A873-A604A630BAFB_Original.JPG\n",
      "Processing: ./samples/Selin/IMG_8609_Original.HEIC\n",
      "Processing: ./samples/Selin/10687F4B-297B-48CA-BCB4-FF4AE45B7906_Original.JPG\n",
      "Processing: ./samples/Selin/IMG_7857.HEIC\n",
      "Processing: ./samples/Selin/825B9E89-67EC-437E-ADD4-7266A548BEA9_Original.JPG\n",
      "Processing: ./samples/Selin/IMG_5799.PNG\n",
      "Processing: ./samples/HoangMinhThu/IMG_5420.jpg\n",
      "Processing: ./samples/HoangMinhThu/IMG_5423.jpg\n",
      "Processing: ./samples/HoangMinhThu/IMG_5618.jpg\n",
      "Processing: ./samples/HoangMinhThu/IMG_5619.JPG\n",
      "Processing: ./samples/HoangMinhThu/IMG_3218.jpg\n",
      "Processing: ./samples/HoangMinhThu/IMG_2703.jpg\n",
      "Processing: ./samples/HoangMinhThu/IMG_3226.jpg\n",
      "Processing: ./samples/HoangMinhThu/IMG_3197.jpg\n",
      "Processing: ./samples/HoangMinhThu/IMG_2704.jpg\n",
      "Processing: ./samples/HoangMinhThu/IMG_5170.jpg\n",
      "Processing: ./samples/HoangMinhThu/IMG_2961.jpg\n",
      "Processing: ./samples/Selle/IMG_4966.HEIC\n",
      "Processing: ./samples/Selle/IMG_8821.HEIC\n",
      "Processing: ./samples/Selle/IMG_0154.HEIC\n",
      "Processing: ./samples/Selle/19557FF4-4EB9-474E-9617-B8550E68B913 2.JPG\n",
      "Processing: ./samples/Selle/412ED8D5-10EF-46BE-8D5F-B4F0ED5DF09B 2.JPG\n",
      "Processing: ./samples/Selle/IMG_5677.JPG\n",
      "Processing: ./samples/Selle/IMG_2394.HEIC\n",
      "Processing: ./samples/Selle/IMG_8388.HEIC\n",
      "Processing: ./samples/Selle/IMG_2300.HEIC\n",
      "Processing: ./samples/Selle/IMG_8120.HEIC\n",
      "Processing: ./samples/Selle/IMG_9475.HEIC\n",
      "Processing: ./samples/Peiwen/8.jpg\n",
      "Processing: ./samples/Peiwen/9.jpg\n",
      "Processing: ./samples/Peiwen/11.jpg\n",
      "Processing: ./samples/Peiwen/10.jpg\n",
      "Processing: ./samples/Peiwen/4.jpg\n",
      "Processing: ./samples/Peiwen/5.jpg\n",
      "Processing: ./samples/Peiwen/7.jpg\n",
      "Processing: ./samples/Peiwen/6.jpg\n",
      "Processing: ./samples/Peiwen/2.jpg\n",
      "Processing: ./samples/Peiwen/3.jpg\n",
      "Processing: ./samples/Peiwen/1.jpg\n",
      "Processing: ./samples/JiyuanPeter/8.jpg\n",
      "Processing: ./samples/JiyuanPeter/9.jpg\n",
      "Processing: ./samples/JiyuanPeter/11.jpg\n",
      "Processing: ./samples/JiyuanPeter/10.jpg\n",
      "Processing: ./samples/JiyuanPeter/4.jpg\n",
      "Processing: ./samples/JiyuanPeter/5.jpg\n",
      "Processing: ./samples/JiyuanPeter/7.jpg\n",
      "Processing: ./samples/JiyuanPeter/6.jpg\n",
      "Processing: ./samples/JiyuanPeter/2.png\n",
      "Processing: ./samples/JiyuanPeter/3.png\n",
      "Processing: ./samples/JiyuanPeter/1.png\n",
      "Processing: ./samples/Jing/IMG_0464.jpeg\n",
      "Processing: ./samples/Jing/IMG_9961.JPG\n",
      "Processing: ./samples/Jing/IMG_2477.JPG\n",
      "Processing: ./samples/Jing/DSCF0872.JPG\n",
      "Processing: ./samples/Jing/34369fc06981b92338712bef4ea982a6.JPG\n",
      "Processing: ./samples/Jing/IMG_2512.JPG\n",
      "Processing: ./samples/Jing/IMG_6545.JPG\n",
      "Processing: ./samples/Jing/IMG_2333.jpeg\n",
      "Processing: ./samples/Jing/IMG_6448.jpeg\n",
      "Processing: ./samples/Jing/FullSizeRender.jpeg\n",
      "Processing: ./samples/Jing/IMG_0734.jpeg\n",
      "Processing: ./samples/Muyun/8.jpg\n",
      "Processing: ./samples/Muyun/11.jpg\n",
      "Processing: ./samples/Muyun/10.jpg\n",
      "Processing: ./samples/Muyun/4.jpg\n",
      "Processing: ./samples/Muyun/5.jpg\n",
      "Processing: ./samples/Muyun/7.jpg\n",
      "Processing: ./samples/Muyun/6.jpg\n",
      "Processing: ./samples/Muyun/2.jpg\n",
      "Processing: ./samples/Muyun/3.jpg\n",
      "Processing: ./samples/Muyun/1.jpg\n",
      "Processing: ./samples/Muyun/9.1.jpg\n",
      "Processing: ./samples/Mathieu/10.jpeg\n",
      "Processing: ./samples/Mathieu/1.jpeg\n",
      "Processing: ./samples/Mathieu/11.jpeg\n",
      "Processing: ./samples/Mathieu/6.jpeg\n",
      "Processing: ./samples/Mathieu/7.jpeg\n",
      "Processing: ./samples/Mathieu/8.jpeg\n",
      "Processing: ./samples/Mathieu/4.jpeg\n",
      "Processing: ./samples/Mathieu/5.jpeg\n",
      "Processing: ./samples/Mathieu/9.jpeg\n",
      "Processing: ./samples/Mathieu/2.jpeg\n",
      "Processing: ./samples/Mathieu/3.jpeg\n",
      "Processing: ./samples/Michelle/img_1.jpg\n",
      "Processing: ./samples/Michelle/img_7.jpeg\n",
      "Processing: ./samples/Michelle/img_6.jpeg\n",
      "Processing: ./samples/Michelle/img_10.jpeg\n",
      "Processing: ./samples/Michelle/img_11.jpeg\n",
      "Processing: ./samples/Michelle/img_3.jpeg\n",
      "Processing: ./samples/Michelle/img_12.jpeg\n",
      "Processing: ./samples/Michelle/img_2.jpeg\n",
      "Processing: ./samples/Michelle/img_9.jpeg\n",
      "Processing: ./samples/Michelle/img_5.jpeg\n",
      "Processing: ./samples/Michelle/img_4.jpeg\n",
      "Processing: ./samples/Michelle/img_8.jpeg\n",
      "Processing: ./samples/Shiyao/10.jpeg\n",
      "Processing: ./samples/Shiyao/1.jpeg\n",
      "Processing: ./samples/Shiyao/11.jpeg\n",
      "Processing: ./samples/Shiyao/6.jpeg\n",
      "Processing: ./samples/Shiyao/7.jpeg\n",
      "Processing: ./samples/Shiyao/8.jpeg\n",
      "Processing: ./samples/Shiyao/4.jpeg\n",
      "Processing: ./samples/Shiyao/5.jpeg\n",
      "Processing: ./samples/Shiyao/9.jpeg\n",
      "Processing: ./samples/Shiyao/2.jpeg\n",
      "Processing: ./samples/Shiyao/3.jpeg\n",
      "Processing: ./samples/Nicholas/2024-02-25 15_30_45.jpg\n",
      "Processing: ./samples/Nicholas/2024-02-25 15_30_18.jpg\n",
      "Processing: ./samples/Nicholas/2024-02-25 15_30_19.jpg\n",
      "Processing: ./samples/Nicholas/2024-02-25 15_30_25.jpg\n",
      "Processing: ./samples/Nicholas/2024-02-25 15_30_33.jpg\n",
      "Processing: ./samples/Nicholas/2024-02-25 15_30_26.jpg\n",
      "Processing: ./samples/Nicholas/2024-02-25 15_30_22.jpg\n",
      "Processing: ./samples/Nicholas/2024-02-25 15_30_23.jpg\n",
      "Processing: ./samples/Nicholas/2024-02-25 15_30_38.jpg\n",
      "Processing: ./samples/Nicholas/2024-02-25 15_30_16.jpg\n",
      "Processing: ./samples/Nicholas/2024-02-25 15_30_15.jpg\n",
      "Processing: ./samples/Thomas/428387176_723257496668904_8814279702069555165_n.jpg\n",
      "Processing: ./samples/Thomas/428341388_1134003208037760_2943348281424681069_n.jpg\n",
      "Processing: ./samples/Thomas/428399519_2658321021001483_5045864128363059489_n.jpg\n",
      "Processing: ./samples/Thomas/428252580_1045571389837631_4679396314507889629_n.jpg\n",
      "Processing: ./samples/Thomas/428420923_301721426260523_4161662334190554138_n.jpg\n",
      "Processing: ./samples/Thomas/428456845_954204499635712_7462196424458951881_n.jpg\n",
      "Processing: ./samples/Thomas/428387179_1441505966743729_2038203758040244010_n.jpg\n",
      "Processing: ./samples/Thomas/double compressed cv pic.png\n",
      "Processing: ./samples/Thomas/428383004_821992539760154_7630262771988091281_n.jpg\n",
      "Processing: ./samples/Thomas/428437823_422965520173150_4911809117888019261_n.jpg\n",
      "Processing: ./samples/Thomas/428303422_686281797005916_7878764615702923470_n.jpg\n",
      "Processing: ./samples/Maria/8.jpg\n",
      "Processing: ./samples/Maria/9.JPG\n",
      "Processing: ./samples/Maria/11.jpg\n",
      "Processing: ./samples/Maria/10.jpg\n",
      "Processing: ./samples/Maria/4.JPG\n",
      "Processing: ./samples/Maria/5.jpg\n",
      "Processing: ./samples/Maria/7.JPG\n",
      "Processing: ./samples/Maria/6.JPG\n",
      "Processing: ./samples/Maria/2.JPG\n",
      "Processing: ./samples/Maria/3.jpg\n",
      "Processing: ./samples/Maria/1.JPG\n",
      "Processing: ./samples/Jad/IMG_5008.HEIC\n",
      "Processing: ./samples/Jad/IMG_5168.JPG\n",
      "Processing: ./samples/Jad/IMG_1156.PNG\n",
      "Processing: ./samples/Jad/IMG_1586.JPG\n",
      "Processing: ./samples/Jad/IMG_4809.JPG\n",
      "Processing: ./samples/Jad/IMG_8966.PNG\n",
      "Processing: ./samples/Jad/IMG_0737.JPG\n",
      "Processing: ./samples/Jad/IMG_4223.JPG\n",
      "Processing: ./samples/Jad/IMG_5317.PNG\n",
      "Processing: ./samples/Jad/IMG_9202.jpg\n",
      "Processing: ./samples/Jad/IMG_9224.HEIC\n",
      "Processing: ./samples/Ata/WhatsApp Image 2024-02-27 at 15.17.50 (5).jpeg\n",
      "Processing: ./samples/Ata/WhatsApp Image 2024-02-27 at 15.17.50 (1).jpeg\n",
      "Processing: ./samples/Ata/WhatsApp Image 2024-02-27 at 15.17.50 (3).jpeg\n",
      "Processing: ./samples/Ata/WhatsApp Image 2024-02-27 at 15.17.49.jpeg\n",
      "Processing: ./samples/Ata/WhatsApp Image 2024-02-27 at 15.17.50 (4).jpeg\n",
      "Processing: ./samples/Ata/WhatsApp Image 2024-02-27 at 15.17.50 (2).jpeg\n",
      "Processing: ./samples/Ata/photo_ataavlar copy.JPG adlı dosyanın kopyası.jpg\n",
      "Processing: ./samples/Ata/2024-02-25 14_49_15.jpg\n",
      "Processing: ./samples/Ata/WhatsApp Image 2024-02-27 at 15.17.50.jpeg\n",
      "Processing: ./samples/Ata/8c677972-561a-44c9-9ee9-ed9378e9a38d.jpeg\n",
      "Processing: ./samples/Ata/WhatsApp Image 2024-02-27 at 15.17.51.jpeg\n",
      "Processing: ./samples/Manuel/IMG-20240225-WA0053.jpg\n",
      "Processing: ./samples/Manuel/IMG-20220715-WA0015.jpg\n",
      "Processing: ./samples/Manuel/IMG_20230528_000006.jpg\n",
      "Processing: ./samples/Manuel/IMG-20240224-WA0004.jpg\n",
      "Processing: ./samples/Manuel/IMG-20240225-WA0057.jpg\n",
      "Processing: ./samples/Manuel/IMG-20210926-WA0044.jpg\n",
      "Processing: ./samples/Manuel/IMG_20220319_163759.jpg\n",
      "Processing: ./samples/Manuel/IMG_20230816_201940.jpg\n",
      "Processing: ./samples/Manuel/IMG-20240225-WA0048.jpg\n",
      "Processing: ./samples/Manuel/IMG-20240225-WA0061.jpg\n",
      "Processing: ./samples/Manuel/IMG_20220724_190254.jpg\n",
      "Processing: ./samples/Yuekai/Yuekai_8.png\n",
      "Processing: ./samples/Yuekai/Yuekai_9.png\n",
      "Processing: ./samples/Yuekai/Yuekai_10.png\n",
      "Processing: ./samples/Yuekai/Yuekai_11.png\n",
      "Processing: ./samples/Yuekai/Yuekai_12.png\n",
      "Processing: ./samples/Yuekai/Yuekai_2.png\n",
      "Processing: ./samples/Yuekai/Yuekai_3.png\n",
      "Processing: ./samples/Yuekai/Yuekai_1.png\n",
      "Processing: ./samples/Yuekai/Yuekai_4.png\n",
      "Processing: ./samples/Yuekai/Yuekai_5.png\n",
      "Processing: ./samples/Yuekai/Yuekai_7.png\n",
      "Processing: ./samples/Yuekai/Yuekai_6.png\n",
      "Processing: ./samples/Wujiachen/{9DB20918-5459-4434-8C18-2DA1DCDCD820}.png\n",
      "Processing: ./samples/Wujiachen/IMG_3447.jpg\n",
      "Processing: ./samples/Wujiachen/{2E35524F-D47B-4285-838F-A1E7F7F5C0E9}.png\n",
      "Processing: ./samples/Wujiachen/{45DCC259-A7F3-4038-BE45-CBE212DD5CAD}.png\n",
      "Processing: ./samples/Wujiachen/{77F47DB9-C4BE-46ed-ADA1-611336F3960E}.png\n",
      "Processing: ./samples/Wujiachen/QQ截图20230128015600.png\n",
      "Processing: ./samples/Wujiachen/{559BF061-8542-4e2c-BBBD-348592A178A7}.png\n",
      "Processing: ./samples/Wujiachen/{B9404637-AD1C-4ae9-9179-5CFFA99E50E6}.png\n",
      "Processing: ./samples/Wujiachen/{D10D0CBE-A4EF-4d6a-8D02-DCC4A21EC7B4}.png\n",
      "Processing: ./samples/Wujiachen/{ECF883E3-C848-4570-8F5B-B3572CB2688A}.png\n",
      "Processing: ./samples/Wujiachen/1.jpg\n",
      "Processing: ./samples/converted_to_jpg/IMG_5804.jpg\n",
      "Processing: ./samples/converted_to_jpg/IMG_5806.jpg\n",
      "Processing: ./samples/converted_to_jpg/Yuekai_8.jpg\n",
      "Processing: ./samples/converted_to_jpg/Yuekai_9.jpg\n",
      "Processing: ./samples/converted_to_jpg/{9DB20918-5459-4434-8C18-2DA1DCDCD820}.jpg\n",
      "Processing: ./samples/converted_to_jpg/IMG_5803.jpg\n",
      "Processing: ./samples/converted_to_jpg/IMG_5800.jpg\n",
      "Processing: ./samples/converted_to_jpg/{2E35524F-D47B-4285-838F-A1E7F7F5C0E9}.jpg\n",
      "Processing: ./samples/converted_to_jpg/IMG_1156.jpg\n",
      "Processing: ./samples/converted_to_jpg/{45DCC259-A7F3-4038-BE45-CBE212DD5CAD}.jpg\n",
      "Processing: ./samples/converted_to_jpg/{77F47DB9-C4BE-46ed-ADA1-611336F3960E}.jpg\n",
      "Processing: ./samples/converted_to_jpg/QQ截图20230128015600.jpg\n",
      "Processing: ./samples/converted_to_jpg/double compressed cv pic.jpg\n",
      "Processing: ./samples/converted_to_jpg/Yuekai_10.jpg\n",
      "Processing: ./samples/converted_to_jpg/Yuekai_11.jpg\n",
      "Processing: ./samples/converted_to_jpg/IMG_8966.jpg\n",
      "Processing: ./samples/converted_to_jpg/Yuekai_12.jpg\n",
      "Processing: ./samples/converted_to_jpg/IMG_5317.jpg\n",
      "Processing: ./samples/converted_to_jpg/{559BF061-8542-4e2c-BBBD-348592A178A7}.jpg\n",
      "Processing: ./samples/converted_to_jpg/Yuekai_2.jpg\n",
      "Processing: ./samples/converted_to_jpg/{B9404637-AD1C-4ae9-9179-5CFFA99E50E6}.jpg\n",
      "Processing: ./samples/converted_to_jpg/{D10D0CBE-A4EF-4d6a-8D02-DCC4A21EC7B4}.jpg\n",
      "Processing: ./samples/converted_to_jpg/{ECF883E3-C848-4570-8F5B-B3572CB2688A}.jpg\n",
      "Processing: ./samples/converted_to_jpg/Yuekai_3.jpg\n",
      "Processing: ./samples/converted_to_jpg/Yuekai_1.jpg\n",
      "Processing: ./samples/converted_to_jpg/IMG_1062.jpg\n",
      "Processing: ./samples/converted_to_jpg/Screenshot_20230302-225001.jpg\n",
      "Processing: ./samples/converted_to_jpg/cachedImage.jpg\n",
      "Processing: ./samples/converted_to_jpg/2.jpg\n",
      "Processing: ./samples/converted_to_jpg/Yuekai_4.jpg\n",
      "Processing: ./samples/converted_to_jpg/IMG_1066.jpg\n",
      "Processing: ./samples/converted_to_jpg/Yuekai_5.jpg\n",
      "Processing: ./samples/converted_to_jpg/3.jpg\n",
      "Processing: ./samples/converted_to_jpg/1.jpg\n",
      "Processing: ./samples/converted_to_jpg/Yuekai_7.jpg\n",
      "Processing: ./samples/converted_to_jpg/image.jpg\n",
      "Processing: ./samples/converted_to_jpg/IMG_1064.jpg\n",
      "Processing: ./samples/converted_to_jpg/IMG_5799.jpg\n",
      "Processing: ./samples/converted_to_jpg/Yuekai_6.jpg\n",
      "Processing: ./samples/Saeed/8.jpg\n",
      "Processing: ./samples/Saeed/9.jpg\n",
      "Processing: ./samples/Saeed/10.jpg\n",
      "Processing: ./samples/Saeed/4.jpg\n",
      "Processing: ./samples/Saeed/5.jpg\n",
      "Processing: ./samples/Saeed/7.jpg\n",
      "Processing: ./samples/Saeed/6.jpg\n",
      "Processing: ./samples/Saeed/2.jpg\n",
      "Processing: ./samples/Saeed/3.jpg\n",
      "Processing: ./samples/Saeed/1.jpg\n",
      "Processing: ./samples/Ahmed/Picture 8.jpg\n",
      "Processing: ./samples/Ahmed/Picture 9.jpg\n",
      "Processing: ./samples/Ahmed/Picture 10.jpg\n",
      "Processing: ./samples/Ahmed/Picture 1.jpg\n",
      "Processing: ./samples/Ahmed/Picture 11.jpg\n",
      "Processing: ./samples/Ahmed/Picture 2.jpg\n",
      "Processing: ./samples/Ahmed/Picture 3.jpg\n",
      "Processing: ./samples/Ahmed/Picture 7.jpg\n",
      "Processing: ./samples/Ahmed/Picture 6.jpg\n",
      "Processing: ./samples/Ahmed/Picture 4.jpg\n",
      "Processing: ./samples/Ahmed/Picture 5.jpg\n",
      "Processing: ./samples/Valentin/v9.jpg\n",
      "Processing: ./samples/Valentin/v8.jpg\n",
      "Processing: ./samples/Valentin/v11.jpg\n",
      "Processing: ./samples/Valentin/v10.jpg\n",
      "Processing: ./samples/Valentin/v1.jpg\n",
      "Processing: ./samples/Valentin/v3.jpg\n",
      "Processing: ./samples/Valentin/v2.jpg\n",
      "Processing: ./samples/Valentin/v6.jpg\n",
      "Processing: ./samples/Valentin/v7.jpg\n",
      "Processing: ./samples/Valentin/v5.jpg\n",
      "Processing: ./samples/Valentin/v4.jpg\n",
      "Processing: ./samples/Mohamed/unnamed-8.jpg\n",
      "Processing: ./samples/Mohamed/unnamed-9.jpg\n",
      "Processing: ./samples/Mohamed/unnamed-10.jpg\n",
      "Processing: ./samples/Mohamed/unnamed.jpg\n",
      "Processing: ./samples/Mohamed/unnamed-4.jpg\n",
      "Processing: ./samples/Mohamed/unnamed-5.jpg\n",
      "Processing: ./samples/Mohamed/unnamed-7.jpg\n",
      "Processing: ./samples/Mohamed/unnamed-6.jpg\n",
      "Processing: ./samples/Mohamed/unnamed-2.jpg\n",
      "Processing: ./samples/Mohamed/unnamed-3.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pillow_heif\n",
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 初始化MTCNN\n",
    "mtcnn = MTCNN(keep_all=True, post_process=False)\n",
    "\n",
    "# 读取HEIC文件并转换为PIL图像\n",
    "def read_heic_image(file_path):\n",
    "    heif_file = pillow_heif.read_heif(file_path)  # 确保你使用的pillow_heif版本与这里的使用方法相匹配\n",
    "    image = Image.frombytes(\n",
    "        heif_file.mode, \n",
    "        heif_file.size, \n",
    "        heif_file.data,\n",
    "        \"raw\",\n",
    "    )\n",
    "    return image\n",
    "\n",
    "# 检测并裁剪图像中的面部，并保存为JPEG格式\n",
    "def process_and_save_images(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # 遍历所有文件\n",
    "    for path in glob.glob(f'{input_dir}/**/*', recursive=True):\n",
    "        if os.path.isdir(path):\n",
    "            continue  # Skip directories\n",
    "        print(f'Processing: {path}')\n",
    "        try:\n",
    "            # Load image, converting to RGB if necessary\n",
    "            if path.lower().endswith('.heic'):\n",
    "                image = read_heic_image(path).convert('RGB')\n",
    "            else:\n",
    "                image = Image.open(path).convert('RGB')  # Convert image to RGB\n",
    "            \n",
    "            # Use MTCNN to detect faces\n",
    "            boxes, _ = mtcnn.detect(image)\n",
    "            if boxes is not None:\n",
    "                for i, box in enumerate(boxes):\n",
    "                    face = image.crop(box)\n",
    "                    # Save the cropped face image as JPEG\n",
    "                    file_name = os.path.basename(path).split('.')[0] + f'_face{i}.jpg'\n",
    "                    person_dir = os.path.join(output_dir, os.path.basename(os.path.dirname(path)))\n",
    "                    if not os.path.exists(person_dir):\n",
    "                        os.makedirs(person_dir)\n",
    "                    face.save(os.path.join(person_dir, file_name), format='JPEG')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "# 设置数据目录\n",
    "input_dir = './samples'\n",
    "output_dir = './faces'\n",
    "\n",
    "process_and_save_images(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 02: Face recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(src_folder, dst_folder):\n",
    "    # Create directories for train, val, and test inside the destination folder\n",
    "    train_dir = os.path.join(dst_folder, 'train')\n",
    "    val_dir = os.path.join(dst_folder, 'val')\n",
    "    test_dir = os.path.join(dst_folder, 'test')\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Get all class names\n",
    "    classes = os.listdir(src_folder)\n",
    "    classes = [c for c in classes if os.path.isdir(os.path.join(src_folder, c))]\n",
    "\n",
    "    # Split data and move to the corresponding folders\n",
    "    for cls in classes:\n",
    "        cls_dir = os.path.join(src_folder, cls)\n",
    "        images = os.listdir(cls_dir)\n",
    "        \n",
    "        # Make sure there is more than two images to split\n",
    "        if len(images) > 2:\n",
    "            # Use train_test_split to partition the data\n",
    "            train_images, test_images = train_test_split(images, test_size=3, random_state=42)\n",
    "            val_images = test_images[:2]  # First image for validation\n",
    "            test_images = test_images[1:]  # Second image for testing\n",
    "        else:\n",
    "            print(f\"Not enough images for class {cls} to split. Needs more than 2 images.\")\n",
    "            continue\n",
    "        \n",
    "        # Function to copy files\n",
    "        def copy_images(images, src_dir, dest_dir):\n",
    "            cls_dest_dir = os.path.join(dest_dir, cls)\n",
    "            os.makedirs(cls_dest_dir, exist_ok=True)\n",
    "            for img in images:\n",
    "                shutil.copy(os.path.join(src_dir, img), cls_dest_dir)\n",
    "        \n",
    "        # Copy images\n",
    "        copy_images(train_images, cls_dir, train_dir)\n",
    "        copy_images(val_images, cls_dir, val_dir)\n",
    "        copy_images(test_images, cls_dir, test_dir)\n",
    "\n",
    "# Define source and destination folders\n",
    "src_folder = './faces'\n",
    "dst_folder = './dataset_split'  # Destination folder is at the same level as 'faces'\n",
    "\n",
    "# Run the function\n",
    "split_data(src_folder, dst_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "# Set the paths to the split dataset\n",
    "train_dir = './dataset_split/train'\n",
    "val_dir = './dataset_split/val'\n",
    "\n",
    "# 1. Load and preprocess data\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),  # FaceNet's default input size is 160x160\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=data_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 2.197802197802198%\n",
      "Epoch 2, Accuracy: 2.197802197802198%\n",
      "Epoch 3, Accuracy: 4.395604395604396%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(imgs)\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     19\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/facenet_pytorch/models/inception_resnet_v1.py:288\u001b[0m, in \u001b[0;36mInceptionResnetV1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    286\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2d_4a(x)\n\u001b[1;32m    287\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2d_4b(x)\n\u001b[0;32m--> 288\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat_1(x)\n\u001b[1;32m    289\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_6a(x)\n\u001b[1;32m    290\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat_2(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/facenet_pytorch/models/inception_resnet_v1.py:61\u001b[0m, in \u001b[0;36mBlock35.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     60\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch0(x)\n\u001b[0;32m---> 61\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch1(x)\n\u001b[1;32m     62\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch2(x)\n\u001b[1;32m     63\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x0, x1, x2), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/facenet_pytorch/models/inception_resnet_v1.py:30\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 2. Create and modify the model\n",
    "# Assuming you already know the number of classes\n",
    "num_classes = len(os.listdir(train_dir))  # The number of folders in the training dataset is the number of classes\n",
    "model = InceptionResnetV1(pretrained='vggface2', classify=True, num_classes=num_classes)\n",
    "\n",
    "# 3. Define the training process\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 4. Train and validate the model\n",
    "num_epochs = 5  # Specify the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for imgs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Simplified validation process\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for imgs, labels in val_loader:\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Finally, save the model for later use\n",
    "torch.save(model.state_dict(), './model_face_recognition.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 5.617977528089888%\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have saved your trained model to 'model_face_recognition.pth'\n",
    "model = InceptionResnetV1(pretrained='vggface2', classify=True, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load('./model_face_recognition.pth'))\n",
    "model.eval()\n",
    "\n",
    "# You would need to have a 'test' folder similar to your 'train' and 'val' folders\n",
    "test_dataset = datasets.ImageFolder(root='./dataset_split/test', transform=data_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Evaluate on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Map label indices to class names (assuming you have a mapping dictionary)\n",
    "idx_to_class = {v: k for k, v in test_dataset.class_to_idx.items()}\n",
    "\n",
    "# Initialize the video feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    image = Image.fromarray(frame)\n",
    "    image = data_transforms(image).unsqueeze(0)\n",
    "\n",
    "    # Predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        pred_class = idx_to_class[preds.item()]\n",
    "\n",
    "    # Display the resulting frame with the prediction\n",
    "    cv2.putText(frame, pred_class, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
