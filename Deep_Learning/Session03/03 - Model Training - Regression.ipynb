{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"images/emlyon.png\" style=\"height:60px; float:left; padding-right:10px; margin-top:5px\" />\n",
    "    <span>\n",
    "        <h1 style=\"padding-bottom:5px;\"> Introduction to Deep Learning </h1>\n",
    "        <a href=\"https://masters.em-lyon.com/fr/msc-in-data-science-artificial-intelligence-strategy\">[DSAIS]</a> MSc in Data Science & Artificial Intelligence Strategy <br/>\n",
    "         Paris | © Saeed VARASTEH\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 03 : Model Training: Regression\n",
    "\n",
    "In this lecture we're going to cover a standard PyTorch training workflow, specifically:\n",
    "\n",
    "- Getting data ready\n",
    "- Building the model\n",
    "- Fitting the model to data (training)\n",
    "- Evaluating the model\n",
    "- Making predications\n",
    "\n",
    "on our previous simple regression model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X + 1. + .1 * np.random.randn(100, 1)\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "# Uses the remaining indices for validation\n",
    "test_idx = idx[80:]\n",
    "\n",
    "# Generates train and test sets\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "# Visualize data\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].scatter(X_train,y_train, c=\"b\", label=\"train data\"); ax[0].legend();\n",
    "ax[1].scatter(X_test,y_test, c=\"r\", label=\"test data\"); ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    "    In PyTorch, a dataset is represented by a regular Python class that inherits from the <b>Dataset</b> class.\n",
    "</div>\n",
    "\n",
    "You can think of it as a kind of a Python list of tuples, each tuple corresponding to one point (features, label).\n",
    "\n",
    "The most fundamental methods it needs to implement are:\n",
    "\n",
    "- `__init__(self)`: it takes whatever arguments needed to build a list of tuples — it may be the name of a CSV file that will be loaded and processed; it may be two tensors, one for features, another one for labels; or anything else, depending on the task at hand.\n",
    "\n",
    "- `__getitem__(self, index)`: it allows the dataset to be indexed, so it can work like a list (dataset[i]) — it must return a tuple (features, label) corresponding to the requested data point. We can return the corresponding slices of our pre-loaded dataset or tensors.\n",
    "\n",
    "- `__len__(self):`: it should simply return the size of the whole dataset so, whenever it is sampled, its indexing is limited to the actual size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s build a simple custom dataset that takes two tensors as arguments: one for the features, one for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.labels = torch.from_numpy(labels).float()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index], self.labels[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Why go through all this trouble to wrap a couple of tensors in a class?</i>\n",
    "\n",
    "The answer is to use the __DataLoader__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders\n",
    "\n",
    "Until now, we have used the __whole training data__ at every training step. It has been __batch gradient descent__ all along. This is fine for our ridiculously small dataset, sure, but if we want to go serious about all this, we must use __mini-batch gradient descent__. Thus, we need mini-batches. Thus, we need to slice our dataset accordingly.\n",
    "\n",
    "So we use PyTorch’s DataLoader class for this job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\">\n",
    " We tell the <b>DataLoader</b> which <b>dataset</b> to use (e.g. the one we just built in the previous section), the desired <b>mini-batch size</b> and if we’d like to shuffle it or not. That’s it!\n",
    "</div>\n",
    "\n",
    "Our loader will behave like an __iterator__, so we can __loop over it__ and __fetch a different mini-batch__ every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "print(len(train_loader)) # How many mini batches we will get from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve a sample mini-batch, one can simply run the command below — it will return a list containing two tensors, one for the features, another one for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training v1\n",
    "\n",
    "How does this change our previous training loop? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel().to(device)\n",
    "\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() \n",
    "    \n",
    "    for x_batch, y_batch in train_loader: # The mini batches loop\n",
    "            \n",
    "        x_batch = x_batch.to(device) # send them to device\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        yhat = model(x_batch) # use the batches instead of the whole training data\n",
    "        loss = loss_fn(yhat, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "        \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Two things are different now: not only we have an inner loop to load each and every mini-batch from our DataLoader but, more importantly, we are now sending <b>only one mini-batch to the device</b>.\n",
    "</div>\n",
    "\n",
    "This is important, particularly for bigger datasets and while we are working with GPUs. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation Split\n",
    "\n",
    "So far, we’ve focused on the training data only. We built a dataset and a data loader for it. We could do the same for the validation/test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "Do not forget, for each subset of data (train, validation, test), we build a corresponding <b>DataLoader</b>.\n",
    "</div>\n",
    "\n",
    "An easiest way to create a validation data out of your training dataset is to split it using Torch `random_split()`. \n",
    "\n",
    "PyTorch’s `random_split()` method is an easy and familiar way of performing a training-validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset) # current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, [60, 20])\n",
    "\n",
    "len(train_dataset), len(val_dataset) # after train/val split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finall, we create the __DataLoaders__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data loader for our validation set, so, it makes sense to use it for the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training v2\n",
    "\n",
    "We need to change the training loop to include the evaluation of our model, that is, computing the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel().to(device)\n",
    "\n",
    "print(model.state_dict())\n",
    "\n",
    "train_losses = [] # To track the training loss\n",
    "validation_losses = [] # To track the validation loss\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training Loop\n",
    "    model.train() \n",
    "    train_loss = 0\n",
    "    \n",
    "    for x_batch, y_batch in train_loader: # The mini batches loop for train\n",
    "            \n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        yhat = model(x_batch)\n",
    "        loss = loss_fn(yhat, y_batch)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss) # keep tracking of the losses\n",
    "    \n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader: # The mini batches loop for validation\n",
    "\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            \n",
    "            validation_loss += val_loss.item()\n",
    "    \n",
    "    validation_loss /= len(val_loader)\n",
    "    validation_losses.append(validation_loss) # keep tracking of the losses\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How have losses changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses,  label=\"train loss\");\n",
    "plt.plot(validation_losses,  label=\"val loss\");\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s pretty much it, but there are two small, yet important, things to consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "    <code>torch.no_grad()</code>: even though it won’t make a difference in our simple model, it is a good practice to wrap the validation inner loop with this <b>context manager</b> to disable any gradient calculation that you may inadvertently trigger — gradients belong in training, not in validation steps;\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\">\n",
    "    <code>eval()</code>: the only thing it does is setting the model to evaluation mode (just like its <code>train()</code> counterpart did), so the model can adjust its behavior regarding some operations, like Dropout.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predications\n",
    "\n",
    "Finally, we can use our trained model to make predictions on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model( torch.from_numpy(X_test).float().to(device) )\n",
    "y_pred = y_pred.detach().numpy()\n",
    "\n",
    "plt.scatter(X_test,y_test, c=\"b\", label=\"actual data\");\n",
    "plt.scatter(X_test,y_pred, c=\"r\", label=\"predicted data\");\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\" style=\"background-color:#fff4e3; padding-bottom:22px; background-image:url(images/arrows.png); background-repeat:no-repeat; background-position: right; background-size: contain;\">\n",
    "    <img src=\"images/homework.png\" style=\"height:60px; float:left; padding-right:10px;\" />\n",
    "    <span style=\"font-weight:bold; color:#db9425;\">\n",
    "        <h4 style=\"padding-top:25px;\"> HOMEWORK 01 </h4>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
