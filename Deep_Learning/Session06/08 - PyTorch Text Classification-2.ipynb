{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"images/emlyon.png\" style=\"height:60px; float:left; padding-right:10px; margin-top:5px\" />\n",
    "    <span>\n",
    "        <h1 style=\"padding-bottom:5px;\"> Introduction to Deep Learning </h1>\n",
    "        <a href=\"https://masters.em-lyon.com/fr/msc-in-data-science-artificial-intelligence-strategy\">[DSAIS]</a> MSc in Data Science & Artificial Intelligence Strategy <br/>\n",
    "         Paris | Â© Saeed VARASTEH\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 08 : PyTorch Text Classification\n",
    "\n",
    "In this notebook, we'll be working with recurrent neural network architectures in simple spam detector model.\n",
    "\n",
    "Our goal at this implementation will be to create a RNN/LSTM model that can accurately classify and distinguish the spam emails.\n",
    "\n",
    "<img style=\"width:20%\" src=\"./images/spam.png\" />\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "np.random.seed(72)\n",
    "torch.manual_seed(72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device automatically\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/spams.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2]['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and the Vocabulary\n",
    "\n",
    "We are creating a dictionary that will map a word to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "word2idx = {'': 0}\n",
    "     \n",
    "for i, row in df_train.iterrows(): # loop over df rows\n",
    "    tokens = row['data'].lower().split() #simple tokenization\n",
    "    for token in tokens: # add new tokens to the dictionary with an index\n",
    "        if token not in word2idx:\n",
    "            word2idx[token] = idx\n",
    "            idx += 1\n",
    "\n",
    "print(\"Done\")\n",
    "print(len(word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert sentences\n",
    "\n",
    "Convert the words in the sentences to their corresponding indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for i, row in df_train.iterrows():\n",
    "    tokens = row['data'].lower().split()\n",
    "    tokens_indices = [word2idx[token] for token in tokens]\n",
    "    X_train.append(tokens_indices)\n",
    "\n",
    "X_test = []\n",
    "for i, row in df_test.iterrows():\n",
    "    tokens = row['data'].lower().split()\n",
    "    tokens_indices = [word2idx[token] for token in tokens if token in word2idx]\n",
    "    X_test.append(tokens_indices)\n",
    "     \n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paddings\n",
    "\n",
    "Padding the sentences with 0s and fix their lengths so that the data can be trained in batches to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_train = np.max([len(x) for x in X_train])\n",
    "max_len_test = np.max([len(x) for x in X_test])\n",
    "max_len_train, max_len_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = np.max([max_len_train, max_len_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(X_train)):\n",
    "    x = X_train[j]\n",
    "    pad = [0] * (max_len - len(x))\n",
    "    X_train[j] = pad + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(X_test)):\n",
    "    x = X_test[j]\n",
    "    pad = [0] * (max_len - len(x))\n",
    "    X_test[j] = pad + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Numpys\n",
    "\n",
    "Converting the data into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels\n",
    "\n",
    "Converting the labels into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.label.values\n",
    "y_test = df_test.label.values\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.from_numpy(data).long()\n",
    "        self.labels = torch.from_numpy(labels).float()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index], self.labels[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "train_dataset = MyDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(train_dataset, [4000, 457])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model (Single-layer RNN)\n",
    "\n",
    "\n",
    "<img style=\"width:70%; margin-top:20px;\" src=\"./images/rnn_unrolled.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = len(word2idx) + 1\n",
    "        self.embedding_dim = 20\n",
    "        self.n_layers = 1\n",
    "        self.hidden_dim = 15\n",
    "\n",
    "        self.emb = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.rnn = nn.LSTM(input_size = self.embedding_dim, hidden_size = self.hidden_dim, \n",
    "                           num_layers = self.n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
    "\n",
    "        out = self.emb(x)\n",
    "\n",
    "        #out, _ = self.rnn(out, h0)\n",
    "        out, _ = self.rnn(out, (h0, c0))\n",
    "\n",
    "        #out = out[:, -1, :]\n",
    "        out, _ = torch.max(out, 1)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timer() # timer start\n",
    "\n",
    "model = MyRNN().to(device)\n",
    "\n",
    "train_losses = []; train_accs = []\n",
    "validation_losses = []; validation_accs = []\n",
    "\n",
    "lr = 0.001\n",
    "n_epochs = 15\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training Loop\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    for x_batch, y_batch in train_loader: \n",
    "            \n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        y_logits = model(x_batch).squeeze() # squeeze to remove extra `1` dimensions,\n",
    "        loss = loss_fn(y_logits, y_batch)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
    "        train_acc += accuracy_fn(y_true=y_batch, y_pred=y_pred) \n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    train_acc /= len(train_loader)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    validation_loss , validation_acc = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            y_logits = model(x_val).squeeze()\n",
    "            val_loss = loss_fn(y_logits, y_val)\n",
    "            \n",
    "            validation_loss += val_loss.item()\n",
    "            \n",
    "            y_pred = torch.round(torch.sigmoid(y_logits)) \n",
    "            validation_acc += accuracy_fn(y_true=y_val, y_pred=y_pred) \n",
    "        \n",
    "    validation_loss /= len(val_loader)\n",
    "    validation_losses.append(validation_loss)\n",
    "    \n",
    "    validation_acc /= len(val_loader)\n",
    "    validation_accs.append(validation_acc)\n",
    "    \n",
    "    # Print out what's happening, every epoch\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f\"Epoch: {epoch+1} | Loss: {train_loss:.5f}, Accuracy: {train_acc:.2f}% | val loss: {validation_loss:.5f}, val acc: {validation_acc:.2f}%\")\n",
    "\n",
    "end_time = timer() # timer end\n",
    "total_time = end_time - start_time\n",
    "print(f\"Train time on {device}: {total_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].plot(train_losses,  label=\"train loss\"); \n",
    "ax[0].plot(validation_losses,  label=\"val loss\"); \n",
    "ax[0].legend();\n",
    "\n",
    "ax[1].plot(train_accs,  label=\"train acc\");\n",
    "ax[1].plot(validation_accs,  label=\"val acc\"); \n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model( torch.from_numpy(X_test).long().to(device) )\n",
    "y_pred = torch.round(torch.sigmoid(y_pred)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print( classification_report(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
